<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Tema 2 Variables Aleatorias | Probabilidad y variables aleatorias para ML con R y Python</title>
  <meta name="description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="generator" content="bookdown 0.15 and GitBook 2.6.7" />

  <meta property="og:title" content="Tema 2 Variables Aleatorias | Probabilidad y variables aleatorias para ML con R y Python" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="github-repo" content="https://github.com/joanby/probabilidad" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Tema 2 Variables Aleatorias | Probabilidad y variables aleatorias para ML con R y Python" />
  
  <meta name="twitter:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  

<meta name="author" content="Ricardo Alberich, Juan Gabriel Gomila y Arnau Mir" />


<meta name="date" content="2019-11-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probabilidad.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Curso completo de Probabilidad y Variables Aleatorias con R y Python</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><i class="fa fa-check"></i>Pre requisitos: Teoría de conjuntos y combinatoria</a><ul>
<li class="chapter" data-level="0.1" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#antes-de-empezar"><i class="fa fa-check"></i><b>0.1</b> Antes de empezar</a><ul>
<li class="chapter" data-level="0.1.1" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#consideraciones"><i class="fa fa-check"></i><b>0.1.1</b> Consideraciones</a></li>
<li class="chapter" data-level="0.1.2" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#consideraciones-1"><i class="fa fa-check"></i><b>0.1.2</b> Consideraciones</a></li>
<li class="chapter" data-level="0.1.3" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#teoría-de-conjuntos"><i class="fa fa-check"></i><b>0.1.3</b> Teoría de conjuntos</a></li>
<li class="chapter" data-level="0.1.4" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#teoría-de-conjuntos-1"><i class="fa fa-check"></i><b>0.1.4</b> Teoría de conjuntos</a></li>
<li class="chapter" data-level="0.1.5" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#conjuntos-básicos"><i class="fa fa-check"></i><b>0.1.5</b> Conjuntos básicos</a></li>
<li class="chapter" data-level="0.1.6" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#conjuntos-básicos-1"><i class="fa fa-check"></i><b>0.1.6</b> Conjuntos básicos</a></li>
<li class="chapter" data-level="0.1.7" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#características-y-propiedades-básicas-de-los-conjuntos"><i class="fa fa-check"></i><b>0.1.7</b> Características y propiedades básicas de los conjuntos</a></li>
<li class="chapter" data-level="0.1.8" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#características-y-propiedades-básicas-de-los-conjuntos-1"><i class="fa fa-check"></i><b>0.1.8</b> Características y propiedades básicas de los conjuntos</a></li>
<li class="chapter" data-level="0.1.9" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#características-y-propiedades-básicas-de-los-conjuntos-2"><i class="fa fa-check"></i><b>0.1.9</b> Características y propiedades básicas de los conjuntos</a></li>
<li class="chapter" data-level="0.1.10" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#características-y-propiedades-básicas-de-los-conjuntos-3"><i class="fa fa-check"></i><b>0.1.10</b> Características y propiedades básicas de los conjuntos</a></li>
<li class="chapter" data-level="0.1.11" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#características-y-propiedades-básicas-de-los-conjuntos-4"><i class="fa fa-check"></i><b>0.1.11</b> Características y propiedades básicas de los conjuntos</a></li>
<li class="chapter" data-level="0.1.12" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#características-y-propiedades-básicas-de-los-conjuntos-5"><i class="fa fa-check"></i><b>0.1.12</b> Características y propiedades básicas de los conjuntos</a></li>
<li class="chapter" data-level="0.1.13" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#operaciones-conjuntos-intersección."><i class="fa fa-check"></i><b>0.1.13</b> Operaciones conjuntos: Intersección.</a></li>
<li class="chapter" data-level="0.1.14" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#operaciones-conjuntos-unión."><i class="fa fa-check"></i><b>0.1.14</b> Operaciones conjuntos: Unión.</a></li>
<li class="chapter" data-level="0.1.15" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#operaciones-conjuntos-diferencia."><i class="fa fa-check"></i><b>0.1.15</b> Operaciones conjuntos: Diferencia.</a></li>
<li class="chapter" data-level="0.1.16" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#operaciones-conjuntos-complementario"><i class="fa fa-check"></i><b>0.1.16</b> Operaciones conjuntos: Complementario</a></li>
<li class="chapter" data-level="0.1.17" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#más-propiedades-y-definiciones"><i class="fa fa-check"></i><b>0.1.17</b> Más propiedades y definiciones</a></li>
<li class="chapter" data-level="0.1.18" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-r-ejemplos."><i class="fa fa-check"></i><b>0.1.18</b> Con R, ejemplos.</a></li>
<li class="chapter" data-level="0.1.19" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-r-ejemplos.-1"><i class="fa fa-check"></i><b>0.1.19</b> Con R, ejemplos.</a></li>
<li class="chapter" data-level="0.1.20" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-r-ejemplos.-2"><i class="fa fa-check"></i><b>0.1.20</b> Con R, ejemplos.</a></li>
<li class="chapter" data-level="0.1.21" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-r-ejemplos.-3"><i class="fa fa-check"></i><b>0.1.21</b> Con R, ejemplos.</a></li>
<li class="chapter" data-level="0.1.22" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-r-ejemplos.-4"><i class="fa fa-check"></i><b>0.1.22</b> Con R, ejemplos.</a></li>
<li class="chapter" data-level="0.1.23" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-r-ejemplos.-5"><i class="fa fa-check"></i><b>0.1.23</b> Con R, ejemplos.</a></li>
<li class="chapter" data-level="0.1.24" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-python"><i class="fa fa-check"></i><b>0.1.24</b> Con python</a></li>
<li class="chapter" data-level="0.1.25" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-python-1"><i class="fa fa-check"></i><b>0.1.25</b> Con python</a></li>
<li class="chapter" data-level="0.1.26" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-python-2"><i class="fa fa-check"></i><b>0.1.26</b> Con python</a></li>
<li class="chapter" data-level="0.1.27" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#con-python-3"><i class="fa fa-check"></i><b>0.1.27</b> Con python</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria"><i class="fa fa-check"></i><b>0.2</b> Combinatoria</a><ul>
<li class="chapter" data-level="0.2.1" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria.-introducción."><i class="fa fa-check"></i><b>0.2.1</b> Combinatoria. Introducción.</a></li>
<li class="chapter" data-level="0.2.2" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria.-número-binomial."><i class="fa fa-check"></i><b>0.2.2</b> Combinatoria. Número binomial.</a></li>
<li class="chapter" data-level="0.2.3" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria.-número-binomial.-1"><i class="fa fa-check"></i><b>0.2.3</b> Combinatoria. Número binomial.</a></li>
<li class="chapter" data-level="0.2.4" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria.-variaciones."><i class="fa fa-check"></i><b>0.2.4</b> Combinatoria. Variaciones.</a></li>
<li class="chapter" data-level="0.2.5" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria.-variaciones-sin-repetición."><i class="fa fa-check"></i><b>0.2.5</b> Combinatoria. Variaciones (sin repetición).</a></li>
<li class="chapter" data-level="0.2.6" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria.-variaciones.-1"><i class="fa fa-check"></i><b>0.2.6</b> Combinatoria. Variaciones.</a></li>
<li class="chapter" data-level="0.2.7" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria.-variaciones-con-repetición."><i class="fa fa-check"></i><b>0.2.7</b> Combinatoria. Variaciones con repetición.</a></li>
<li class="chapter" data-level="0.2.8" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#permutaciones"><i class="fa fa-check"></i><b>0.2.8</b> Permutaciones</a></li>
<li class="chapter" data-level="0.2.9" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#permutaciones-1"><i class="fa fa-check"></i><b>0.2.9</b> Permutaciones</a></li>
<li class="chapter" data-level="0.2.10" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#permutaciones-2"><i class="fa fa-check"></i><b>0.2.10</b> Permutaciones</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#para-acabar"><i class="fa fa-check"></i><b>0.3</b> Para acabar</a><ul>
<li class="chapter" data-level="0.3.1" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#otros-asuntos-básicos"><i class="fa fa-check"></i><b>0.3.1</b> Otros asuntos básicos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="probabilidad.html"><a href="probabilidad.html"><i class="fa fa-check"></i><b>1</b> Probabilidad</a><ul>
<li class="chapter" data-level="1.1" data-path="probabilidad.html"><a href="probabilidad.html#probabilidades-básicas"><i class="fa fa-check"></i><b>1.1</b> Probabilidades Básicas</a><ul>
<li class="chapter" data-level="1.1.1" data-path="probabilidad.html"><a href="probabilidad.html#definiciones-básicas"><i class="fa fa-check"></i><b>1.1.1</b> Definiciones básicas</a></li>
<li class="chapter" data-level="1.1.2" data-path="probabilidad.html"><a href="probabilidad.html#definiciones-básicas-1"><i class="fa fa-check"></i><b>1.1.2</b> Definiciones básicas</a></li>
<li class="chapter" data-level="1.1.3" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-n-grama"><i class="fa fa-check"></i><b>1.1.3</b> Ejemplo <span class="math inline">\(n\)</span>-grama</a></li>
<li class="chapter" data-level="1.1.4" data-path="probabilidad.html"><a href="probabilidad.html#operaciones-con-sucesos"><i class="fa fa-check"></i><b>1.1.4</b> Operaciones con sucesos</a></li>
<li class="chapter" data-level="1.1.5" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-género"><i class="fa fa-check"></i><b>1.1.5</b> Ejemplo género</a></li>
<li class="chapter" data-level="1.1.6" data-path="probabilidad.html"><a href="probabilidad.html#propiedades"><i class="fa fa-check"></i><b>1.1.6</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.7" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-1"><i class="fa fa-check"></i><b>1.1.7</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.8" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-2"><i class="fa fa-check"></i><b>1.1.8</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.9" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-3"><i class="fa fa-check"></i><b>1.1.9</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.10" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-4"><i class="fa fa-check"></i><b>1.1.10</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.11" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-5"><i class="fa fa-check"></i><b>1.1.11</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.12" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-6"><i class="fa fa-check"></i><b>1.1.12</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.13" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-7"><i class="fa fa-check"></i><b>1.1.13</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.14" data-path="probabilidad.html"><a href="probabilidad.html#definición-de-probabilidad"><i class="fa fa-check"></i><b>1.1.14</b> Definición de probabilidad</a></li>
<li class="chapter" data-level="1.1.15" data-path="probabilidad.html"><a href="probabilidad.html#definición-de-probabilidad-1"><i class="fa fa-check"></i><b>1.1.15</b> Definición de probabilidad</a></li>
<li class="chapter" data-level="1.1.16" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-grupos-sangíneos"><i class="fa fa-check"></i><b>1.1.16</b> Ejemplo: grupos sangíneos</a></li>
<li class="chapter" data-level="1.1.17" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-8"><i class="fa fa-check"></i><b>1.1.17</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.18" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-9"><i class="fa fa-check"></i><b>1.1.18</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.19" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-10"><i class="fa fa-check"></i><b>1.1.19</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.20" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-11"><i class="fa fa-check"></i><b>1.1.20</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.21" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-frecuencia-de-vocales"><i class="fa fa-check"></i><b>1.1.21</b> Ejemplo: Frecuencia de vocales</a></li>
<li class="chapter" data-level="1.1.22" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-consumo-de-drogas"><i class="fa fa-check"></i><b>1.1.22</b> Ejemplo: Consumo de drogas</a></li>
<li class="chapter" data-level="1.1.23" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-consumo-de-drogas"><i class="fa fa-check"></i><b>1.1.23</b> Ejemplos: Consumo de drogas</a></li>
<li class="chapter" data-level="1.1.24" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-control-de-drogas"><i class="fa fa-check"></i><b>1.1.24</b> Ejemplo: Control de drogas</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-condicionada"><i class="fa fa-check"></i><b>1.2</b> Probabilidad condicionada</a><ul>
<li class="chapter" data-level="1.2.1" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-condicionada-1"><i class="fa fa-check"></i><b>1.2.1</b> Probabilidad condicionada</a></li>
<li class="chapter" data-level="1.2.2" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-frecuencia-género-y-gafas"><i class="fa fa-check"></i><b>1.2.2</b> Ejemplo: frecuencia género y gafas</a></li>
<li class="chapter" data-level="1.2.3" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-sexo-y-gafas"><i class="fa fa-check"></i><b>1.2.3</b> Ejemplo: sexo y gafas</a></li>
<li class="chapter" data-level="1.2.4" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo"><i class="fa fa-check"></i><b>1.2.4</b> Ejemplo</a></li>
<li class="chapter" data-level="1.2.5" data-path="probabilidad.html"><a href="probabilidad.html#atención"><i class="fa fa-check"></i><b>1.2.5</b> ¡Atención!</a></li>
<li class="chapter" data-level="1.2.6" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-condicionada.-propiedades"><i class="fa fa-check"></i><b>1.2.6</b> Probabilidad condicionada. Propiedades</a></li>
<li class="chapter" data-level="1.2.7" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos"><i class="fa fa-check"></i><b>1.2.7</b> Ejemplos</a></li>
<li class="chapter" data-level="1.2.8" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-1"><i class="fa fa-check"></i><b>1.2.8</b> Ejemplo</a></li>
<li class="chapter" data-level="1.2.9" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-dígitos-de-control"><i class="fa fa-check"></i><b>1.2.9</b> Ejemplos: dígitos de control</a></li>
<li class="chapter" data-level="1.2.10" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-1"><i class="fa fa-check"></i><b>1.2.10</b> Ejemplos</a></li>
<li class="chapter" data-level="1.2.11" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-2"><i class="fa fa-check"></i><b>1.2.11</b> Ejemplos</a></li>
<li class="chapter" data-level="1.2.12" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-3"><i class="fa fa-check"></i><b>1.2.12</b> Ejemplos</a></li>
<li class="chapter" data-level="1.2.13" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-spam-continuación"><i class="fa fa-check"></i><b>1.2.13</b> Ejemplos SPAM continuación</a></li>
<li class="chapter" data-level="1.2.14" data-path="probabilidad.html"><a href="probabilidad.html#teorema-de-la-probabilidad-total"><i class="fa fa-check"></i><b>1.2.14</b> Teorema de la probabilidad total</a></li>
<li class="chapter" data-level="1.2.15" data-path="probabilidad.html"><a href="probabilidad.html#teorema-de-la-probabilidad-total-1"><i class="fa fa-check"></i><b>1.2.15</b> Teorema de la probabilidad total</a></li>
<li class="chapter" data-level="1.2.16" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-4"><i class="fa fa-check"></i><b>1.2.16</b> Ejemplos</a></li>
<li class="chapter" data-level="1.2.17" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos"><i class="fa fa-check"></i><b>1.2.17</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.18" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos-1"><i class="fa fa-check"></i><b>1.2.18</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.19" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos-2"><i class="fa fa-check"></i><b>1.2.19</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.20" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos-3"><i class="fa fa-check"></i><b>1.2.20</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.21" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos-4"><i class="fa fa-check"></i><b>1.2.21</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.22" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos-5"><i class="fa fa-check"></i><b>1.2.22</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.23" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos-6"><i class="fa fa-check"></i><b>1.2.23</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.24" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos-7"><i class="fa fa-check"></i><b>1.2.24</b> Clasificación o Diagnósticos</a></li>
<li class="chapter" data-level="1.2.25" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-5"><i class="fa fa-check"></i><b>1.2.25</b> Ejemplos</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probabilidad.html"><a href="probabilidad.html#bayes"><i class="fa fa-check"></i><b>1.3</b> Bayes</a><ul>
<li class="chapter" data-level="1.3.1" data-path="probabilidad.html"><a href="probabilidad.html#fórmula-de-bayes"><i class="fa fa-check"></i><b>1.3.1</b> Fórmula de Bayes</a></li>
<li class="chapter" data-level="1.3.2" data-path="probabilidad.html"><a href="probabilidad.html#fórmula-de-bayes-1"><i class="fa fa-check"></i><b>1.3.2</b> Fórmula de Bayes</a></li>
<li class="chapter" data-level="1.3.3" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-6"><i class="fa fa-check"></i><b>1.3.3</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3.4" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-7"><i class="fa fa-check"></i><b>1.3.4</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3.5" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-8"><i class="fa fa-check"></i><b>1.3.5</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3.6" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-9"><i class="fa fa-check"></i><b>1.3.6</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3.7" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-10"><i class="fa fa-check"></i><b>1.3.7</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3.8" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos-11"><i class="fa fa-check"></i><b>1.3.8</b> Ejemplos</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probabilidad.html"><a href="probabilidad.html#independencia-de-sucesos"><i class="fa fa-check"></i><b>1.4</b> Independencia de sucesos</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probabilidad.html"><a href="probabilidad.html#sucesos-independientes"><i class="fa fa-check"></i><b>1.4.1</b> Sucesos independientes</a></li>
<li class="chapter" data-level="1.4.2" data-path="probabilidad.html"><a href="probabilidad.html#sucesos-independientes-1"><i class="fa fa-check"></i><b>1.4.2</b> Sucesos independientes</a></li>
<li class="chapter" data-level="1.4.3" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-billete-avión"><i class="fa fa-check"></i><b>1.4.3</b> Ejemplo billete avión</a></li>
<li class="chapter" data-level="1.4.4" data-path="probabilidad.html"><a href="probabilidad.html#sucesos-independientes-vs-disjuntos"><i class="fa fa-check"></i><b>1.4.4</b> Sucesos independientes vs disjuntos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html"><i class="fa fa-check"></i><b>2</b> Variables Aleatorias</a><ul>
<li class="chapter" data-level="2.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#introducción-a-las-variables-aleatorias"><i class="fa fa-check"></i><b>2.1</b> Introducción a las variables aleatorias</a><ul>
<li class="chapter" data-level="2.1.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#definición-de-variable-aleatoria"><i class="fa fa-check"></i><b>2.1.1</b> Definición de variable aleatoria</a></li>
<li class="chapter" data-level="2.1.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-2"><i class="fa fa-check"></i><b>2.1.2</b> Ejemplo</a></li>
<li class="chapter" data-level="2.1.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-3"><i class="fa fa-check"></i><b>2.1.3</b> Ejemplo</a></li>
<li class="chapter" data-level="2.1.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#tipos-de-variables-aleatorias"><i class="fa fa-check"></i><b>2.1.4</b> Tipos de variables aleatorias</a></li>
<li class="chapter" data-level="2.1.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-4"><i class="fa fa-check"></i><b>2.1.5</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#variables-aleatorias-discretas"><i class="fa fa-check"></i><b>2.2</b> Variables aleatorias discretas</a><ul>
<li class="chapter" data-level="2.2.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#distribuciones-de-probabilidad-para-v.a.-discretas."><i class="fa fa-check"></i><b>2.2.1</b> Distribuciones de probabilidad para v.a. discretas.</a></li>
<li class="chapter" data-level="2.2.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#función-de-probabilidad-para-variables-discretas"><i class="fa fa-check"></i><b>2.2.2</b> Función de probabilidad para variables discretas</a></li>
<li class="chapter" data-level="2.2.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#funcion-probabilidad-discretas"><i class="fa fa-check"></i><b>2.2.3</b> Funcion probabilidad discretas</a></li>
<li class="chapter" data-level="2.2.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-5"><i class="fa fa-check"></i><b>2.2.4</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-6"><i class="fa fa-check"></i><b>2.2.5</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2.6" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-7"><i class="fa fa-check"></i><b>2.2.6</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2.7" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-148"><i class="fa fa-check"></i><b>2.2.7</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2.8" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales"><i class="fa fa-check"></i><b>2.2.8</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.9" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-1"><i class="fa fa-check"></i><b>2.2.9</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.10" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-2"><i class="fa fa-check"></i><b>2.2.10</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.11" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-3"><i class="fa fa-check"></i><b>2.2.11</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.12" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-4"><i class="fa fa-check"></i><b>2.2.12</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.13" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-5"><i class="fa fa-check"></i><b>2.2.13</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.14" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-6"><i class="fa fa-check"></i><b>2.2.14</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.15" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-7"><i class="fa fa-check"></i><b>2.2.15</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.16" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-8"><i class="fa fa-check"></i><b>2.2.16</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.17" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#convergencia-de-los-momentos-muestrales-9"><i class="fa fa-check"></i><b>2.2.17</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="2.2.18" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#leyes-fuertes-de-los-grandes-números"><i class="fa fa-check"></i><b>2.2.18</b> Leyes fuertes de los grandes números</a></li>
<li class="chapter" data-level="2.2.19" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#leyes-fuertes-de-los-grandes-números-1"><i class="fa fa-check"></i><b>2.2.19</b> Leyes fuertes de los grandes números</a></li>
<li class="chapter" data-level="2.2.20" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#leyes-fuertes-de-los-grandes-números-2"><i class="fa fa-check"></i><b>2.2.20</b> Leyes fuertes de los grandes números</a></li>
<li class="chapter" data-level="2.2.21" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#leyes-fuertes-de-los-grandes-números-3"><i class="fa fa-check"></i><b>2.2.21</b> Leyes fuertes de los grandes números</a></li>
<li class="chapter" data-level="2.2.22" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-149"><i class="fa fa-check"></i><b>2.2.22</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2.23" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-150"><i class="fa fa-check"></i><b>2.2.23</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2.24" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-151"><i class="fa fa-check"></i><b>2.2.24</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-central-del-límite"><i class="fa fa-check"></i><b>2.3</b> Teorema Central del Límite</a><ul>
<li class="chapter" data-level="2.3.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#introducción-16"><i class="fa fa-check"></i><b>2.3.1</b> Introducción</a></li>
<li class="chapter" data-level="2.3.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#introducción-17"><i class="fa fa-check"></i><b>2.3.2</b> Introducción</a></li>
<li class="chapter" data-level="2.3.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-central-del-límite-1"><i class="fa fa-check"></i><b>2.3.3</b> Teorema Central del Límite</a></li>
<li class="chapter" data-level="2.3.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-central-del-límite-2"><i class="fa fa-check"></i><b>2.3.4</b> Teorema Central del Límite</a></li>
<li class="chapter" data-level="2.3.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-3"><i class="fa fa-check"></i><b>2.3.5</b> Demostración</a></li>
<li class="chapter" data-level="2.3.6" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-4"><i class="fa fa-check"></i><b>2.3.6</b> Demostración</a></li>
<li class="chapter" data-level="2.3.7" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-5"><i class="fa fa-check"></i><b>2.3.7</b> Demostración</a></li>
<li class="chapter" data-level="2.3.8" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-6"><i class="fa fa-check"></i><b>2.3.8</b> Demostración</a></li>
<li class="chapter" data-level="2.3.9" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-7"><i class="fa fa-check"></i><b>2.3.9</b> Demostración</a></li>
<li class="chapter" data-level="2.3.10" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-8"><i class="fa fa-check"></i><b>2.3.10</b> Demostración</a></li>
<li class="chapter" data-level="2.3.11" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-9"><i class="fa fa-check"></i><b>2.3.11</b> Demostración</a></li>
<li class="chapter" data-level="2.3.12" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#demostración-10"><i class="fa fa-check"></i><b>2.3.12</b> Demostración</a></li>
<li class="chapter" data-level="2.3.13" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-central-del-límite-en-la-práctica"><i class="fa fa-check"></i><b>2.3.13</b> Teorema Central del Límite en la práctica</a></li>
<li class="chapter" data-level="2.3.14" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-central-del-límite-en-la-práctica-1"><i class="fa fa-check"></i><b>2.3.14</b> Teorema Central del Límite en la práctica</a></li>
<li class="chapter" data-level="2.3.15" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-de-moivre-laplace"><i class="fa fa-check"></i><b>2.3.15</b> Teorema de Moivre-Laplace</a></li>
<li class="chapter" data-level="2.3.16" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-de-moivre-laplace-1"><i class="fa fa-check"></i><b>2.3.16</b> Teorema de Moivre-Laplace</a></li>
<li class="chapter" data-level="2.3.17" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#aproximación-de-una-suma-de-variables-poisson"><i class="fa fa-check"></i><b>2.3.17</b> Aproximación de una suma de variables Poisson</a></li>
<li class="chapter" data-level="2.3.18" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#aproximación-de-una-suma-de-variables-poisson-1"><i class="fa fa-check"></i><b>2.3.18</b> Aproximación de una suma de variables Poisson</a></li>
<li class="chapter" data-level="2.3.19" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#aproximación-de-una-suma-de-variables-poisson-2"><i class="fa fa-check"></i><b>2.3.19</b> Aproximación de una suma de variables Poisson</a></li>
<li class="chapter" data-level="2.3.20" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#aproximación-de-una-suma-de-variables-poisson-3"><i class="fa fa-check"></i><b>2.3.20</b> Aproximación de una suma de variables Poisson</a></li>
<li class="chapter" data-level="2.3.21" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-152"><i class="fa fa-check"></i><b>2.3.21</b> Ejemplo</a></li>
<li class="chapter" data-level="2.3.22" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejemplo-153"><i class="fa fa-check"></i><b>2.3.22</b> Ejemplo</a></li>
<li class="chapter" data-level="2.3.23" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#corrección-de-continuidad-de-fisher"><i class="fa fa-check"></i><b>2.3.23</b> Corrección de continuidad de Fisher</a></li>
<li class="chapter" data-level="2.3.24" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#corrección-de-continuidad-de-fisher-1"><i class="fa fa-check"></i><b>2.3.24</b> Corrección de continuidad de Fisher</a></li>
<li class="chapter" data-level="2.3.25" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#simulación-del-teorema-central-del-límite"><i class="fa fa-check"></i><b>2.3.25</b> Simulación del Teorema Central del Límite</a></li>
<li class="chapter" data-level="2.3.26" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#simulación-del-teorema-central-del-límite-1"><i class="fa fa-check"></i><b>2.3.26</b> Simulación del Teorema Central del Límite</a></li>
<li class="chapter" data-level="2.3.27" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#simulación-del-teorema-central-del-límite-2"><i class="fa fa-check"></i><b>2.3.27</b> Simulación del Teorema Central del Límite</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.udemy.com/course/probabilidad-y-variables-aleatorias-para-ml-con-r-y-python/?couponCode=B85F8D52148DF5AAD8F7" target="blank">Curso en Udemy</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probabilidad y variables aleatorias para ML con R y Python</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variables-aleatorias" class="section level1">
<h1><span class="header-section-number">Tema 2</span> Variables Aleatorias</h1>
<div id="introducción-a-las-variables-aleatorias" class="section level2">
<h2><span class="header-section-number">2.1</span> Introducción a las variables aleatorias</h2>
<ul>
<li>Hasta ahora nuestros sucesos han sido de varios tipos: <span class="math inline">\(\{C,+\}\)</span> en
la moneda, nombres de periódicos, ángulos en una ruleta, número de
veces que sale cara en el lanzamiento de una moneda etc.</li>
<li>Necesitamos estandarizar de alguna manera todos estos sucesos. Una
solución es asignar a cada suceso un cierto conjunto de
números reales, es decir, convertir todos los sucesos en
<em>sucesos de números reales</em> para trabajar con ellos de forma
unificada.</li>
<li>Para conseguirlo utilizaremos unas funciones que
transformen los elementos del espacio muestral en números; esta funciones son las
variables aleatorias.</li>
</ul>
<div id="definición-de-variable-aleatoria" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Definición de variable aleatoria</h3>
<p>Comenzaremos dando una definición poco rigurosa, pero suficiente, de variable aleatoria.</p>
<div class="definition">
<em>Variable Aleatoria (definición práctica)</em>
</div>
<p>Una variable aleatoria (v.a.) es una aplicación que toma valores numéricos determinados por el resultado de un experimento aleatorio</p>
<div class="observ">
<p><strong>Notación</strong>:</p>
</div>
<ul>
<li>Normalmente representaremos las v.a. por letras mayúsculas <span class="math inline">\(X,Y,Z\)</span></li>
<li>Los valores que “<em>toman</em>” las v.a. los representaremos por letras minúsculas (las mismas en principio) <span class="math inline">\(x,y,z\ldots\)</span></li>
</ul>
</div>
<div id="ejemplo-2" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Lanzamos un dado convencional de parchís el espacio muestral del experimento es</p>
<p><span class="math display">\[\Omega=\{1,2, 3, 4,  5, 6\}\]</span></p>
</div>
<div class="example-sol">
<p>Una v.a <span class="math inline">\(X:\Omega\to\mathbb{R}\)</span>
sobre este espacio queda definida por</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
X(1)&amp;=1,X(2)=2,X(3)=3,\\
X(4)&amp;=4,X(5)=5,X(6)=6.
\end{split}
\end{equation*}\]</span></p>
<ul>
<li>Ahora el suceso <span class="math inline">\(A=\{2, 4, 6\}\)</span>, es decir “salir
número par”, es equivalente a <span class="math inline">\(\{X=2,X=4,X=6\}\)</span>.</li>
<li>El suceso <span class="math inline">\(B=\{1,2,3\}\)</span>, es decir “salir un número
inferior o igual a <span class="math inline">\(3\)</span>” es en términos de la v.a. <span class="math inline">\(\{X=1,X=2,X=3\}\)</span> o también <span class="math inline">\(\{X\leq 3\}\)</span>.</li>
</ul>
</div>
</div>
<div id="ejemplo-3" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos el experimento lanzar una anilla al cuello de una botella. Si acertamos a
ensartar la anilla en la botella el resultado del experimento es <em>éxito</em> y
<em>fracaso</em> en caso contrario.</p>
</div>
<div class="example-sol">
<p>El espacio muestral asociado a este experimento será
<span class="math inline">\(\Omega=\{\mbox{éxito, fracaso}\}\)</span>. Construyamos la siguiente variable aleatoria:</p>
<p><span class="math display">\[X:\{\mbox{éxito, fracaso}\}\to\mathbb{R}\]</span></p>
<p>definida por</p>
<p><span class="math display">\[X(\mbox{éxito})=1 \mbox{ y } X(\mbox{fracaso})=0.\]</span></p>
</div>
</div>
<div id="tipos-de-variables-aleatorias" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Tipos de variables aleatorias</h3>
<p>Hay dos tipos fundamentales de variables aleatorias, las discretas y las continuas.</p>
<p>Damos a continuación una definición informal.</p>
<p><l class="definition">Variables Aleatórias Discretas y Continuas </l></p>
<ul>
<li>Una variable aleatoria es <strong>discreta</strong> si sólo puede tomar una cantidad numerable de valores con probabilidad positiva.</li>
<li>Las variables aleatorias <strong>continuas</strong> toman valores en intervalos.</li>
<li>También existen las variables aleatorias <strong>mixtas</strong>; con una parte discreta y otra continua.</li>
</ul>
</div>
<div id="ejemplo-4" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Son variables <em>aleatorias discretas</em>:</p>
<ul>
<li>Número de artículos defectuosos en un cargamento.</li>
<li>Número de clientes que llegan a una ventanilla de un banco en una hora.</li>
<li>Número de errores detectados en las cuentas de una compañía.</li>
<li>Número de reclamaciones de una póliza de un seguro médico.</li>
</ul>
<p>Son variables <em>aleatorias continuas</em>:</p>
<ul>
<li>Renta anual de una familia.</li>
<li>Cantidad de petróleo importado por un país</li>
<li>Variación del precio de las acciones de una compañía de telecomunicaciones.</li>
<li>Porcentaje de impurezas en un lote de productos químicos.</li>
</ul>
</div>
</div>
</div>
<div id="variables-aleatorias-discretas" class="section level2">
<h2><span class="header-section-number">2.2</span> Variables aleatorias discretas</h2>
<div id="distribuciones-de-probabilidad-para-v.a.-discretas." class="section level3">
<h3><span class="header-section-number">2.2.1</span> Distribuciones de probabilidad para v.a. discretas.</h3>
<ul>
<li>Pasamos ahora a describir el comportamiento de la v.a.
Para ello utilizaremos distintas
funciones que nos darán algunas probabilidades de la variable aleatoria.</li>
<li>En el caso discreto estas funciones son la de probabilidad, y la función de distribución o de probabilidad acumulada.</li>
<li>En el caso discreto la función de probabilidad es la que nos da las probabilidades de los
sucesos elementales de la v.a. que definimos a continuación.</li>
</ul>
</div>
<div id="función-de-probabilidad-para-variables-discretas" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Función de probabilidad para variables discretas</h3>
<p><l class="definition"> <strong>Función de Probabilidad</strong></l>
La <em>función de probabilidad</em> (<em>probability mass function</em> o incluso abusando de notación <em>probability density function</em>)</p>
<p>de una variable aleatoria discreta <span class="math inline">\(X\)</span> a la que denotaremos por <span class="math inline">\(P_{X}(x)\)</span>
está definida por</p>
<p><span class="math display">\[P_{X}(x)=P(X=x)\]</span></p>
<p>es decir la probabilidad de que <span class="math inline">\(X\)</span> tome el valor <span class="math inline">\(x\)</span>.</p>
Si <span class="math inline">\(X\)</span> no asume ese valor <span class="math inline">\(x\)</span>, entonces
<span class="math inline">\(P_{X}(x)=0\)</span>.
</div>
</div>
<div id="funcion-probabilidad-discretas" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Funcion probabilidad discretas</h3>
<p><l class="definition"> <strong>Dominio de una variable aleatoria discreta</strong> </l></p>
<p>El conjunto <span class="math display">\[D_X=\{ x\in\mathbb{R} \mid P_X(x)&gt;0\}\]</span> recibe el nombre de
<em>dominio</em> de la v.a. y son los valores posibles de esta variable.</p>
<p>En el caso discreto lo más habitual es que <span class="math inline">\(X(\Omega)=D_X\)</span>.</p>
</div>
<div id="ejemplo-5" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo parchís</strong></p>
<p>Lanzamos un dado de parchís una vez, en esta ocasión representaremos los
sucesos elementales por el número de puntos de la cara obtenida, tenemos que
<span class="math display">\[\Omega=\{\mbox{1-puntos,2-puntos,3-puntos,4-puntos,5-puntos,6-puntos}\}\]</span>
y la variables aleatoria <span class="math inline">\(X:\Omega\to \mathbb{R}\)</span> viene definida por</p>
<p><span class="math display">\[X(\mbox{i-puntos})=i\mbox{ para } i=1,2,3,4,5,6.\]</span></p>
</div>
<div class="example-sol">
<p>Supongamos que el dado está bien balanceado. Entonces
<span class="math display">\[P_{X}(1)=P_{X}(2)=P_{X}(3)=P_{X}(4)=P_{X}(5)=P_{X}(6)=\frac16\]</span>
Concretamente:
<span class="math display">\[
P_{X}(x)=
  \left\{
  \begin{array}{ll}
   \frac16 &amp; \mbox{si } x=1,2,3,4,5,6\\
  0 &amp; \mbox{en otro caso }
  \end{array}
  \right.
\]</span></p>
<p>Su dominio es <span class="math display">\[D_X=\{1,2,3,4,5,6\}.\]</span></p>
</div>
</div>
<div id="ejemplo-6" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo lanzamiento moneda</strong></p>
<p>Sea <span class="math inline">\(X\)</span> la v.a. asociada al lanzamiento de una moneda. Su espacio muestral es <span class="math inline">\(\Omega=\{c,+\}\)</span>, la v.a. queda definida por:</p>
<p><span class="math display">\[X(\omega)=\left\{\begin{array}{ll} 1 &amp; \mbox{si } \omega=c \\
0 &amp; \mbox{si }\omega=+\end{array}\right.\]</span></p>
</div>
<div class="example-sol">

<p>Su función de probabilidad es:</p>
<p><span class="math display">\[P_{X}(x)=P(X=x)=\left\{\begin{array}{ll} \frac12 &amp; \mbox{si } x=0,1\\
0 &amp; \mbox{en otro caso}\end{array}\right.\]</span></p>
<p>Finalmente su dominio es <span class="math inline">\(D_X=\{0,1\}.\)</span></p>
</div>
<div id="ejemplo-7" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo urna con bolas</strong></p>
<p>Tenemos una urna con tres bolas rojas, una negra y dos blancas. Realizamos una extracción y observamos el color de la bola entonces un espacio muestral es
<span class="math display">\[\Omega=\{roja, blanca, negra\}.\]</span></p>
</div>
<div class="example-sol">
<p>Una variable aleatoria asociada al experimento es:</p>
<p><span class="math display">\[X(\omega)=\left\{\begin{array}{ll} 1 &amp; \mbox{si } \omega=roja  \\
2 &amp; \mbox{si }\omega=negra \\ 3 &amp; \mbox{si } \omega=blanca \end{array}\right.\]</span></p>
<h3 id="ejemplo-8"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example-sol">
<p>La función de probabilidad es</p>
<p><span class="math display">\[P_{X}(x)=\left\{\begin{array}{ll} \frac36 &amp; \mbox{si } x=1\\
\frac16 &amp; \mbox{si } x=2\\ \frac26 &amp; \mbox{si } x=3\\ 0 &amp; \mbox{en otro
caso}\end{array}\right.\]</span></p>
<p>El dominio de la v.a. <span class="math inline">\(X\)</span> es <span class="math inline">\(D_X=\{1,2,3\}.\)</span></p>
</div>
<h3 id="propiedades-de-la-función-de-probabilidad."><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad.</h3>
<p><l class="prop"> Propiedades básicas de la función de porbabilidad </l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. discreta <span class="math inline">\(X:\Omega:\to\mathbb{R}\)</span> con dominio <span class="math inline">\(D_X\)</span>. Su función de probabilidad <span class="math inline">\(P_{X}\)</span> verifica las siguientes propiedades:</p>
<ul>
<li><span class="math inline">\(0\leq P_{X}(x)\leq 1\)</span> para todo <span class="math inline">\(x\in\mathbb{R}\)</span></li>
<li><span class="math inline">\(\sum\limits_{x\in D_X} P_{X}(x)=1\)</span></li>
</ul>
<h3 id="ejemplo-9"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: Lanzamiento moneda</strong></p>
<p>Lanzamos al aire tres veces, de forma independiente, una moneda perfecta. El espacio muestral de este experimento es
<span class="math display">\[\Omega=\{ccc,cc+,c+c,+cc,c++,+c+,++c,+++\}\]</span> (expresados en orden de aparición).</p>
</div>
<div class="example-sol">
<p>Este espacio tiene todos los sucesos elementales
equiprobables.</p>
<p>Consideremos la variable aleatoria asociada a este experimento:</p>
<p><span class="math display">\[X=\mbox{ número de caras en los tres lanzamientos}.\]</span></p>
<p>Su función de probabilidad es:</p>
<p><span class="math display">\[
\begin{array}{l}
P(X=0)=P(\{+++\})=\frac18\\ P(X=1)=P(\{c++,+c+,++c\})=\frac38\\
    P(X=2)=P(\{cc+,c+c,+cc\})=\frac38\\
    P(X=3)=P(\{ccc\})=\frac18
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-10"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example-sol">
<p>Podemos reescribir la función de probabilidad de <span class="math inline">\(X\)</span> de forma simplificada:</p>
<p><span class="math display">\[P_{X}(x)=\left\{\begin{array}{ll} \frac18 &amp; \mbox{si } x=0, 3\\
\frac38 &amp; \mbox{si } x=1,2\\ 0 &amp; \mbox{en otro caso}\end{array}\right.\]</span></p>
<p>Efectivamente los valores de la función de distribución suman 1</p>
<p><span class="math display">\[\sum_{x=0}^3 P_X(x)= \frac18+\frac38+\frac38+\frac18=1.\]</span></p>
</div>
<h3 id="función-de-distribución-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Función de distribución de variables aleatorias</h3>
<p><l class="definition"> <strong>Distribución de Probabilidad</strong></l></p>
<p>La función de <em>distribución de probabilidad</em> (acumulada) de la v.a. <span class="math inline">\(X\)</span> (de cualquier tipo;
discreta o continua) <span class="math inline">\(F_{X}(x)\)</span> representa la probabilidad de que <span class="math inline">\(X\)</span> tome un menor o igual que <span class="math inline">\(x\)</span> es decir</p>
<p><span class="math display">\[F_{X}(x)=P(X\leq x)\]</span></p>
<p>Esta función también se denomina función de <em>distribución de
probabilidad o simplemente función de distribución</em> de una v.a., y en inglés
<em>cumulative distribution function</em> por lo que se abrevia con el acrónimo <code>cdf</code>.</p>
<h3 id="propiedades-12"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p><l class="definition"> <strong>Propiedades de la Función de Distribución</strong></l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. y <span class="math inline">\(F_{X}\)</span> su función
de distribución:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P(X&gt;x)=1-P(X\leq x)=1-F_{X}(x)\)</span></li>
<li>Sea a y b tales que <span class="math inline">\(a&lt;b\)</span>, <span class="math inline">\(P(a&lt;X\leq b)=P(X\leq b)-P(X\leq a)=F_{X}(b)-F_{X}(a)\)</span></li>
</ol>
<h3 id="propiedades-13"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<div class="dem">
<p><strong>Demostración</strong>:</p>
<p>Tenemos que el complementario de <span class="math inline">\(X\)</span> mayor que <span class="math inline">\(x\)</span> es: <span class="math inline">\(\overline{\left\{X&gt;x\right\}}=\left\{X&gt;x\right\}^c=\left\{X\leq x\right\}\)</span>. Además</p>
<p><span class="math display">\[P(X&gt;x)=1-P(\overline{\left\{X&gt;x\right\}})=1-P(X\leq x)=1-F_{X}(x)\]</span></p>
<p>lo que demuestra la primera propiedad</p>
<p>Por otro lado, que <span class="math inline">\(X\)</span> se encuentre entre dos valores <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> es <span class="math inline">\(\left\{a&lt; X \leq b\right\}= \left\{X\leq b\right\}-\left\{X\leq a\right\}\)</span></p>
<p>ahora podemos hacer</p>
<p><span class="math display">\[\begin{eqnarray*}
P(a&lt;X\leq b)&amp;=&amp;P(\left\{X\leq b\right\}-\left\{X\leq a\right\})\\
&amp;=&amp; P(\left\{X\leq b\right\})-P(\left\{X\leq a\right\})\\
&amp;=&amp; F_{X}(b)-F_{X}(a).
\end{eqnarray*}\]</span></p>
</div>
<h3 id="propiedades-14"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p><l class="definition"> <strong>Propiedades de la Función de Distribución</strong></l></p>
<p>Sea <span class="math inline">\(F_{X}\)</span> la función de distribución de una v.a. <span class="math inline">\(X\)</span> entonces:</p>
<ul>
<li><span class="math inline">\(0\leq F_{X}(x)\leq 1\)</span>.</li>
<li>La función <span class="math inline">\(F_{X}\)</span> es no decreciente.</li>
<li>La función <span class="math inline">\(F_{X}\)</span> es continua por la derecha.</li>
<li>Si denotamos por <span class="math inline">\(F_X(x_0^{-})=\displaystyle \lim_{x\to x_0^{-}} F(x)\)</span>,
entonces se cumple que <span class="math inline">\(P(X&lt; x_0)=F_X(x_0^{-})\)</span> y que <span class="math inline">\(P(X=x_0)=F_X(x_0)-F_X(x_0^{-})\)</span>.</li>
</ul>
<h3 id="propiedades-15"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p><l class="definition"> <strong>Propiedades de la Función de Distribución</strong></l></p>
<ul>
<li>Se cumple que <span class="math inline">\(\displaystyle \lim_{x\to\infty} F_{X}(x)=1\)</span>; <span class="math inline">\(\displaystyle \lim_{x\to-\infty}F_{X}(x)=0\)</span>.</li>
<li>Toda función <span class="math inline">\(F\)</span> verificando las propiedades anteriores es función de distribución de alguna v.a. <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(P(X&gt;x)=1-F_{X}(x)\)</span></li>
<li>Dados <span class="math inline">\(a,b\in \mathbb{R}\)</span> con <span class="math inline">\(a&lt;b\)</span> <span class="math display">\[P(a&lt;X\leq b)=F_{X}(b)-F_{X}(a).\]</span></li>
</ul>
<h3 id="advertencia-desigualdades-estrictas"><span class="header-section-number">2.2.6</span> Advertencia desigualdades estrictas</h3>
<p>En las propiedades anteriores no se pueden cambiar en general las desigualdades de
estrictas o no estrictas.</p>
<p>Veamos que propiedades tenemos cuando se cambian estas
desigualdades.</p>
<p>Dada una <span class="math inline">\(F_{X}\)</span> una función de distribución de la v.a. <span class="math inline">\(X\)</span> y denotamos por <span class="math display">\[F_{X}(x_0^{-})=\displaystyle \lim_{x\to x_0^{-}} F_{X}(x),\]</span>,</p>
<p>entonces se cumplen las siguientes igualdades…</p>
<h3 id="advertencia-desigualdades-estrictas-1"><span class="header-section-number">2.2.6</span> Advertencia desigualdades estrictas</h3>
<p><l class="prop">Propiedades </l></p>
<ul>
<li><span class="math inline">\(P(X=x)=F_{X}(x)-F_{X}(x^{-})\)</span></li>
<li><span class="math inline">\(P(a&lt; X&lt; b)=F_{X}(b^{-})-F_{X}(a)\)</span></li>
<li><span class="math inline">\(P(a\leq X&lt; b)=F_{X}(b^{-})-F_{X}(a^{-})\)</span></li>
<li><span class="math inline">\(P(X&lt;a)=F_{X}(a^{-})\)</span></li>
<li><span class="math inline">\(P(a\leq X\leq b)=F_{X}(b)-F_{X}(a^{-})\)</span></li>
<li><span class="math inline">\(P(X\geq a)=1-F_{X}(a^{-})\)</span></li>
</ul>
<h3 id="propiedades-16"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p><l class="prop">Más propiedades de la función de distribución </l></p>
<ul>
<li>Si <span class="math inline">\(F_X\)</span> es continua en <span class="math inline">\(x\)</span> se tiene que <span class="math inline">\(P(X=x)=0\)</span>.
Así que si la v.a. es continua <span class="math inline">\(P(X\leq a)=P(X&lt; a)+P(X=a)=P(X&lt;a)\)</span> y propiedades similares.</li>
<li>Sea <span class="math inline">\(X\)</span> una variable aleatoria discreta que con dominio <span class="math inline">\(D_X\)</span> y
que tiene por función de probabilidad <span class="math inline">\(P_{X}(x)\)</span> entonces su función de distribución
<span class="math inline">\(F_{X}(x_0)\)</span> es
<span class="math display">\[F_{X}(x_0)=\sum_{x\leq x_0} P_{X}(x)\]</span></li>
</ul>
<p>donde <span class="math inline">\(\sum_{x\leq x_0}\)</span> indica que sumamos todos los <span class="math inline">\(x \in D_X\)</span> tales que <span class="math inline">\(x\leq x_0\)</span></p>
<h3 id="propiedades-17"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<div class="dem">
<p><strong>Demostración</strong>:</p>
<p>Si <span class="math inline">\(X\)</span> es continua <span class="math display">\[P(X=a)=F(a)-F(a^{-})=F(a)-F(a)=0\]</span>
por lo tanto</p>
<p><span class="math display">\[P(X\leq a)=P(X&lt;a)+P(X=a)= P(X&lt;a)+0= P(X&lt;a).\]</span></p>
<p>lo que demuestra la primera propiedad.</p>
<p>Para demostrar la segunda basta hacer</p>
<p><span class="math display">\[ 
F_{X}(x_0)= P(X\leq x_0)=P\left(\bigcup_{x\leq
x_0; x\in D_X} \{x\}\right)= \sum_{x\leq x_0}P(X=x)= \sum_{x\leq x_0}P_{X}(x).
\]</span></p>
</div>
<h3 id="ejemplo-11"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>En el experimento del dado se tiene que:</p>
<p><span class="math display">\[P_{X}(x)=\left\{\begin{array}{ll} \frac16 &amp; \mbox{si } x=1,2,3,4,5,6\\ 0 &amp; \mbox{en el resto de casos}\end{array}\right.,\]</span></p>
<p>por lo tanto</p>
<p><span class="math display">\[F_{X}(x)=P(X\leq x)=\left\{\begin{array}{ll}
   0 &amp; \mbox{si } x&lt;1\\
   \frac16 &amp;\mbox{si } 1\leq x&lt;2\\
   \frac26 &amp;\mbox{si } 2\leq x&lt;3\\
   \frac36 &amp;\mbox{si } 3\leq x&lt;4\\
   \frac46 &amp;\mbox{si } 4\leq x&lt;5\\
   \frac56 &amp;\mbox{si } 5\leq x&lt;6\\
   1 &amp;\mbox{si } 6\leq x\end{array}\right.\]</span></p>
</div>
<h3 id="ejemplo-12"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example-sol">
<p>Calculemos más detalladamente algún valor de <span class="math inline">\(F_{X}\)</span>, por ejemplo:</p>
<p><span class="math display">\[\begin{eqnarray*}
F_{X}(3.5) &amp; = &amp; P(X\leq 3.5)=  P(\{X=1\}\cup\{X=2\}\cup \{X=3\})\\
&amp;=&amp; P(\{X=1\})+P(\{X=2\})+P(\{X=3\})\\
&amp;=&amp; \frac16+\frac16+\frac16=\frac36 =\frac12,
\end{eqnarray*}\]</span></p>
<p>o de otra forma</p>
<p><span class="math display">\[\begin{eqnarray*}
F_{X}(3.5)&amp;=&amp;\sum_{x\leq 3.5} P_X(x)=\sum_{x=1}^3 P(X=x)\\&amp;=&amp;\sum_{x=1}^3 \frac16= 3 \cdot
   \frac16=\frac12.
\end{eqnarray*}\]</span></p>
</div>
<h3 id="propiedades-de-la-función-de-distribución"><span class="header-section-number">2.2.6</span> Propiedades de la función de distribución</h3>
<p><l class="prop"> Propiedad</l></p>
<p>Sea <span class="math inline">\(X\)</span> una variable con función de distribución <span class="math inline">\(F_{X}\)</span> entonces:</p>
<ul>
<li><span class="math inline">\(0\leq F_{X}(x)\leq 1\)</span> para todo <span class="math inline">\(x\)</span></li>
<li>Si <span class="math inline">\(x&lt;x&#39;\)</span> entonces <span class="math display">\[F_{X}(x)\leq F_{X}(x&#39;).\]</span>
Es una función creciente, no necesariamente estrictamente creciente.</li>
<li><span class="math inline">\(\displaystyle \lim_{x\to -\infty}F_{X}(x)=0\)</span> y <span class="math inline">\(\displaystyle \lim_{x\to +\infty}F_{X}(x)=1\)</span></li>
<li>Es continua por la derecha <span class="math inline">\(\displaystyle \lim_{x\to x_0^{+}}F_{X}(x)=F_{X}(x_0)\)</span>.</li>
</ul>
<h2 id="valores-esperados-o-esperanza"><span class="header-section-number">2.2.6</span> Valores esperados o esperanza</h2>
<h3 id="momentos-de-variables-aleatorias-discretas"><span class="header-section-number">2.2.6</span> Momentos de variables aleatorias discretas</h3>
<ul>
<li><p>Al igual que en la estadística descriptiva se utilizan distintas medidas para
resumir los valores centrales y para medir la dispersión de una muestra, podemos definir
las correspondiente medidas para variables aleatorias.</p></li>
<li><p>A estas medidas se les suele añadir el adjetivo <em>poblacionales</em> mientras que a las que provienen de la muestra se las adjetiva como <em>muestrales</em>.</p></li>
</ul>
<p>Por ejemplo podemos buscar un valor que resuma toda la variable. Este valor es el que “<em>esperamos</em>” que se resuma la v.a. o esperamos que las realizaciones de la v.a. queden cerca de él. Veamos su definición formal.</p>
<h3 id="esperanza-de-un-variable-aleatoria-discreta"><span class="header-section-number">2.2.6</span> Esperanza de un variable aleatoria discreta</h3>
<p><l class="definition">Esperanza de una variable aleatoria discreta </l></p>
<p>El valor <em>esperado o esperanza</em> (<em>expected value</em> en inglés) <span class="math inline">\(E(X)\)</span> de una v.a. discreta <span class="math inline">\(X\)</span>, se define como</p>
<p><span class="math display">\[
E(X)=\sum_{x\in X(\Omega)} x P_{X}(x)
\]</span></p>
<p>En ocasiones se le domina <em>media</em> (<em>mean</em> en inglés <em>mitjana</em> en catalán) poblacional o simplemente media y muy frecuentemente se la denota <span class="math inline">\(\mu_{X}=E(X)\)</span> o simplemente <span class="math inline">\(\mu=E(X)\)</span>.</p>
<h3 id="interpretación-de-la-media-aritmética-para-v.a.-discretas"><span class="header-section-number">2.2.6</span> Interpretación de la media aritmética para v.a. discretas</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Supongamos que lanzamos un dado <span class="math inline">\(n\)</span> veces y obtenemos unas frecuencias absolutas <span class="math inline">\(n_{i}\)</span>
para el resultado <span class="math inline">\(i\)</span> con <span class="math inline">\(i=1,\ldots,6\)</span>. Sea <span class="math inline">\(X\)</span> la v.a. que nos representa el valor de
una tirada del dado.</p>
<p>Calculemos la media aritmética (o media muestral) de los datos</p>
<p><span class="math display">\[
\overline{x}=\frac{1\cdot n_1+2\cdot  n_2+3\cdot  n_3+4\cdot  n_4+5\cdot  n_5+6 \cdot 
n_6}{n}=\sum_{x=1}^6 x \frac{n_{x}}{n}.
\]</span></p>
<p>Si <span class="math inline">\(n\to \infty\)</span> se tiene que <span class="math inline">\(\displaystyle\lim_{n\to \infty} \frac{n_{x}}{n}=P_{X}(x).\)</span></p>
<p>Por lo tanto <span class="math inline">\(E(X)=\displaystyle \lim_{n\to\infty}\sum_{x=1}^6x \frac{n_{x}}{n}.\)</span></p>
<p>Entonces el valor esperado en una v.a. discreta puede entenderse como el valor promedio que
tomaría una v.a. en un número grande de repeticiones.</p>
</div>
<h3 id="ejemplo-13"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: Erratas en un texto</strong></p>
<p>Sea <span class="math inline">\(X\)</span>= número de erratas en una página de un texto con dominio <span class="math inline">\(D_X=\{0,1,2\}\)</span>.</p>
<p>Resulta que</p>
<p><span class="math display">\[
P(X=0)=0.42, P(X=1)=0.4, P(X=2)=0.18.
\]</span></p>
<p>entonces</p>
<p><span class="math display">\[
E(X)=0\cdot 0.42+ 1\cdot 0.4 + 2 \cdot 0.18=0.76.
\]</span></p>
<p>Elegida una página del texto al azar esperamos encontrar <span class="math inline">\(0.76\)</span> errores por página.</p>
<p>Supongamos que en el editor nos paga <span class="math inline">\(2\)</span> euros por cada página que
encontremos con <span class="math inline">\(1\)</span> error y <span class="math inline">\(3\)</span> euros por cada página con dos errores (y nada por las
páginas correctas) ¿Cuánto esperamos cobrar si analizamos una página?</p>
<p><span class="math display">\[0\cdot 0.42 + 2\cdot 0.4 + 3\cdot 0.18=1.34\]</span></p>
</div>
<h3 id="esperanzas-de-funciones-de-variables-aleatorias-discretas"><span class="header-section-number">2.2.6</span> Esperanzas de funciones de variables aleatorias discretas</h3>
<p><l class="definition"> <strong>Esperanzas de funciones de variables aleatorias discretas</strong> </l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. discreta con función de probabilidad <span class="math inline">\(P_{X}\)</span> y de distribución
<span class="math inline">\(F_{X}\)</span>. Entonces el <em>valor esperado de una función</em> <span class="math inline">\(g(x)\)</span> es :</p>
<p><span class="math display">\[E(g(X))=\sum_{x}g(x) P_{X}(x).\]</span></p>
<h3 id="propiedades-de-los-valores-esperados"><span class="header-section-number">2.2.6</span> Propiedades de los valores esperados</h3>
<p><l class="prop"> Propiedades</l></p>
<ul>
<li><span class="math inline">\(E(k)=k\)</span> para cualquier constante <span class="math inline">\(k\)</span>.</li>
<li>Si <span class="math inline">\(a\leq X\leq b\)</span> entonces <span class="math inline">\(a\leq E(X)\leq b\)</span>.</li>
<li>Si <span class="math inline">\(X\)</span> es una v.a. discreta que toma valores enteros no negativos entonces
<span class="math inline">\(E(X)=\sum_{x=0}^{+\infty}(1- F_X(x)).\)</span></li>
</ul>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>La demostración de las propiedades anteriores se deja como ejercicio.</p>
</div>
<h3 id="ejemplo-14"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: paleta de colores aleatoria</strong></p>
<p>Supongamos que estamos sentados delante de nuestro ordenador con un amigo y
le decimos que en dos minutos podemos programar una paleta para poner colores a unos
gráficos.</p>
<p>Queremos la que paleta tenga dos botones con las opciones color rojo y color azul.
Como hemos programado a gran velocidad resulta que el programa tiene un error; cada vez que
se abre la paleta los colores se colocan al azar (con igual probabilidad) en cada botón,
así que no sabemos en que color hemos de pinchar.</p>
<p>Además, como nos sobraron <span class="math inline">\(15\)</span> segundos
para hacer el programa y pensando en la comodidad del usuario, la paleta se cierra después de haber seleccionado un
color y hay que volverla a abrir de nuevo.</p>
<p>La pregunta es ¿cuál es el valor esperado del
número de veces que hemos pinchar el botón de color azul antes de obtener este color?</p>
</div>
<h3 id="ejemplo-15"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example-sol">
<p>Llamemos <span class="math inline">\(X\)</span> al número de veces que pinchamos en el botón azul (y nos sale rojo) hasta
obtener el primer azul. La variable <span class="math inline">\(X\)</span> toma valores en los enteros no negativos. Su
función de probabilidad queda determinada por</p>
<p><span class="math display">\[
P_X(x)=P(X=x)=P(\stackrel{x \mbox{veces}}{\overbrace{rojo, rojo,\ldots,rojo},azul})
=\left(\frac12\right)^{x+1}.
\]</span></p>
</div>
<h3 id="propiedades-de-las-series-geométricas"><span class="header-section-number">2.2.6</span> Propiedades de las series geométricas</h3>
<p><l class="prop">Series geométricas</l></p>
<ul>
<li>Una <em>progresión geométrica</em> de razón <span class="math inline">\(r\)</span> es una sucesión de la forma<br />
<span class="math display">\[
r^0, r^1,\ldots,r^n,\ldots.
\]</span></li>
<li>La serie geométrica es la suma de todos los
valores de la progresión geométrica <span class="math inline">\(\displaystyle\sum_{k=0}^{+\infty} r^k\)</span>.</li>
<li>Las sumas parciales desde el término <span class="math inline">\(n_0\)</span> al <span class="math inline">\(n\)</span> de una progresión geométrica valen
<span class="math display">\[
\sum_{k=n_0}^n r^k=\frac{r^{n_0}- r^n r}{1-r}.
\]</span></li>
</ul>
<h3 id="propiedades-de-las-series-geométricas-1"><span class="header-section-number">2.2.6</span> Propiedades de las series geométricas</h3>
<p><l class="prop">Propiedades</l></p>
<ul>
<li>Si <span class="math inline">\(|r|&lt;1\)</span> la serie geométrica es convergente y <span class="math display">\[\sum_{k=0}^{+\infty }
r^k=\frac1{1-r}\]</span>.</li>
<li>En el caso en que se comience en <span class="math inline">\(n_0\)</span> se tiene que
<span class="math display">\[\sum_{k=n_0}^{+\infty} r^k=\frac{r^{n_0}}{1-r}.\]</span></li>
</ul>
<h3 id="propiedades-de-las-series-geométricas-2"><span class="header-section-number">2.2.6</span> Propiedades de las series geométricas</h3>
<p><l class="prop">Propiedades</l></p>
<ul>
<li>Si <span class="math inline">\(|r|&lt;1\)</span> también son convergentes las derivadas, respecto de <span class="math inline">\(r\)</span>, de la serie geométrica y convergen a la derivada correspondiente. Así tenemos que</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
\left(\sum_{k=0}^{+\infty} r^k\right)&#39;= &amp; \sum_{k=1}^{+\infty}k
r^{k-1} \left(\frac1{1-r}\right)&#39;=\frac1{(1-r)^2}\\
\left(\sum_{k=0}^{+\infty} r^k\right)^{&#39;&#39;}=&amp; \sum_{k=2}^{+\infty}k (k-1)
r^{k-2}\left(\frac1{1-r}\right)^{&#39;&#39;}=\frac2{(1-r)^3}
\end{eqnarray*}\]</span>.</p>
<h3 id="ejemplo-16"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo (cont)</strong></p>
<p>Si seguimos con el ejemplo de la paleta de colores, su esperanza es:</p>
<p><span class="math display">\[\begin{eqnarray*}
E(X)&amp;=&amp;\sum_{x=0}^{+\infty} x P(X=x)=\sum_{x=0}^{+\infty} x
\left(\frac12\right)^{x+1}\\
&amp;= &amp; \left(\frac12\right)^2\sum_{x=1}^{+\infty} x
\left(\frac12\right)^{x-1}=\left(\frac12\right)^2
\frac1{\left(1-\frac12\right)^2}=1.
\end{eqnarray*}\]</span></p>
<p>Ahora calculemos su función de distribución</p>
<p><span class="math display">\[\begin{eqnarray*}
F_X(x)&amp;=&amp; P(X\leq x)=\sum_{k=0}^x P(X=k)=\sum_{k=0}^x
\left(\frac12\right)^{k+1}\\
&amp;=&amp; \frac{\frac12-\frac12^{x+1}
\frac12}{1-\frac12}=1-\left(\frac12\right)^{x+1}
\end{eqnarray*}\]</span>.</p>
</div>
<h3 id="ejemplo-17"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Como la variable toma valores enteros positivos, podemos calcular su valor esperado
de esta otra manera</p>
<p><span class="math display">\[E(X)=\sum_{x=0}^{+\infty} (1-F_X(x))=\sum_{x=0}^{+\infty}(\frac12)^{x+1}=\frac12
\frac1{1-\frac12}=1.\]</span></p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Calculad el valor esperado de la variable</p>
<p><span class="math display">\[
Y=\mbox{número de intentos para conseguir el color azul.}
\]</span></p>
</div>
<h3 id="momentos-de-una-variable-aleatoria"><span class="header-section-number">2.2.6</span> Momentos de una variable aleatoria</h3>
<div class="definition">
Momentos de orden <span class="math inline">\(m\)</span>
</div>
<p>Llamaremos <em>momento de orden <span class="math inline">\(m\)</span></em> respecto al punto <span class="math inline">\(C\)</span> a
<span class="math display">\[E\left((X-C)^m\right)\]</span></p>
<ul>
<li>Cuando <span class="math inline">\(C=0\)</span> los momentos reciben el nombre de <em>momentos respecto al origen</em>.</li>
<li>Cuando <span class="math inline">\(C=E(X)\)</span> reciben el nombre de <em>momentos centrales o respecto de la media</em>. Luego la esperanza es el momento de orden <span class="math inline">\(1\)</span> respecto al origen. Estos momentos son la versión poblacional de los momentos que vimos en el curso de estadística descriptiva, recibiendo estos último el nombre de momentos muestrales.</li>
</ul>
<h3 id="resumen-conceptos"><span class="header-section-number">2.2.6</span> Resumen conceptos</h3>
<ul>
<li>Hemos descrito el comportamiento aleatorio de una v.a. discreta mediante sus funciones de probabilidad <span class="math inline">\(P_{X}\)</span> y de distribución <span class="math inline">\(F_{X}\)</span>.</li>
<li>También tenemos un valor central; el valor esperado <span class="math inline">\(E(X)\)</span>.</li>
<li>Como medida básica nos queda definir una medida de lo lejos que están los datos del valor central <span class="math inline">\(E(X)\)</span> una de estas medidas es la varianza de <span class="math inline">\(X\)</span>.</li>
</ul>
<h2 id="medidas-de-la-variabilidad"><span class="header-section-number">2.2.6</span> Medidas de la variabilidad</h2>
<h3 id="medidas-de-la-variabilidad-la-varianza"><span class="header-section-number">2.2.6</span> Medidas de la variabilidad: la varianza</h3>
<div class="definition">
Varianza
</div>
<p>Sea <span class="math inline">\(X\)</span> una v.a. Llamaremos <em>varianza</em> de <span class="math inline">\(X\)</span> a</p>
<p><span class="math display">\[Var(X)=E((X-E(X))^2)\]</span></p>
<p>Por lo tanto la varianza es el momento central de orden <span class="math inline">\(2\)</span>.</p>
<p>De forma frecuente se utiliza la notación <span class="math display">\[\sigma_{X}^2=Var(X).\]</span></p>
<p>A la raíz cuadrada positiva de la varianza
<span class="math display">\[\sigma_{X}=\sqrt{Var(X)}\]</span></p>
<p>se la denomina desviación típica o estándar de <span class="math inline">\(X\)</span>.</p>
<h3 id="propiedades-de-la-varianza"><span class="header-section-number">2.2.6</span> Propiedades de la varianza</h3>
<p><l class="prop"> Propiedad </l></p>
<ul>
<li>Si <span class="math inline">\(X\)</span> es una v.a. discreta con función de probabilidad <span class="math inline">\(P_X\)</span> su varianza es
<span class="math display">\[\sigma_{X}^2=Var(X)=E((X-E(X))^2)=\sum_{x}(x-E(X))^2 P_{X}(x).\]</span></li>
<li>Sea <span class="math inline">\(X\)</span> una v.a.
<span class="math display">\[Var(X)=E(X^2)-(E(X))^2=\sum_{x} x^2 P_{X}(X)-(E(X))^2\]</span></li>
</ul>
<h3 id="demostración"><span class="header-section-number">2.2.6</span> Demostración</h3>
<div class="dem">
<p><strong>Demostración de b)</strong></p>
<p><span class="math display">\[\begin{eqnarray*}
Var(X)&amp;= &amp; \sum_{x}(x-E(X))^2 P_{X}(x) = \sum_{x}(x^2 -2 x E(X)+(E(X)^2) P_{X}(x)\\
&amp;=&amp; \sum_{x}x^2P_{X}(x) -  E(X)\sum_{x}2 x P_{X}(x) + (E(X)^2)\sum_{x} P_{X}(x)\\
&amp;=&amp; E(X^2)- 2 E(X) E(X) + (E(X))^2=E(X^2)-(E(X))^2.
\end{eqnarray*}\]</span></p>
</div>
<h3 id="ejemplo-18"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Calculemos en el ejemplo anterior la varianza del número de errores.</p>
</div>
<div class="example-sol">
<p>Recordemos que:</p>
<p><span class="math display">\[
P(X=0)=0.42,\quad P(X=1)=0.4, \quad P(X=2)=0.18
\]</span></p>
<p>y que</p>
<p><span class="math display">\[
E(X)=0.76
\]</span></p>
<p>Entonces:</p>
<p><span class="math display">\[
Var(X)=E(X^2)-(E(X))^2 = E(X^2)-(0.76)^2.
\]</span></p>
</div>
<h3 id="ejemplo-19"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example-sol">
<p>Ahora necesitamos calcular</p>
<p><span class="math display">\[E(X^2)= 0^2 (0.41)+ 1^2 (0.4)+ 2^2 (0.18)=0.4+0.72=1.12\]</span>
y por lo tanto</p>
<p><span class="math display">\[Var(X)= E(X^2)-(0.76)^2=1.12-0.5776=0.542\]</span>
y <span class="math display">\[\sqrt{Var(X)}=\sqrt{0.542}\]</span></p>
<p>En resumen <span class="math inline">\(\sigma_{X}^2=0.542\)</span> y <span class="math inline">\(\sigma_{X}=\sqrt{0.542}\)</span></p>
</div>
<h3 id="propiedades-de-la-varianza-1"><span class="header-section-number">2.2.6</span> Propiedades de la varianza</h3>
<p><l class="prop"> <strong>Propiedades de la varianza</strong></l></p>
<ul>
<li><span class="math inline">\(Var(X)\geq 0\)</span></li>
<li><span class="math inline">\(Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0\)</span></li>
<li>El mínimo de <span class="math inline">\(E((X-C)^2)\)</span> se alcanza cuando <span class="math inline">\(C=E(X)\)</span> y es <span class="math inline">\(Var(X)\)</span>. Esta propiedad es una de las que hace útil a la varianza como medida de dispersión.</li>
</ul>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Se deja como ejercicio la demostración de estas propiedades.</p>
</div>
<h2 id="esperanza-y-varianza-de-transformaciones-lineales."><span class="header-section-number">2.2.6</span> Esperanza y varianza de transformaciones lineales.</h2>
<h3 id="transformaciones-lineales."><span class="header-section-number">2.2.6</span> Transformaciones lineales.</h3>
<p><l class="definition"> Transformación lineal </l></p>
<p>Un <strong>cambio de variable lineal</strong> o <strong>transformación lineal</strong> de una v.a. <span class="math inline">\(X\)</span> es otra v.a. <span class="math inline">\(Y= a+ b X\)</span> donde <span class="math inline">\(a,b\in\mathbb{R}\)</span>.</p>
<p><l class="prop"> Esperanza de una transformación lineal</l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. con <span class="math inline">\(E(X)=\mu_{X}\)</span> y <span class="math inline">\(Var(X)=\sigma_{X}^2\)</span> y <span class="math inline">\(a,b\in\mathbb{R}\)</span>.
Entonces si <span class="math inline">\(Y=a+b X\)</span>:</p>
<ul>
<li><span class="math inline">\(E(Y)=E(a + b X)=a+ b E(X)= a + b \mu_{X}\)</span>.</li>
<li><span class="math inline">\(Var(Y)=Var(a+bX)=b^2 Var(X)= b^2 \sigma_{X}^2\)</span></li>
<li><span class="math inline">\(\sigma_{Y}=\sqrt{Var(Y)}=\sqrt{b^2 Var(X)}=|b| \sigma_{X}\)</span></li>
</ul>
<h3 id="demostración-1"><span class="header-section-number">2.2.6</span> Demostración</h3>
<div class="dem">
<p><strong>Demostración</strong>:</p>
<p><span class="math display">\[\begin{eqnarray*}
E(Y)&amp;=&amp; E(a+bX)=\sum_{x}(a+b\cdot X)\cdot P_{X}(x)\\
&amp;=&amp; a \sum_{x} P_{X}(x) + b \sum_{x} x\cdot P_{X}(x)\\ 
&amp;=&amp; a + b\cdot E(X)=a + b \mu_{X}
\end{eqnarray*}\]</span></p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Las demostración de las demás propiedades se dejan como ejercicio.</p>
</div>
<h2 id="variables-aleatorias-continuas"><span class="header-section-number">2.2.6</span> Variables aleatorias continuas</h2>
<h3 id="variables-aleatorias-continuas-definición."><span class="header-section-number">2.2.6</span> Variables aleatorias continuas definición.</h3>
<ul>
<li>Como ya hemos dicho las variables aleatorias continuas toman valores en
intervalos o áreas.</li>
<li>Lo más habitual es que estas variables tengan función de distribución continua y
derivable (salvo a los más en una cantidad finita o numerable de puntos:-)).</li>
<li>En lo que sigue supondremos que la función de distribución de variables
aleatorias continuas cumplen estas propiedades.</li>
<li>Notemos que si <span class="math inline">\(X\)</span> es una v.a. con función de distribución continua se tiene que
<span class="math inline">\(P(X=x_0)=F_X(x_0)-F(x_0^{-})=0\)</span>. Por lo que no tiene sentido definir <em>función de probabilidad</em>.</li>
</ul>
<h3 id="variables-aleatorias-continuas-1"><span class="header-section-number">2.2.6</span> Variables aleatorias continuas</h3>
<ul>
<li>En general tendremos que <span class="math inline">\(P(X&lt;x_0)=P(X\leq x_0)\)</span>.</li>
<li>Por otra parte podemos utilizar una regla parecida del
cociente entre casos favorables y casos posibles de Laplace pero en
este caso el conteo se hace por la <em>medida</em> de los casos
posibles partida por la <em>medida</em> de los casos favorables.</li>
<li>Veamos un ejemplo de v.a. continua, que ampliaremos en el tema siguiente, en el que se utilizan todos estos conceptos.</li>
</ul>
<h3 id="ejemplo-distribución-uniforme-en-01."><span class="header-section-number">2.2.6</span> Ejemplo: Distribución uniforme en <span class="math inline">\([0,1]\)</span>.</h3>
<div class="example">
<p><strong>Ejemplo: distancia el dardo centro</strong></p>
<p>Supongamos que lanzamos un dardo a una diana de radio <span class="math inline">\(1\)</span>, de forma que sea <em>equiprobable</em> cualquier distancia al centro (¡Cuidado! esto no es equivalente
a que cualquier punto de la diana sea <em>equiprobable</em>).</p>
<p>Consideremos la v.a. continua <span class="math inline">\(X=\)</span> distancia al centro.</p>
</div>
<div class="example-sol">
<p>Su función de distribución es</p>
<p><span class="math display">\[
F_{X}(x)=
\left\{
\begin{array}{ll}
0 &amp; \mbox{si } x\leq 0\\
x &amp; \mbox{si } 0&lt;x&lt;1\\
1 &amp; \mbox{si } x\geq 1
\end{array}
\right.
.
\]</span></p>
<p>Ya que</p>
<ul>
<li>C.F. <em>longitud favorable</em> es <span class="math inline">\(x-0\)</span>.</li>
<li>C.P. <em>longitud posible</em> es <span class="math inline">\(1-0\)</span>.</li>
<li>Luego
<span class="math display">\[P(X\leq x)=\frac{C.F.}{C.P.}=\frac{x-0}{1-0}=x\]</span></li>
</ul>
</div>
<h3 id="gráfica-de-la-función-de-distribución-uniforme"><span class="header-section-number">2.2.6</span> Gráfica de la función de distribución uniforme</h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/figUNIF-7.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="propiedades-18"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p>En las variables continuas los sucesos del tipo <span class="math inline">\(\{X\leq x \}\)</span> y <span class="math inline">\(\{X&lt; x \}\)</span> tendrán la
misma probabilidad, y otros tipos de sucesos similares también, algunas de estas
propiedades se explicitan en la siguiente proposición.</p>
<p><l class="prop">Propiedades</l></p>
<p>Dada una v.a. continua <span class="math inline">\(X\)</span> se tiene que:</p>
<ul>
<li><span class="math inline">\(P(X\leq b)=P(X&lt;b)\)</span></li>
<li><span class="math inline">\(P(X&lt;b)=P(X&lt;a)+P(a&lt;X&lt;b)\)</span></li>
<li><span class="math inline">\(P(a&lt;X&lt;b)=P(X&lt;b)-P(X&lt;a)\)</span></li>
</ul>
<h3 id="demostración-2"><span class="header-section-number">2.2.6</span> Demostración</h3>
<div class="dem">
<p><strong>Demostración:</strong></p>
<p>La primera es evidente <span class="math inline">\(P(X\leq b)=P(X&lt;b)+P(X=b)=P(X&lt;b)\)</span></p>
<p>Para demostrar la segunda, tenemos</p>
<p><span class="math display">\[\{X&lt;a\}\cap \{a&lt;X&lt;b\}=\emptyset\]</span>
<span class="math display">\[\{X&lt;a\}\cup \{a&lt;X&lt;b\}=\{X&lt;b\}\]</span></p>
<p>entonces</p>
<p><span class="math display">\[\begin{eqnarray*}
P(X\leq b)= &amp; P(\{X&lt;a\}\cup \{a&lt;X&lt;b\})\\
&amp; = P(X&lt;a)+P(a&lt;X&lt;b)
\end{eqnarray*}\]</span></p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>La demostración de la tercera propiedad es similar a la segunda pero aplicando la primera. Queda de ejercicio.</p>
</div>
<h3 id="propiedades-de-la-función-de-distribución-1"><span class="header-section-number">2.2.6</span> Propiedades de la función de distribución</h3>
<p>Las propiedades anteriores y combinaciones de ellas se pueden
escribir utilizando la función de distribución de <span class="math inline">\(X\)</span>:</p>
<p><l class="prop"> Propiedades de la Función de Distribución </l></p>
<p>Dada una variable aleatoria continua se tiene que:</p>
<ul>
<li><span class="math inline">\(F_{X}(b)=F_{X}(a)+P(a&lt;X&lt;b)\)</span></li>
<li><span class="math inline">\(P(a&lt;X&lt;b)=F_{X}(b)-F_{X}(a)\)</span></li>
<li><span class="math inline">\(P(a\leq X\leq b)=F_{X}(b)-F_{X}(a)\)</span></li>
</ul>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Se deja la demostración como ejercicio</p>
</div>
<h3 id="ejemplo-20"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: lanzamiento de dardos</strong></p>
<p>En los dardos:
<span class="math display">\[P(0.25&lt;X&lt;0.3)=F_{X}(0.3)-F_{X}(0.25)=\]</span>
<span class="math display">\[=0.3-0.25=0.05\]</span></p>
</div>
<h3 id="función-de-densidad"><span class="header-section-number">2.2.6</span> Función de densidad</h3>
<p><l class="definition"> <strong>Función de densidad</strong> </l></p>
<p>Una función <span class="math inline">\(f:\mathbb{R}\to\mathbb{R}\)</span> es una función de densidad sobre <span class="math inline">\(\mathbb{R}\)</span> si cumple que</p>
<ul>
<li><span class="math inline">\(f_{X}(x)\geq 0\)</span> para todo <span class="math inline">\(x \in\mathbb{R}.\)</span></li>
<li><span class="math inline">\(f\)</span> es continua salvo a lo más en una cantidad finita de puntos sobre
cada intervalo acotado de <span class="math inline">\(\mathbb{R}\)</span>.</li>
<li><span class="math inline">\(\displaystyle\int\limits_{-\infty}^{+\infty} f_{X}(x) dx=1.\)</span></li>
</ul>
<h3 id="función-de-densidad-de-una-variable-aleatoria."><span class="header-section-number">2.2.6</span> Función de densidad de una variable aleatoria.</h3>
<p><l class="definition"> <strong>Función de densidad de una variable aleatoria</strong> </l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. con función de distribución <span class="math inline">\(F_X\)</span>. Sea <span class="math inline">\(f:\mathbb{R}\to\mathbb{R}\)</span> una función de densidad tal que</p>
<p><span class="math display">\[F_X(x)=\displaystyle\int_{-\infty}^{x} f_X(t) dt.\mbox{ para todo } x\in\mathbb{R}.\]</span></p>
<p>Entonces <span class="math inline">\(X\)</span> es una variable aleatoria continua y <span class="math inline">\(f_X\)</span> es la densidad de la v.a. <span class="math inline">\(X\)</span>.</p>
<h3 id="dominio-de-una-variable-aleatoria-continua"><span class="header-section-number">2.2.6</span> Dominio de una variable aleatoria continua</h3>
<p>El conjunto <span class="math inline">\(D_X=\{x\in\mathbb{R}| f_x(x)&gt;0\}\)</span> recibe el nombre de <l class="definition"> soporte o dominio de la
variable aleatoria continua</l> y se interpreta su conjunto de resultados posibles.</p>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>En nuestra diana, la función <span class="math inline">\(f\)</span> es una densidad</p>
<p><span class="math display">\[
f_{X}(x)=\left\{
\begin{array}{ll}
0 &amp; \mbox{si } x\leq 0\\
1 &amp; \mbox{si } 0 &lt; x &lt; 1\\
0 &amp; \mbox{si } 1\leq x
\end{array}\right.
\]</span></p>
<p>que es la densidad de <span class="math inline">\(X\)</span>, en efecto:</p>
</div>
<h3 id="densidad-diana"><span class="header-section-number">2.2.6</span> Densidad diana</h3>
<div class="example-sol">
<p><span class="math display">\[
f_{X}(x)=\left\{
\begin{array}{ll}
0 &amp; \mbox{si } x\leq 0\\
1 &amp; \mbox{si } 0 &lt; x &lt; 1\\
0 &amp; \mbox{si } 1\leq x
\end{array}\right.
\]</span></p>
<p>Si <span class="math inline">\(x \leq 0\)</span> entonces <span class="math inline">\(\displaystyle\int_{-\infty}^x f_X(t) dt = 0.\)</span></p>
<p>Si <span class="math inline">\(0\leq x\leq 1\)</span> entonces <span class="math inline">\(\displaystyle\int_{-\infty}^x f_X(t) dt = \int_0^x 1 dt = x.\)</span></p>
<p>Si <span class="math inline">\(x\geq 1\)</span> entonces <span class="math inline">\(\displaystyle\int_{-\infty}^x f_X(t) dt = \int_0^1 1 dt = 1.\)</span></p>
<p>Por lo tanto <span class="math inline">\(F_X(x)=\displaystyle\int_{-\infty}^x f_X(t) dt\)</span> para todo <span class="math inline">\(x\in\mathbb{R}.\)</span></p>
</div>
<h3 id="densidad-diana-1"><span class="header-section-number">2.2.6</span> Densidad diana</h3>
<div class="example-sol">
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="kw">curve</span>(<span class="kw">dunif</span>(x,<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>,<span class="fl">1.5</span>),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,</a>
<a class="sourceLine" id="cb51-2" data-line-number="2">      <span class="dt">main=</span><span class="st">&quot;Densidad de la distribución uniforme en [0,1]&quot;</span>)</a></code></pre></div>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<h3 id="utilidad-de-la-función-de-densidad"><span class="header-section-number">2.2.6</span> Utilidad de la función de densidad</h3>
<p>La función de densidad nos permite calcular diversas probabilidades.</p>
<p><l class="prop">Propiedades de la función de densidad </l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. continua con función de distribución <span class="math inline">\(F_X\)</span> y de
densidad <span class="math inline">\(f_X\)</span>, entonces</p>
<ul>
<li><p><span class="math display">\[\begin{eqnarray*}
P(a&lt; X&lt; b) &amp;=&amp;  P(a&lt;X\leq b)= P(a\leq X&lt; b)=\\
 &amp; &amp; P(a\leq X\leq b)= \displaystyle\int_{a}^b f_X(x) dx.
\end{eqnarray*}\]</span></p></li>
<li><p>Si <span class="math inline">\(A\)</span> es un conjunto adecuado de <span class="math inline">\(\mathbb{R}\)</span> entonces</p></li>
</ul>
<p><span class="math display">\[
P(X\in A)=\displaystyle\int_{A} f(x) dx=\displaystyle\int_{A\cap D_X} f(x) dx.
\]</span></p>
<h3 id="utilidad-de-la-función-de-densidad-1"><span class="header-section-number">2.2.6</span> Utilidad de la función de densidad</h3>
<p><l class="prop">Propiedades de la función de densidad </l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. continua con función de distribución <span class="math inline">\(F_X\)</span> y de densidad <span class="math inline">\(f_X\)</span>, entonces:</p>
<ul>
<li>Si <span class="math inline">\(f_x\)</span> es continua en un punto <span class="math inline">\(x\)</span>, <span class="math inline">\(F_X\)</span> es derivable en ese punto y
<span class="math inline">\(F_X&#39;(x)=f_X(x).\)</span></li>
<li><span class="math inline">\(P(X=x)=0\)</span> para todo <span class="math inline">\(x\in\mathbb{R}.\)</span></li>
</ul>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
Comprobar estas propiedades en el ejemplo de la diana.
</div>
<h3 id="ejemplo-tiempo-ejecución-de-un-proceso"><span class="header-section-number">2.2.6</span> Ejemplo tiempo ejecución de un proceso</h3>
<div class="example">
<p><strong>Ejemplo: Tiempo ejecución de un proceso.</strong></p>
<p>Sea <span class="math inline">\(X=\)</span> tiempo de ejecución de un proceso. Se supone que <span class="math inline">\(X\)</span> sigue una distribución uniforme en dos unidades de tiempo, si tarda más el proceso se cancela.</p>
<p>Calculemos la función de densidad y de distribución de la v.a <span class="math inline">\(X\)</span>.</p>
</div>
<div class="example-sol">
<p>Entonces</p>
<p><span class="math display">\[
F_{X}(x)=P(X\leq x)=\frac{CF}{CP}=\frac{x}2
\]</span></p>
<p>Luego su función de distribución es:</p>
<p><span class="math display">\[
F_{X}(x)=\left\{\begin{array}{ll}
0 &amp; \mbox{si } x\leq 0\\
\frac{x}2 &amp; \mbox{si } 0&lt;x&lt;2\\
1 &amp; \mbox{si } 2\leq x
\end{array}\right.
\]</span></p>
<h3 id="ejemplo-tiempo-ejecución-de-un-proceso-1"><span class="header-section-number">2.2.6</span> Ejemplo tiempo ejecución de un proceso</h3>
<p>Su función de densidad por su lado es:
<span class="math display">\[
f_{X}(x)=F_{X}&#39;(x)=\left\{\begin{array}{ll}
0 &amp; \mbox{si } x\leq 0\\
\frac12 &amp; \mbox{si } 0&lt;x\leq 2\\
0 &amp; \mbox{si } 2\leq x
\end{array}\right.
\]</span></p>
<p>Efectivamente</p>
<ul>
<li><span class="math inline">\(f_{X}(x)\geq 0,\)</span> y tiene un conjunto finito de discontinuidades.</li>
<li><span class="math inline">\(F_X(x)=\int_{-\infty}^x f_X(t) dt.\)</span> para todo <span class="math inline">\(x\in \mathbb{R}\)</span> <l class="exercise">(Ejercicio,
resolverlo gráficamente.)</l></li>
<li><span class="math inline">\(\int\limits_{-\infty}^{+\infty}f_{X}(x)dx= \int\limits_0^2\frac12dx=\frac{x}2\mid_0^2= =\frac22-\frac02=1.\)</span></li>
</ul>
</div>
<h3 id="ejemplo-tiempo-ejecución-de-un-proceso-2"><span class="header-section-number">2.2.6</span> Ejemplo tiempo ejecución de un proceso</h3>
<div class="exercise">
<p><strong>Ejercicio: Tiempo de un proceso:</strong></p>
<p>Calcular la probabilidad de que uno de nuestros procesos tarde
más de una unidad de tiempo en ser procesado. Calcular también la probabilidad de
que dure entre <span class="math inline">\(0.5\)</span> y <span class="math inline">\(1.5\)</span> unidades de tiempo.</p>
</div>
<h2 id="esperanza-y-varianza-para-variables-aleatorias-continuas"><span class="header-section-number">2.2.6</span> Esperanza y varianza para variables aleatorias continuas</h2>
<h3 id="esperanza-y-varianza-para-variables-aleatorias-continuas-1"><span class="header-section-number">2.2.6</span> Esperanza y varianza para variables aleatorias continuas</h3>
<p>Los mismos comentarios y definiciones que se dieron en la sección correspondiente del tema
de estadística descriptiva son aplicables aquí.</p>
<p>Así que sólo daremos las definiciones, la forma de cálculo y algunos ejemplos.</p>
<p>En lo que sigue, salvo que diagamos lo contrario, <span class="math inline">\(X\)</span> es una v.a. continua con función de densidad <span class="math inline">\(f_{X}(x)\)</span></p>
<h3 id="esperanza-y-varianza-para-variables-aleatorias-continuas-2"><span class="header-section-number">2.2.6</span> Esperanza y varianza para variables aleatorias continuas</h3>
<p><l class="definition"> Esperanza v.a. continuas </l></p>
<ul>
<li>Su esperanza es:
<span class="math display">\[E(X)=\displaystyle\int\limits_{-\infty}^{+\infty} xf_{X}(x)dx.\]</span></li>
<li>Si <span class="math inline">\(g(x)\)</span> es una función de la variable <span class="math inline">\(X\)</span> entonces:
<span class="math display">\[E(g(X))=\displaystyle\int\limits_{-\infty}^{+\infty} g(x)\cdot f_{X}(x)dx.\]</span></li>
</ul>
<h3 id="esperanza-y-varianza-para-variables-aleatorias-continuas-3"><span class="header-section-number">2.2.6</span> Esperanza y varianza para variables aleatorias continuas</h3>
<p><l class="definition"> Varianza v.a. continuas </l></p>
<ul>
<li>Su varianza es:
<span class="math display">\[
Var(X)=\sigma_{X}^2=E((X-\mu_{X})^2)=
\displaystyle\int\limits_{-\infty}^{+\infty} (x-\mu_{X})^2 f_{X}(x)dx.
\]</span></li>
<li>Su desviación típica es: <span class="math display">\[\sigma_{X}=+\sqrt{\sigma_{X}^2}.\]</span></li>
</ul>
<h3 id="propiedades-19"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p><l class="prop"> Propiedades </l></p>
<ul>
<li><span class="math inline">\(\sigma_{X}^2\geq 0\)</span></li>
<li><span class="math inline">\(Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0\)</span></li>
<li><span class="math inline">\(Var(x)=E(X^2)-\mu_{X}^2=\int\limits_{-\infty}^{+\infty}x^2 f_{X}(x)dx - \mu_{X}^2.\)</span></li>
<li>El mínimo de <span class="math inline">\(E((X-C)^2)\)</span> se alcanza cuando <span class="math inline">\(C=E(X)\)</span> y es <span class="math inline">\(Var(X)\)</span>.</li>
</ul>
<h3 id="ejemplo-21"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: Dardo</strong></p>
<p>Calcular <span class="math inline">\(\mu_{X}\)</span> y <span class="math inline">\(\sigma_{X}^2\)</span> en el dardo.</p>
</div>
<div class="example-sol">
<p>Resultado
<span class="math display">\[\mu_{X}=\frac12,\]</span>
<span class="math display">\[E(X^2)=\frac13,\]</span>
<span class="math display">\[Var(X)=\frac1{12}.\]</span></p>
</div>
<h3 id="esperanza-de-trasformaciones-lineales-de-v.a.-continuas"><span class="header-section-number">2.2.6</span> Esperanza de trasformaciones lineales de v.a. continuas</h3>
<p><l class="prop"> Proposición</l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. continua con <span class="math inline">\(E(X)=\mu_{X}\)</span> y <span class="math inline">\(Var(X)=\sigma_{X}^2\)</span> sea <span class="math inline">\(Y=a+b X\)</span>, donde
<span class="math inline">\(a,b\in\mathbb{R}\)</span>, es una nueva v.a. continua obtenida mediante una transformación lineal de <span class="math inline">\(X\)</span>.
Se verifican las mismas propiedades que en el caso discreto:</p>
<ul>
<li><span class="math inline">\(E(Y)=E(a+b X)=a+b E(X)\)</span></li>
<li><span class="math inline">\(Var(Y)=Var(a+b X)=b^2 Var(X)\)</span></li>
<li><span class="math inline">\(\sigma_{Y}=|b| \sigma_{X}\)</span></li>
<li><span class="math inline">\(Z=\frac{X-\mu_{X}}{\sigma_{X}}\)</span> es una transformación
lineal de <span class="math inline">\(X\)</span> de forma que
<span class="math display">\[E(Z)=0 \mbox{ y } Var(Z)=1\]</span></li>
</ul>
<h3 id="ejemplo-22"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>En una empresa de venta de vinos por internet, sea
<span class="math inline">\(X=\)</span> número de litros de vino del país vendidos en un año.
Supongamos que sabemos que <span class="math inline">\(E(X)=10000\)</span> y que <span class="math inline">\(Var(X)=100\)</span>
Supongamos que los gastos fijos de distribución son
50.000 € y el beneficio por litro es de 10 € por botella.
Definimos <span class="math inline">\(T=10 X-50000\)</span> que será el beneficio después de gastos.</p>
</div>
<div class="example-sol">
<p>Entonces la esperanza del beneficio es
<span class="math display">\[E(T)=10 E(X)-50000 = 50000\]</span>
y
<span class="math display">\[Var(T)=10^2 Var(X)= 10000\]</span></p>
</div>
<h2 id="transformaciones-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Transformaciones de variables aleatorias</h2>
<h3 id="transformaciones-de-variables-aleatorias-1"><span class="header-section-number">2.2.6</span> Transformaciones de variables aleatorias</h3>
<p>Muchas variables aleatorias son funciones de otras v.a. En lo que sigue resumiremos diversas técnicas para dada una v.a. <span class="math inline">\(X\)</span> y una
transformación <span class="math inline">\(Y=h(X)\)</span> encontrar <span class="math inline">\(F_{Y}\)</span> a
partir de <span class="math inline">\(F_{X}\)</span>.</p>
<h3 id="propiedad"><span class="header-section-number">2.2.6</span> Propiedad</h3>
<p><l class="prop">Tranformación de v.a. discretas </l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. discreta con <span class="math inline">\(X(\Omega)=\{x_1,x_2,\ldots,x_{n},..\}\)</span> y sea <span class="math inline">\(h:\mathbb{R}\to\mathbb{R}\)</span> una aplicación.
Entonces <span class="math inline">\(Y=h(X)\)</span> es también una v.a. discreta. Además si <span class="math inline">\(P_X\)</span>
y <span class="math inline">\(F_{X}\)</span> son las funciones de probabilidad y de distribución de
<span class="math inline">\(X\)</span> entonces</p>
<ul>
<li><span class="math inline">\(\displaystyle P_{Y}(y)=\sum_{x_{i}|h(x_{i})=y}P_X(x_{i}).\)</span></li>
<li><span class="math inline">\(\displaystyle F_{Y}(y)=\sum_{x_{i}|h(x_{i})\leq y} P_X(x_{i}).\)</span></li>
</ul>
<h3 id="propiedades-20"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p>Desafortunadamente para variables no discetas el asunto no es tan sencillo como el anterior, pues la transformación de, por ejemplo, una v.a. continua puede ser continua, discreta, mixta…</p>
<p><l class="prop">Transformación de v.a. continuas en continuas</l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. continua cuya función de densidad es <span class="math inline">\(f_{X}\)</span>. Sea
<span class="math inline">\(h:\mathbb{R}\to\mathbb{R}\)</span> una aplicación estrictamente monótona y derivable, tal
que <span class="math inline">\(h&#39;(x)\not=0\)</span> para todo <span class="math inline">\(x\in\mathbb{R}\)</span>. Sea <span class="math inline">\(Y=h(X)\)</span> la
transformación de <span class="math inline">\(X\)</span> por <span class="math inline">\(h\)</span>. Entonces <span class="math inline">\(Y\)</span> es una v.a. continua con función
de densidad</p>
<p><span class="math display">\[f_{Y}(y)=\left.\frac{f_{X}(x)}
{\left|h&#39;(x)\right|}\right|_{x=h^{-1}(y)}\]</span></p>
<h3 id="propiedades-21"><span class="header-section-number">2.2.6</span> Propiedades</h3>
<p><l class="prop"> Densidad de una transformación de una v.a. continua</l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. continua cuya función de densidad es <span class="math inline">\(f_{X}\)</span>. Sea
<span class="math display">\[h:\mathbb{R}\to\mathbb{R}\]</span>
una aplicación, no necesariamente monótona tal que :</p>
<ul>
<li>sea derivable con derivada no nula</li>
<li>la ecuación <span class="math inline">\(h(x)=y\)</span> tiene un número finito de soluciones
<span class="math inline">\(x_1,x_2,..,x_{n}\)</span></li>
</ul>
<p>entonces:</p>
<p><span class="math display">\[
\displaystyle f_{Y}(y)=\left.\sum_{k=1}^{n} \frac{f_{X}(x)}
{\left|h&#39;(x)\right|}\right|_{x=x_{k}}.
\]</span></p>
<h3 id="método-general-del-transformación-de-v.a."><span class="header-section-number">2.2.6</span> Método general del transformación de v.a.</h3>
<p>Cuando no podamos aplicar las propiedades anteriores intentaremos
calcular primero la función de distribución de la transformación
y luego su densidad.</p>
<p>Notemos que en general si <span class="math inline">\(Y=g(X)\)</span> es una v.a. transformación de la
v.a. <span class="math inline">\(X\)</span> entonces</p>
<p><span class="math display">\[
F_{Y}(y)=P(Y\leq y)=P(g(X)\leq y).
\]</span></p>
<h3 id="método-general-del-transformación-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Método general del transformación de variables aleatorias</h3>
<p>Por ejemplo si <span class="math inline">\(g\)</span> es estrictamente creciente y cont.</p>
<p><span class="math display">\[
F_{Y}(y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_{X}(g^{-1}(y)).
\]</span></p>
<p>y si <span class="math inline">\(g\)</span> es estrictamente decreciente y cont.
<span class="math display">\[
F_{Y}(y)=P(g(X)\leq y)=P(X\geq g^{-1}(y))=1-F_{X}(g^{-1}(y)).
\]</span></p>
<h2 id="desigualdades-básicas-markov-y-chebychev"><span class="header-section-number">2.2.6</span> Desigualdades básicas: Markov y Chebychev</h2>
<h3 id="desigualdades-de-markov-y-de-chebychev"><span class="header-section-number">2.2.6</span> Desigualdades de Markov y de Chebychev</h3>
<ul>
<li>En esta sección distintas desigualdades que acotan determinadas probabilidades de
una variable aleatoria.</li>
<li>Estas desigualdades sirven en algunos casos para acotar probabilidades de determinados sucesos.</li>
<li>También son útiles desde el punto de vista teórico, por ejemplo para justificar que la varianza es una mediada de la dispersión de
los datos.</li>
</ul>
<h3 id="desigualdad-de-markov"><span class="header-section-number">2.2.6</span> Desigualdad de Markov</h3>
<p><l class="prop"> Desigualdad de Markov</l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. positiva con <span class="math inline">\(E(X)\)</span> finita. Entonces</p>
<p><span class="math display">\[P(X\geq a)\leq \frac{E(X)}{a}\mbox{ para todo }a&gt;0.\]</span></p>
<div class="dem">
<p><strong>Demostración</strong>:</p>
<p>Si <span class="math inline">\(X\)</span> es continua y solo toma valores positivos</p>
<p><span class="math display">\[\begin{eqnarray*}
E(X) &amp;=&amp; \int_{-\infty}^{+\infty} x f_{X}(x) dx=  \int_0^{+\infty} x f_{X}(x) dx=  \int_0^{a} x f_{X}(x) dx +\int_{a}^{+\infty} x f_{X}(x) dx \\
&amp; &amp;\geq   \int_{a}^{+\infty} x
f_{X}(x) dx \geq a \int_{a}^{+\infty}
f_{X}(x) dx = a \cdot  P(X\geq a)
\end{eqnarray*}\]</span>.</p>
<p>de donde se sigue que</p>
<p><span class="math display">\[P(X\geq a)\leq \frac{E(X)}{a}.\]</span></p>
</div>
<h3 id="desigualdad-de-markov-1"><span class="header-section-number">2.2.6</span> Desigualdad de Markov</h3>
<p><l class="prop"> Corolario</l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a. con <span class="math inline">\(E(X)\)</span> finita entonces para todo <span class="math inline">\(a&gt;0\)</span></p>
<span class="math display">\[P(|X|\geq a )\leq \frac{E(|X|)}{a}.\]</span>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Demuestra el corolario anterior a partir de la desigualdad de Markov.</p>
</div>
<h3 id="desigualdad-de-chebychev"><span class="header-section-number">2.2.6</span> Desigualdad de Chebychev</h3>
<p>La desigualdad de Chebychev también se denomina de Chebyshov y en inglés Chebyshev.</p>
<p><l class="prop"> Desigualdad de Chebychev</l></p>
<p>Sea <span class="math inline">\(X\)</span> una v.a.con <span class="math inline">\(E(X)=\mu\)</span> y <span class="math inline">\(Var(X)=\sigma^2\)</span> entonces para todo <span class="math inline">\(a&gt;0\)</span></p>
<p><span class="math display">\[P(|X-\mu|\geq a)\leq \frac{\sigma^2}{a^2}\]</span></p>
<h3 id="demostración-desigualdad-de-chebychev"><span class="header-section-number">2.2.6</span> Demostración desigualdad de Chebychev</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Apliquemos la consecuencia de la desigualdad de Markov a la v.a.
no negativa</p>
<p><span class="math display">\[Y^2=(X-\mu)^2\]</span></p>
<p>entonces</p>
<p><span class="math display">\[
P(Y^2\geq a^2) \leq 
\frac{E(Y^2)}{a^2}=\frac{E((X-\mu)^2)}{a^2}
= \frac{Var(X)}{a^2}=\frac{\sigma^2}{a^2}
.
\]</span></p>
<p>Por otra parte</p>
<p><span class="math display">\[
P(Y^2\geq a^2)=P(|Y|\geq a)= P(|X-\mu|\geq a).
\]</span></p>
<p>hecho que, junto con la desigualdad anterior,
demuestra el resultado.</p>
</div>
<h3 id="uso-de-la-desigualdad-de-chebychev"><span class="header-section-number">2.2.6</span> Uso de la desigualdad de Chebychev</h3>
<div class="observ">
<strong>Utilidad básica de la desigualdad de Chebychev</strong>
</div>
<p>Supongamos que <span class="math inline">\(X\)</span> es una v.a. con <span class="math inline">\(Var(X)=0\)</span>
entonces.</p>
<p>Aplicando la desigualdad anterior</p>
<p><span class="math display">\[P(|X-E(X)|\geq a )=0\mbox{ para todo }a&gt;0\]</span>
lo que implica que</p>
<p><span class="math display">\[P(X=E(X))=1.\]</span></p>
<p>Por lo que probabilidad de que <span class="math inline">\(X\)</span> sea
constantemente <span class="math inline">\(E(X)\)</span> es 1.</p>
<p>Lo que nos confirma la utilidad de la varianza es una
medida de la dispersión de los datos.</p>
<h3 id="ejemplo-23"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Se sabe que el tiempo de respuesta medio y la desviación típica de un sistema multiusuario son 15 y 3 u.t.respectivamente. Entonces:</p>
<p><span class="math display">\[
P(|X-15|\geq 5)\leq \frac9{25}=0.36.
\]</span></p>
</div>
<p>Si substituimos <span class="math inline">\(a\)</span> por <span class="math inline">\(a\cdot \sigma\)</span> en la
desigualdad de Chebychev, nos queda:</p>
<p><span class="math display">\[
P(|X-\mu|\geq a \sigma)\leq
\frac{\sigma^2}{(a\sigma)^2}=\frac1{a^2}.
\]</span></p>
<p>Que es otra manera de expresar la desigualdad de Chebychev.</p>
<h3 id="más-formas-de-la-desgualdad-de-chebychev"><span class="header-section-number">2.2.6</span> Más formas de la desgualdad de Chebychev</h3>
<p>La desigualdad de Chebychev también se puede escribir de al menos dos maneras más:</p>
<p><span class="math display">\[
P(\mu-a\leq X\leq \mu+a)\geq 1-\frac{\sigma^2}{a^2}
\]</span></p>
<p>y tomado como <span class="math inline">\(a=k\cdot \sigma\)</span></p>
<p><span class="math display">\[
P(\mu-k\cdot \sigma\leq X\leq \mu+ k \cdot \sigma)\geq 1-\frac1{k^2}
\]</span></p>
<h3 id="la-varianza-como-medida-de-dispersión"><span class="header-section-number">2.2.6</span> La varianza como medida de dispersión</h3>
<p>Tomando la segunda expresión que hemos visto para la desigualdad de
Chebychev para distintos valores de <span class="math inline">\(k&gt;0\)</span> tenemos la siguiente tabla.</p>
<center>
<table>
<thead>
<tr class="header">
<th>k</th>
<th><span class="math inline">\(P(|X-E(X)|\geq k \cdot \sigma)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(\leq 1\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(\leq 0.25\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(\leq 0.111\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td><span class="math inline">\(\leq 0.0025\)</span></td>
</tr>
</tbody>
</table>
</center>
<h3 id="interpretación-de-la-desigualdad"><span class="header-section-number">2.2.6</span> Interpretación de la desigualdad</h3>
<ul>
<li><p>Por ejemplo para <span class="math inline">\(k=2\)</span> esta desigualdad se puede interpretar como que dada una v.a. <span class="math inline">\(X\)</span> con cualquier distribución que tenga <span class="math inline">\(E(X)\)</span> y <span class="math inline">\(Var(X)\)</span> finitos <em>la probabilidad de que un valor se aleje de la media <span class="math inline">\(\mu\)</span> más de <span class="math inline">\(a=2\)</span> desviaciones típicas es menor o igual que <span class="math inline">\(0.25\)</span></em>.</p></li>
<li><p>Es decir sólo el 25% de los valores estarán alejados de la media
más de <span class="math inline">\(2\sigma\)</span></p></li>
</ul>
<p>¡<em>Sea cual sea la distribución de la v.a.</em>!</p>

<h1 id="variables-aleatorias.-complementos"><span class="header-section-number">2.2.6</span> Variables Aleatorias. Complementos</h1>
<h2 id="momentos-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Momentos de variables aleatorias</h2>
<p><l class="definition"> Definición. </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria. Definimos el <strong>momento de orden <span class="math inline">\(n\)</span></strong> como
<span class="math inline">\(m_n = E\left(X^n\right)\)</span>.</p>
<p><l class="observ"> Observación.</l>
El momento de orden <span class="math inline">\(1\)</span> de una variable aleatoria es su valor medio o <span class="math inline">\(E(X)\)</span>.</p>
<p>Los momentos de orden <span class="math inline">\(n\)</span> caracterizan una variable <span class="math inline">\(X\)</span>. O sea, que si conocemos todos los momentos de orden <span class="math inline">\(n\)</span>, podemos deducir cuál es la distribución de <span class="math inline">\(X\)</span>.</p>
<p>En general, el cálculo de los momentos de orden <span class="math inline">\(n\)</span> para una variable <span class="math inline">\(X\)</span> es bastante tedioso.</p>
<h3 id="ejemplo-momento-de-orden-n"><span class="header-section-number">2.2.6</span> Ejemplo momento de orden <span class="math inline">\(n\)</span></h3>
<div class="example">
<p><strong>Ejemplo: momento de orden <span class="math inline">\(n\)</span> de una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ p_X(1)=p.
\]</span>
Su momento de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
m_n = E\left(X^n\right)=p\cdot 1^n+(1-p)\cdot 0^n = p.
\]</span>
En este caso, todos los momentos de orden <span class="math inline">\(n\)</span> valen <span class="math inline">\(p\)</span>.</p>
</div>
<h3 id="ejemplo-momento-de-orden-n-1"><span class="header-section-number">2.2.6</span> Ejemplo momento de orden <span class="math inline">\(n\)</span></h3>
<div class="example">
<p><strong>Ejemplo: momento de orden <span class="math inline">\(n\)</span> de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Consideremos ahora una variable <span class="math inline">\(X\)</span> exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(0\)</span>, en caso contrario.</p>
<p>Su momento de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
m_n = E\left(X^n\right)=\int_0^\infty \lambda \mathrm{e}^{-\lambda x} x^n\, dx =\frac{n!}{\lambda^n}.
\]</span></p>
<p>La expresión anterior se puede obtener integrando por partes <span class="math inline">\(n\)</span> veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes.</p>
<p>Fijémonos que los momentos de orden <span class="math inline">\(n\)</span> tienden a infinito a medida que <span class="math inline">\(n\)</span> crece: <span class="math inline">\(\lim\limits_{n\to\infty}m_n = \lim\limits_{n\to\infty}\frac{n!}{\lambda^n}=\infty\)</span>.</p>
</div>
<h3 id="ejemplo-momento-de-orden-n-2"><span class="header-section-number">2.2.6</span> Ejemplo momento de orden <span class="math inline">\(n\)</span></h3>
<div class="example">
<p><strong>Ejemplo: momento de orden <span class="math inline">\(n\)</span> de una variable normal de parámetros <span class="math inline">\(m=0\)</span> y <span class="math inline">\(\sigma =1\)</span></strong></p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su momento de orden 1 será la esperanza de <span class="math inline">\(X\)</span>: <span class="math inline">\(m_1 = 0\)</span> i su momento de orden 2 será:
<span class="math inline">\(m_2 = E\left(X^2\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^2\, dx = 1.\)</span>
La integral anterior se resuelve usando técnicas de integrales de dos variables.
Dicho valor también se puede obtener usando que su varianza vale 1:
<span class="math inline">\(m_2 = \mathrm{Var}(X)+E(X)^2 = \sigma^2 +0^2 = 1.\)</span></p>
<p>Los momentos de orden impar <span class="math inline">\(n\)</span> serán cero ya que integramos una función impar:
<span class="math inline">\(m_n = E\left(X^n\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^n\, dx = 0.\)</span>
O sea, si consideramos <span class="math inline">\(g(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^n\)</span>, se verifica <span class="math inline">\(g(-x)=-g(x)\)</span>, para todo <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<p>Si intentamos calcular el momento de orden 4, obtenemos:
<span class="math inline">\(m_4 = E\left(X^4\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^4\, dx = 3,\)</span> usando técnicas de integración de dos variables otra vez.</p>
</div>
<h3 id="momento-central-de-orden-n-de-una-variable-aleatoria"><span class="header-section-number">2.2.6</span> Momento central de orden <span class="math inline">\(n\)</span> de una variable aleatoria</h3>
<p><l class="definition"> Definición. </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria. Definimos el <strong>momento central de orden <span class="math inline">\(n\)</span></strong> como
<span class="math inline">\(\mu_n = E\left((X-\mu)^n\right)\)</span>, donde <span class="math inline">\(\mu =E(X)\)</span> es la media o la esperanza de la variable aleatoria <span class="math inline">\(X\)</span>.</p>
<p><l class="observ"> Observación.</l>
El momento central de orden <span class="math inline">\(1\)</span> de una variable aleatoria es siempre 0:
<span class="math display">\[
\mu_1 = E\left((X-\mu)\right)=E(X)-E(\mu)=E(X)-E(X)=0.
\]</span></p>
<h3 id="momento-central-de-orden-n-de-una-variable-aleatoria-1"><span class="header-section-number">2.2.6</span> Momento central de orden <span class="math inline">\(n\)</span> de una variable aleatoria</h3>
<p><l class="observ"> Observación.</l>
El momento central de orden <span class="math inline">\(2\)</span> de una variable aleatoria es la varianza:
<span class="math display">\[
\mu_2 = E\left((X-\mu)^2\right):= \mathrm{Var}(X).
\]</span></p>
<p>Los momentos centrales de orden <span class="math inline">\(n\)</span> caracterizan también una variable <span class="math inline">\(X\)</span>. O sea, que si conocemos todos los momentos centrales de orden <span class="math inline">\(n\)</span>, podemos deducir cuál es la distribución de <span class="math inline">\(X\)</span>.</p>
<h3 id="momento-central-de-orden-n-de-una-variable-aleatoria-2"><span class="header-section-number">2.2.6</span> Momento central de orden <span class="math inline">\(n\)</span> de una variable aleatoria</h3>
<p><l class="prop"> Proposición.</l>
La relación que hay entre los momentos centrales y los momentos de una variable aleatoria es la siguiente:
<span class="math display">\[
\mu_n = \sum_{k=0}^n (-1)^{n-k} \binom{n}{k} \mu^{n-k} m_k = \sum_{k=0}^n (-1)^{k} \binom{n}{k} \mu^{k} m_{n-k},
\]</span>
donde <span class="math inline">\(\mu =E(X)\)</span> recordemos que es la esperanza de la variable aleatoria <span class="math inline">\(X\)</span>.</p>
<h3 id="momento-central-de-orden-n-de-una-variable-aleatoria-3"><span class="header-section-number">2.2.6</span> Momento central de orden <span class="math inline">\(n\)</span> de una variable aleatoria</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Recordemos la definición de momento central de orden <span class="math inline">\(n\)</span> y desarrollemos su expresión aplicando el <strong>binomio de Newton</strong>:
<span class="math display">\[
\mu_n = E\left((X-\mu)^n\right) =E\left(\sum_{k=0}^n (-1)^{n-k} \binom{n}{k} X^k\mu^{n-k}\right).
\]</span>
Aplicando la propiedad de la esperanza que la esperanza de la suma es la suma de esperanzas, obtenemos la expresión dada por la proposición:
<span class="math display">\[
\mu_n =\sum_{k=0}^n (-1)^{n-k} \binom{n}{k} \mu^{n-k} E\left(X^k\right) = \sum_{k=0}^n (-1)^{n-k} \binom{n}{k} \mu^{n-k} m_k.
\]</span></p>
</div>
<h3 id="ejemplo-momento-central-de-orden-n"><span class="header-section-number">2.2.6</span> Ejemplo momento central de orden <span class="math inline">\(n\)</span></h3>
<div class="example">
<p><strong>Ejemplo: momento central de orden <span class="math inline">\(n\)</span> de una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ P_X(1)=p.
\]</span>
Usando que <span class="math inline">\(E(X)=p\)</span>, su momento central de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
\mu_n = E\left((X-p)^n\right)=p\cdot (1-p)^n+(1-p)\cdot (0-p)^n = p(1-p)^n + (-1)^n (1-p) p^n.
\]</span></p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Demostrar que la expresión anterior corresponde a un polinomio de grado <span class="math inline">\(n\)</span>.</p>
</div>
<h3 id="ejemplo-momento-central-de-orden-n-1"><span class="header-section-number">2.2.6</span> Ejemplo momento central de orden <span class="math inline">\(n\)</span></h3>
<div class="example">
<p><strong>Ejemplo: momento central de orden <span class="math inline">\(n\)</span> de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Consideremos ahora una variable <span class="math inline">\(X\)</span> exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span>.</p>
<p>Usando que <span class="math inline">\(E(X)=\frac{1}{\lambda}\)</span>, su momento central de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
\mu_n = E\left(\left(X-\frac{1}{\lambda}\right)^n\right)=\int_0^\infty \lambda \mathrm{e}^{-\lambda x} \left(x-\frac{1}{\lambda}\right)^n\, dx =\frac{a_n}{\lambda^n},
\]</span>
donde <span class="math inline">\(a_n = n!\sum\limits_{k=0}^n \frac{(-1)^k}{k!}.\)</span></p>
</div>
<h3 id="ejemplo-momento-central-de-orden-n-2"><span class="header-section-number">2.2.6</span> Ejemplo momento central de orden <span class="math inline">\(n\)</span></h3>
<div class="example">
<p><strong>Ejemplo: momento central de orden <span class="math inline">\(n\)</span> de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>La expresión anterior fijado <span class="math inline">\(n\)</span> se puede obtener integrando por partes <span class="math inline">\(n\)</span> veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes. Sin embargo, la obtención de la fórmula general para <span class="math inline">\(n\)</span> se sale del nivel del curso.</p>
<p>Fijémonos que los momentos centrales de orden <span class="math inline">\(n\)</span> también tienden a infinito a medida que <span class="math inline">\(n\)</span> crece: <span class="math inline">\(\lim\limits_{n\to\infty}\mu_n = \lim\limits_{n\to\infty}\frac{a_n}{\lambda^n}=\infty\)</span>:
<span class="math display">\[
\lim_{n\to\infty}\mu_n =\lim_{n\to\infty} \frac{n!\sum\limits_{k=0}^n \frac{(-1)^k}{k!}}{\lambda^n}= 
\lim_{n\to\infty}\sum\limits_{k=0}^n \frac{(-1)^k}{k!}\cdot \lim_{n\to\infty} \frac{n!}{\lambda^n}= \mathrm{e}^{-1}\cdot \infty = \infty.
\]</span></p>
</div>
<h3 id="ejemplo-momento-central-de-orden-n-3"><span class="header-section-number">2.2.6</span> Ejemplo momento central de orden <span class="math inline">\(n\)</span></h3>
<div class="example">
<p><strong>Ejemplo: momento central de orden <span class="math inline">\(n\)</span> de una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su momento central de orden 2 será la varianza <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\mu_2 =\sigma^2.\)</span></p>
<p>Los momentos centrales de orden impar <span class="math inline">\(n\)</span> serán cero ya que integramos una función impar respecto <span class="math inline">\(x=\mu\)</span>:
<span class="math inline">\(\mu_n = E\left((X-\mu)^n\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\cdot (x-\mu)^n\, dx = 0.\)</span>
O sea, si consideramos <span class="math inline">\(g(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\cdot (x-\mu)^n\)</span>, se verifica <span class="math inline">\(g(\mu-x)=-g(\mu +x)\)</span>, para todo <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<p>Si intentamos calcular el momento central de orden 4, obtenemos:
<span class="math inline">\(\mu_4 = E\left((X-\mu)^4\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\cdot (x-\mu)^4\, dx = 3\sigma^4.\)</span> La integral anterior puede resolverse con el cambio de variable <span class="math inline">\(t=\frac{x-\mu}{\sigma}\)</span> y usando que: <span class="math inline">\(\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^4\, dx = 3.\)</span></p>
</div>
<h2 id="asimetría-de-una-variable-aleatoria"><span class="header-section-number">2.2.6</span> Asimetría de una variable aleatoria</h2>
<h3 id="definición"><span class="header-section-number">2.2.6</span> Definición</h3>
<p>Una variable aleatoria tiene <strong>asimetría positiva</strong> si su función de densidad o de probabilidad presenta una cola a la
<strong>derecha</strong> y <strong>asimetría negativa</strong>, si su función de densidad o de probabilidad presenta cola a la <strong>izquierda</strong>.</p>
<p>Por ejemplo, en la figura siguiente, vemos la gráfica de la función de probabilidad de una variable aleatoria que presenta <strong>asimetría negativa</strong> a la izquierda y una función de densidad de una variable aleatoria que presenta <strong>asimetría positiva</strong> a la derecha:</p>
<h3 id="definición-1"><span class="header-section-number">2.2.6</span> Definición</h3>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<h3 id="cómo-calcular-la-asimetría-de-una-variable-aleatoria"><span class="header-section-number">2.2.6</span> ¿Cómo calcular la asimetría de una variable aleatoria?</h3>
<p>La asimetría de una variable aleatoria <span class="math inline">\(X\)</span> se calcula a partir de sus momentos centrales de segundo y tercer orden:
<span class="math display">\[
\gamma_1 = E\left({\left(\frac{X-\mu}{\sigma}\right)}^3\right)=\frac{\mu_3}{\sigma^3},
\]</span>
donde <span class="math inline">\(\mu = E(X)\)</span> y <span class="math inline">\(\sigma^2 =\mathrm{Var}(X)\)</span>.</p>
<p>Dicho valor se denomina <strong>coeficiente de asimetría de Pearson</strong>.</p>
<h3 id="cómo-calcular-la-asimetría-de-una-variable-aleatoria-1"><span class="header-section-number">2.2.6</span> ¿Cómo calcular la asimetría de una variable aleatoria?</h3>
<p>Usando la relación ya vista entre los momentos centrales y los momentos, podemos expresar el <strong>coeficiente de asimetría</strong> en función de los momentos:
<span class="math display">\[
\gamma_1 = \frac{m_3 -3\mu\sigma^2-\mu^3}{\sigma^3}.
\]</span>
Dejamos al lector la comprobación de la expresión anterior.</p>
<p>Por tanto, una variable aleatoria <span class="math inline">\(X\)</span> tendrá simetría positiva o a la derecha si <span class="math inline">\(\gamma_1 &gt;0\)</span> y tendrá asimetría negativa o a la izquierda, si <span class="math inline">\(\gamma_1 &lt;0\)</span>.</p>
<h3 id="ejemplo-de-cálculo-de-asimetría"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de asimetría</h3>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de asimetría para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Usando que <span class="math inline">\(m_n =p\)</span>, para todo <span class="math inline">\(n\)</span> y que <span class="math inline">\(\mu_2 = \sigma^2 = p-p^2\)</span>, el coeficiente de asimetría <span class="math inline">\(\gamma_1\)</span> será:
<span class="math display">\[
\gamma_1 = \frac{p-3p(p-p^2)-p^3}{\sqrt{(p-p^2)^3}} = \frac{p (1-p) (1-2p)}{{\sqrt{(p-p^2)^3}}}.
\]</span>
Por tanto, la variable de Bernoulli de parámetro <span class="math inline">\(p\)</span> tendrá simetria negativa si <span class="math inline">\(p&gt;\frac{1}{2}\)</span> y positiva, si <span class="math inline">\(p&lt;\frac{1}{2}\)</span>:</p>
</div>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<h3 id="ejemplo-de-cálculo-de-asimetría-1"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de asimetría</h3>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de asimetría para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span>.
Usando que <span class="math inline">\(\sigma^2=\frac{1}{\lambda^2}\)</span> y <span class="math inline">\(\mu_3 =\frac{a_3}{\lambda^3}=\frac{2}{\lambda^3}\)</span>, su coeficiente de asimetría de Pearson será:
<span class="math inline">\(\gamma_1 = \frac{\frac{2}{\lambda^3}}{\frac{1}{\lambda^3}}=2.\)</span></p>
<p>Entonces presenta asimetría positiva o a la derecha tal como se observa en su función de densidad:</p>
</div>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<h3 id="ejemplo-de-cálculo-de-asimetría-2"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de asimetría</h3>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de asimetría para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Tal como se ha indicado anteriormente, los momentos centrales de orden impar son nulos.</p>
<p>Por tanto, en este caso <span class="math inline">\(\mu_3=0\)</span> y, por tanto, <span class="math inline">\(\gamma_1=0\)</span>.</p>
<p>Deducimos que la distribución normal es totalmente simétrica.</p>
<p>De hecho, usando que su función de densidad es <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>, se puede comprobar que <span class="math inline">\(f_X(\mu-x)=f_X(\mu +x)\)</span>, o sea, tiene el eje de simetría <span class="math inline">\(x=\mu\)</span>:</p>
</div>
<h3 id="ejemplo-de-cálculo-de-asimetría-3"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de asimetría</h3>
<div class="example">
<strong>Ejemplo: cálculo del coeficiente de asimetría para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
</div>
<h2 id="curtosis-o-apuntamiento-de-una-variable-aleatoria"><span class="header-section-number">2.2.6</span> Curtosis o apuntamiento de una variable aleatoria</h2>
<h3 id="definición-2"><span class="header-section-number">2.2.6</span> Definición</h3>
<p>La curtosis de una variable aleatoria <span class="math inline">\(X\)</span> es una medida de cómo son las colas de su función de densidad.</p>
<p>Dicho en otras palabras, queremos medir de alguna manera la <em>tendencia</em> que tiene la variable aleatoria a tener valores atípicos o <em>outliers</em>.</p>
<p>La manera estándard de medir la curtosis de una variable aleatoria <span class="math inline">\(X\)</span> es a partir de su <strong>momento central de cuarto orden</strong>:
<span class="math display">\[
\gamma_2 = E\left(\left(\frac{X-\mu}{\sigma}\right)^4\right) = \frac{\mu_4}{\sigma^4},
\]</span>
donde recordemos que <span class="math inline">\(\mu=E(X)\)</span> y <span class="math inline">\(\sigma^2 =\mathrm{Var}(X)\)</span>.</p>
<p>A la expresión anterior se le denomina <strong>medida de curtosis de Pearson</strong>.</p>
<h3 id="definición-3"><span class="header-section-number">2.2.6</span> Definición</h3>
<ul>
<li><p>Diremos que una variable aleatoria no tiene exceso de curtosis o <strong>mesocúrtica</strong> si <span class="math inline">\(\gamma_2 \approx 3\)</span>.</p></li>
<li><p>Diremos que una variable aleatoria tiene exceso positivo de curtosis o <strong>leptocúrtica</strong> si <span class="math inline">\(\gamma_2 &gt;3\)</span>.</p></li>
<li><p>Diremos que una variable aleatoria tiene exceso negativo de curtosis o <strong>platicúrtica</strong> si <span class="math inline">\(\gamma_2 &lt;3\)</span>.</p></li>
</ul>
<h3 id="ejemplo-de-cálculo-de-curtosis"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de curtosis</h3>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de curtosis para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria de parámetro <span class="math inline">\(p\)</span>.</p>
<p>El momento central de cuarto orden de <span class="math inline">\(X\)</span> será:
<span class="math display">\[
\mu_4 = p (1-p)^4 +(1-p)p^4 = p (1-p) (3 p^2-3p+1).
\]</span>
La medida de curtosis de Pearson será:
<span class="math display">\[
\gamma_2 = \frac{p (1-p) (3 p^2-3p+1)}{p^2 (1-p)^2} = \frac{3 p^2-3p+1}{p(1-p)}.
\]</span>
Se puede comprobar (ejercicio para el lector) que si <span class="math inline">\(p\in \left(\frac{3-\sqrt{3}}{6},\frac{3+\sqrt{3}}{6}\right)\approx (0.211,0.789)\)</span>, <span class="math inline">\(\gamma_2 &lt;3\)</span> y, por tanto <span class="math inline">\(X\)</span> será platicúrtica y en caso contrario, si <span class="math inline">\(p\in \left(0,\frac{3-\sqrt{3}}{6}\right)\cup \left(\frac{3+\sqrt{3}}{6},1\right)\)</span>, <span class="math inline">\(\gamma_2 &gt;3\)</span> y, por tanto, <span class="math inline">\(X\)</span> será leptocúrtica.</p>
</div>
<h3 id="ejemplo-de-cálculo-de-curtosis-1"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de curtosis</h3>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de curtosis para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span>.
Usando que <span class="math inline">\(\sigma^2=\frac{1}{\lambda^2}\)</span> y <span class="math inline">\(\mu_4 =\frac{a_4}{\lambda^3}=\frac{9}{\lambda^4}\)</span>, su coeficiente de asimetría de Pearson será:
<span class="math inline">\(\gamma_2 = \frac{\frac{9}{\lambda^4}}{\frac{1}{\lambda^4}}=9.\)</span></p>
<p>Como <span class="math inline">\(\gamma_2 &gt;3\)</span>, se trataría de una distribución leptocúrtica.</p>
</div>
<h3 id="ejemplo-de-cálculo-de-curtosis-2"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de curtosis</h3>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de curtosis para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Tal como se ha indicado anteriormente, el momento central de orden 4 vale: <span class="math inline">\(\mu_4 = 3\sigma^4\)</span>.</p>
<p>Su coeficiente de curtosis será:
<span class="math display">\[
\gamma_2 =\frac{\mu_4}{\sigma^4}=\frac{3\sigma^4}{\sigma^4}=3.
\]</span>
Deducimos, por tanto, que toda distribución normal es mesocúrtica o no tiene exceso (ni positivo ni negativo) de curtosis.</p>
</div>
<h2 id="métodos-de-transformación"><span class="header-section-number">2.2.6</span> Métodos de transformación</h2>
<h3 id="introducción"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Hemos visto anteriormente que el cálculo de los <strong>momentos</strong> o los <strong>momentos centrados</strong> de una variable aleatoria <span class="math inline">\(X\)</span> puede ser muy complicado y muy tedioso.</p>
<p>Por dicho motivo, vamos a introducir un conjunto de funciones que nos permitirán calcular los <strong>momentos</strong> de la variable <span class="math inline">\(X\)</span> de forma relativamente sencilla.</p>
<h3 id="función-generatriz-de-momentos"><span class="header-section-number">2.2.6</span> Función generatriz de momentos</h3>
<p><l class="definition">Definición de función generatriz de momentos: </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria <span class="math inline">\(X\)</span> con función de probabilidad <span class="math inline">\(P_X\)</span> en el caso discreto o función
de densidad <span class="math inline">\(f_X\)</span> en el caso continuo.</p>
<p>Sea <span class="math inline">\(t\in\mathbb{R}\)</span> un valor real cualquiera.</p>
<p>Definimos la función generatriz de momentos <span class="math inline">\(m_X(t)\)</span> en el valor <span class="math inline">\(t\)</span> como: <span class="math inline">\(m_X(t)=E\left(\mathrm{e}^{tX}\right).\)</span></p>
<h3 id="ejemplo-de-cálculo-de-función-generatriz-de-momentos"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de función generatriz de momentos</h3>
<div class="example">
<p><strong>Ejemplo: cálculo de la función generatriz de momentos para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ p_X(1)=p.
\]</span>
Su función generatriz de momentos será:
<span class="math display">\[
m_X (t)=E\left(\mathrm{e}^{tX}\right) =p\mathrm{e}^{t\cdot 1}+(1-p)\mathrm{e}^{t\cdot 0}=p\mathrm{e}^t+(1-p)=1+p\left(\mathrm{e}^t -1 \right).
\]</span></p>
</div>
<h3 id="ejemplo-de-cálculo-de-función-generatriz-de-momentos-1"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de función generatriz de momentos</h3>
<div class="example">
<p><strong>Ejemplo: cálculo de la función generatriz de momentos para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>. Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(0\)</span>, en caso contrario.</p>
<p>Su función generatriz de momentos será:
<span class="math display">\[
m_X (t)=E\left(\mathrm{e}^{tX}\right)=\int_0^\infty \mathrm{e}^{t x}\lambda \mathrm{e}^{-\lambda x}\, dx = \lambda \int_0^\infty\mathrm{e}^{(t-\lambda)x}\, dx = \lambda\left[\frac{\mathrm{e}^{(t-\lambda)x}}{t-\lambda}\right]_{x=0}^{x=\infty} = \frac{\lambda}{\lambda -t},\ \mbox{si } t&lt;\lambda. 
\]</span>
En este caso vemos que el dominio de la función generatriz de momentos <span class="math inline">\(m_X\)</span> es <span class="math inline">\((-\infty,\lambda)\)</span>, ya que si <span class="math inline">\(t\geq \lambda\)</span>, la integral anterior no es convergente.</p>
<p>Fijémonos por lo que vendrá más adelante que, como <span class="math inline">\(\lambda &gt;0\)</span>, el valor <span class="math inline">\(0\)</span> pertenece al dominio de <span class="math inline">\(m_X\)</span>.</p>
</div>
<h3 id="ejemplo-de-cálculo-de-función-generatriz-de-momentos-2"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de función generatriz de momentos</h3>
<div class="example">
<p><strong>Ejemplo: cálculo de la función generatriz de momentos para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su función generatriz de momentos será:</p>
<p><span class="math display">\[
\begin{array}{rl}
m_X (t) &amp; =E\left(\mathrm{e}^{tX}\right)=\int_{-\infty}^\infty \mathrm{e}^{tx}\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{tx-\frac{(x-\mu)^2}{2\sigma^2}}\, dx \\  &amp; =  \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}\left((x-(\sigma^2 t+\mu))^2-2\sigma^2 t \mu-\sigma^4t^2\right)}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{e}^{\frac{1}{2}(2 t \mu +\sigma^2 t^2)}\int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 t+\mu))^2}\, dx\\ &amp;  = \mathrm{e}^{\frac{1}{2}(2 t \mu +\sigma^2 t^2)} \left( \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 t+\mu))^2}\, dx\right) =  \mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}}.
\end{array}
\]</span>
La integral del último paréntesis se resuelve haciento el cambio de variable <span class="math inline">\(u=x-\sigma^2 t\)</span> y usando que la integral de la función de densidad de <span class="math inline">\(X\)</span> sobre todo <span class="math inline">\(\mathbb{R}\)</span> vale 1.</p>
</div>
<h3 id="relación-entre-la-función-generatriz-de-momentos-y-los-momentos"><span class="header-section-number">2.2.6</span> Relación entre la función generatriz de momentos y los momentos</h3>
<p>La razón del nombre que lleva la <strong>función generatriz de momentos</strong> es que podemos obtener todos los momentos de la variable a partir de ella:</p>
<p><l class="prop"> Proposición </l>
Sean <span class="math inline">\(X\)</span> una variable aleatoria con <strong>función generatriz de momentos</strong> <span class="math inline">\(m_X(t)\)</span>. Entonces, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> se puede obtener de la forma siguiente:
<span class="math display">\[
m_n =E\left(X^n\right)=\frac{d}{d t^n}m_X(t)|_{t=0} =m_X^{(n)}(0).
\]</span>
O sea, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> es la derivada <span class="math inline">\(n\)</span>-ésima de la función generatriz de momentos evaluada en <span class="math inline">\(t=0\)</span>.</p>
<h3 id="relación-entre-la-función-generatriz-de-momentos-y-los-momentos-1"><span class="header-section-number">2.2.6</span> Relación entre la función generatriz de momentos y los momentos</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Recordemos la definición de la función generatriz de momentos: <span class="math inline">\(m_X(t)=E\left(\mathrm{e}^{tX}\right).\)</span></p>
<p>La idea de la demostración es probar por inducción que <span class="math inline">\(m_X^{(n)}(t) =E\left(\mathrm{e}^{tX}\cdot X^n\right)\)</span>.</p>
<p>Veámoslo para <span class="math inline">\(n=1\)</span>: <span class="math inline">\(m_X&#39;(t)=E\left(\mathrm{e}^{tX}\cdot X\right)\)</span>.</p>
<p>Seguidamente, apliquemos inducción sobre <span class="math inline">\(n\)</span>. Supongamos que <span class="math inline">\(m_X^{(n)}(t) =E\left(\mathrm{e}^{tX}\cdot X^n\right)\)</span> y veamos que <span class="math inline">\(m_X^{(n+1)}(t) =E\left(\mathrm{e}^{tX}\cdot X^{n+1}\right)\)</span>:
<span class="math inline">\(m_X^{(n+1)}(t) =\frac{d}{dt}(m_X^{(n)}(t)) =\frac{d}{dt}E\left(\mathrm{e}^{tX}\cdot X^n\right) = E\left(\mathrm{e}^{tX}\cdot X^{n+1}\right),\)</span>
tal como queríamos demostrar.</p>
<p>Ahora si aplicamos la expresión demostrada <span class="math inline">\(m_X^{(n)}(t) =E\left(\mathrm{e}^{tX}\cdot X^n\right)\)</span> a <span class="math inline">\(t=0\)</span>, obtenemos:
<span class="math inline">\(m_X^{(n)}(0) =E\left(X^n\right)=m_n,\)</span>
tal como dice la proposición.</p>
</div>
<h3 id="ejemplo-24"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: aplicación de la proposición en el caso en que <span class="math inline">\(X\)</span> es una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>En este caso, recordemos que: <span class="math inline">\(m_X (t)=1+p\left(\mathrm{e}^t -1 \right).\)</span></p>
<p>Se puede comprobar que <span class="math inline">\(m_X^{(n)}(t)=p\mathrm{e}^t\)</span>. Por tanto:
<span class="math display">\[
m_n = m_X^{(n)}(0)=p,
\]</span>
tal como habíamos calculado anteriormente.</p>
</div>
<h3 id="ejemplo-25"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: aplicación de la proposición en el caso en que <span class="math inline">\(X\)</span> es una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>En este caso, recordemos que: <span class="math inline">\(m_X (t)=\frac{\lambda}{\lambda -t},\)</span> para <span class="math inline">\(t&lt;\lambda\)</span> pero como <span class="math inline">\(\lambda &gt;0\)</span>, <span class="math inline">\(t=0\)</span> cumple la expresión anterior.</p>
<p>Dejamos como ejercicio para el lector comprobar que: <span class="math inline">\(m_X^{(n)}(t)=\frac{\lambda n!}{(\lambda-t)^{n+1}}\)</span>.</p>
<p>Por tanto:
<span class="math display">\[
m_n = m_X^{(n)}(0) = \frac{\lambda n!}{\lambda^{n+1}}=\frac{n!}{\lambda^n},
\]</span>
expresión que ya habíamos obtenido anteriormente.</p>
</div>
<h3 id="ejemplo-26"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: aplicación de la proposición en el caso en que <span class="math inline">\(X\)</span> es una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>En este caso, recordemos que: <span class="math inline">\(m_X (t)=\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}}.\)</span></p>
<p>Aplicando la fórmula de los momentos para <span class="math inline">\(n=1\)</span> obtenemos:
<span class="math inline">\(m&#39;(t)=\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}} \left(\mu+t\sigma^2\right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale:
<span class="math inline">\(m&#39;(0)=\mu=E(X)\)</span>, tal como ya sabemos.</p>
<p>Si la aplicamos para <span class="math inline">\(n=2\)</span>, obtenemos:
<span class="math inline">\(m&#39;&#39;(t)=\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}} \left((\mu+t\sigma^2)^2+ \sigma^2 \right) =\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}} \left(t^2\sigma^4+\mu^2+\sigma^2+ 2t\mu\sigma^2 \right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale:
<span class="math inline">\(m&#39;&#39;(0)=\mu^2+\sigma^2=E\left(X^2\right)\)</span>, tal como ya sabemos.</p>
<p>Para <span class="math inline">\(n=3\)</span>m obtenemos:
<span class="math inline">\(m&#39;&#39;&#39;(t)=e^{\mu t+\frac{\sigma ^2 t^2}{2}}\left(\mu +\sigma ^2 t\right) \left(\left(\mu +\sigma ^2 t\right)^2+3 \sigma ^2\right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale: <span class="math inline">\(m&#39;&#39;&#39;(0)=3\sigma^2\mu = E\left(X^3\right)\)</span>, valor que correspondería al momento de tercer orden de <span class="math inline">\(X\)</span>.</p>
<p>Por último, para <span class="math inline">\(n=4\)</span>, obtenemos:
<span class="math inline">\(m^{(iv)}(t)=e^{\mu t+\frac{\sigma ^2 t^2}{2}}  \left(6 \sigma ^2 \left(\mu  +\sigma ^2 t\right)^2+\left(\mu  +\sigma ^2 t\right)^4+3 \sigma  ^4\right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale: <span class="math inline">\(m^{(iv)}(0)=6\sigma^2\mu^2+\mu^4+3\sigma^4=E\left(X^4\right)\)</span>, valor que correspondería al momento de cuarto orden de <span class="math inline">\(X\)</span>.</p>
</div>
<h3 id="función-característica"><span class="header-section-number">2.2.6</span> Función característica</h3>
<p><l class="definition">Definición de función característica: </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria <span class="math inline">\(X\)</span> con función de probabilidad <span class="math inline">\(P_X\)</span> en el caso discreto o función
de densidad <span class="math inline">\(f_X\)</span> en el caso continuo.</p>
<p>Sea <span class="math inline">\(w\in\mathbb{R}\)</span> un valor real cualquiera.</p>
<p>Definimos la función característica <span class="math inline">\(\phi_X(w)\)</span> en el valor <span class="math inline">\(w\)</span> como: <span class="math inline">\(\phi_X(w)=E\left(\mathrm{e}^{\mathrm{i} w X}\right),\)</span> donde <span class="math inline">\(\mathrm{i}\)</span> es el número complejo <span class="math inline">\(\mathrm{i}=\sqrt{-1}\)</span>.</p>
<h3 id="función-característica-1"><span class="header-section-number">2.2.6</span> Función característica</h3>
<p><l class="observ">Observación: </l>
Si <span class="math inline">\(X\)</span> es una variable continua, la <strong>función característica</strong> <span class="math inline">\(\phi_X(w)\)</span> puede interpretarse como la <strong>transformada de Fourier</strong> de la <strong>función de densidad</strong> de <span class="math inline">\(X\)</span>:
<span class="math inline">\(\phi(w)=\int_{-\infty}^\infty f_X(x)\mathrm{e}^{\mathrm{i}w x}\, dx.\)</span></p>
<p>Por tanto, usando la fórmula de la <strong>antitransformada de Fourier</strong>, podemos escribir la <strong>función de densidad</strong> <span class="math inline">\(f_X(x)\)</span> como función de la <strong>función característica</strong> de <span class="math inline">\(X\)</span>, <span class="math inline">\(\phi(w)\)</span>:
<span class="math inline">\(f_X(x)=\frac{1}{2\pi}\int_{-\infty}^\infty \phi_X(w)\mathrm{e}^{-\mathrm{i}w x}\, dw.\)</span></p>
<h3 id="función-característica-2"><span class="header-section-number">2.2.6</span> Función característica</h3>
<p><l class="observ">Observación: </l>
En el caso discreto, o sea, Si <span class="math inline">\(X\)</span> es una variable discreta, la <strong>función característica</strong> <span class="math inline">\(\phi_X(w)\)</span> se escribe como función de la <strong>función de probabilidad</strong> <span class="math inline">\(P_X(x_k)\)</span> con <strong>Dominio</strong> <span class="math inline">\(D_X=\{x_k,\ k\}\)</span> como:
<span class="math inline">\(\phi(w)=\sum_{k} P_X(x_k)\mathrm{e}^{\mathrm{i}w x_k}.\)</span></p>
<p>En los casos en que los <span class="math inline">\(x_k\)</span> sean enteros, <span class="math inline">\(x_k=k\)</span>, que son la mayoría, la ecuación anterior es la <strong>tranformada de Fourier de la secuencia</strong> <span class="math inline">\(P_X(k)\)</span>. Dicha función es una <em>función periódica</em> en <span class="math inline">\(w\)</span> de periodo <span class="math inline">\(2\pi\)</span> ya que <span class="math inline">\(\mathrm{e}^{\mathrm{i}(w+2\pi)k}=\mathrm{e}^{\mathrm{i}wk}.\)</span></p>
<p>Por tanto, usando la fórmula de <strong>inversión</strong>, podemos escribir la <strong>función de probabilidad</strong> <span class="math inline">\(P_X(k)\)</span> como función de la función característica de <span class="math inline">\(X\)</span>, <span class="math inline">\(\phi(w)\)</span>:
<span class="math inline">\(P_X(k)=\frac{1}{2\pi}\int_{0}^{2\pi} \phi_X(w)\mathrm{e}^{-\mathrm{i}w k}\, dw.\)</span></p>
<h3 id="ejemplo-de-cálculo-de-función-característica"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de función característica</h3>
<div class="example">
<p><strong>Ejemplo: cálculo de la función característica para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ p_X(1)=p.
\]</span>
Su función característica será:
<span class="math display">\[
\phi_X (w)=E\left(\mathrm{e}^{\mathrm{i}wX}\right) =p\mathrm{e}^{\mathrm{i}w\cdot 1}+(1-p)\mathrm{e}^{\mathrm{i}w\cdot 0}=p\mathrm{e}^{\mathrm{i}w}+(1-p)=1+p\left(\mathrm{e}^{\mathrm{i}w} -1 \right).
\]</span>
Comprobemos la fórmula de la inversión:
<span class="math display">\[
\begin{array}{rl}
P_X(1) &amp; = \frac{1}{2\pi}\int_0^{2\pi} \left(1+p\left(\mathrm{e}^{\mathrm{i}w} -1 \right)\right) e^{-\mathrm{i}w\cdot 1}\, dw =\frac{1}{2\pi}\left(\int_0^{2\pi} (1-p)e^{-\mathrm{i}w}\, dw + \int_0^{2\pi} p\, dw\right) \\ &amp; = \frac{1}{2\pi}\left( (1-p) \left[\frac{\mathrm{e}^{-\mathrm{i}w}}{-\mathrm{i}}\right]_0^{2\pi} +2\pi p\right)=\frac{1}{2\pi}\left((1-p)\cdot 0 +2\pi p\right)=p, \\
P_X(0) &amp; = \frac{1}{2\pi}\int_0^{2\pi} \left(1+p\left(\mathrm{e}^{\mathrm{i}w} -1 \right)\right) e^{-\mathrm{i}w\cdot 0}\, dw =\frac{1}{2\pi}\left(\int_0^{2\pi} (1-p) \, dw + \int_0^{2\pi} p \mathrm{e}^{\mathrm{i}w}\, dw\right) \\ &amp; = \frac{1}{2\pi}\left( (1-p) \cdot 2\pi  +p \left[\frac{\mathrm{e}^{\mathrm{i}w}}{\mathrm{i}}\right]_0^{2\pi}\right)=\frac{1}{2\pi}\left((1-p)\cdot 2\pi + p\cdot 0\right)=1-p.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-de-cálculo-de-función-característica-1"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de función característica</h3>
<div class="example">
<p><strong>Ejemplo: cálculo de la función característica para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>. Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(0\)</span>, en caso contrario.</p>
<p>Su función característica será:
<span class="math display">\[
\phi_X (w)=E\left(\mathrm{e}^{\mathrm{i}wX}\right)=\int_0^\infty \mathrm{e}^{\mathrm{i}w x}\lambda \mathrm{e}^{-\lambda x}\, dx = \lambda \int_0^\infty\mathrm{e}^{(\mathrm{i}w-\lambda)x}\, dx = \lambda\left[\frac{\mathrm{e}^{(\mathrm{i}w-\lambda)x}}{\mathrm{i}w-\lambda}\right]_{x=0}^{x=\infty} = \frac{\lambda}{\lambda -\mathrm{i} w}. 
\]</span>
La expresión anterior es válida para todo <span class="math inline">\(w\in\mathbb{R}\)</span> ya que su valor sería:
<span class="math inline">\(\phi_X (w)=\frac{\lambda}{\lambda -\mathrm{i} w}\cdot \frac{\lambda +\mathrm{i} w}{\lambda +\mathrm{i} w}=\frac{\lambda^2+\mathrm{i}\lambda w}{\lambda^2+w^2}=\frac{\lambda^2}{\lambda^2+w^2}+\mathrm{i}\frac{\lambda w}{\lambda^2+w^2}.\)</span>
En la última expresión hemos separado la parte real de la imaginaria.</p>
<p>Calculemos la función de densidad a partir de la función característica:
<span class="math display">\[
f_X(x)=\frac{1}{2\pi}\int_{-\infty}^\infty \frac{\lambda}{\lambda -\mathrm{i} w}\mathrm{e}^{-\mathrm{i}wx}\, dw = a\mathrm{e}^{-a x},
\]</span>
si <span class="math inline">\(x&gt;0\)</span> y <span class="math inline">\(0\)</span> en caso contrario. El cálculo de la integral anterior debe realizarse usando el <em>Teorema de los Residuos</em>, <a href="https://en.wikipedia.org/wiki/Residue_theorem">Residue theorem</a> y se sale de los objetivos de este curso.</p>
</div>
<h3 id="ejemplo-de-cálculo-de-función-característica-2"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de función característica</h3>
<div class="example">
<p><strong>Ejemplo: cálculo de la función característica para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su función característica será:</p>
<p><span class="math display">\[
\begin{array}{rl}
\phi_X (w) &amp; =E\left(\mathrm{e}^{\mathrm{i}w X}\right)=\int_{-\infty}^\infty \mathrm{e}^{\mathrm{i}w x}\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{\mathrm{i}wx-\frac{(x-\mu)^2}{2\sigma^2}}\, dx \\  &amp; =  \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}\left((x-(\sigma^2 \mathrm{i}w+\mu))^2-2\sigma^2 \mathrm{i}w \mu+\sigma^4 w^2\right)}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{e}^{\frac{1}{2}(2 \mathrm{i}w \mu -\sigma^2 w^2)}\int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 \mathrm{i}w+\mu))^2}\, dx\\ &amp;  = \mathrm{e}^{\frac{1}{2}(2 \mathrm{i}w \mu -\sigma^2 w^2)} \left( \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 \mathrm{i}w+\mu))^2}\, dx\right) =  \mathrm{e}^{ \mathrm{i}w \mu -\frac{\sigma^2 w^2}{2}}.
\end{array}
\]</span>
La integral del último paréntesis se resuelve haciento el cambio de variable <span class="math inline">\(u=x-\sigma^2 \mathrm{i}w\)</span> y usando que la integral de la función de densidad de <span class="math inline">\(X\)</span> sobre todo <span class="math inline">\(\mathbb{R}\)</span> vale 1.</p>
</div>
<h3 id="ejemplo-de-cálculo-de-función-característica-3"><span class="header-section-number">2.2.6</span> Ejemplo de cálculo de función característica</h3>
<div class="example">
<p><strong>Ejemplo: cálculo de la función característica para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Calculemos la función de densidad a partir de la función característica:
<span class="math display">\[
\begin{array}{rl}
f_X(x) &amp; =\frac{1}{2\pi}\int_{-\infty}^\infty \mathrm{e}^{ \mathrm{i}w \mu -\frac{\sigma^2 w^2}{2}}\mathrm{e}^{-\mathrm{i} w x}\, dw = \frac{1}{2\pi}\int_{-\infty}^\infty \mathrm{e}^{\left(\frac{\mathrm{i}w\sigma}{\sqrt{2}}+\frac{\mu-x}{\sigma\sqrt{2}}\right)^2-\frac{(\mu-x)^2}{2\sigma^2}}\, dw =\frac{1}{2\pi}\mathrm{e}^{-\frac{(\mu-x)^2}{2\sigma^2}}\int_{-\infty}^\infty \mathrm{e}^{\left(\frac{\mathrm{i}w\sigma}{\sqrt{2}}+\frac{\mu-x}{\sigma\sqrt{2}}\right)^2}\, dw \\ &amp; = \frac{1}{2\pi}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\int_{-\infty}^\infty \mathrm{e}^{-\left(\frac{w\sigma}{\sqrt{2}}+\frac{\mu-x}{\mathrm{i}\sigma\sqrt{2}}\right)^2}\, dw \stackrel{\mbox{cambio de variable } u=\frac{w\sigma}{\sqrt{2}}+\frac{\mu-x}{\mathrm{i}\sigma\sqrt{2}}}{=} \frac{1}{2\pi}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\int_{-\infty}^\infty \frac{\sqrt{2}}{\sigma}\mathrm{e}^{-u^2}\, du \\ &amp; \stackrel{\int_{-\infty}^\infty \mathrm{e}^{-u^2}\, du =\sqrt{\pi}}{=} \frac{1}{\sqrt{2}\pi\sigma} \mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}} \sqrt{\pi} = \frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},
\end{array}
\]</span>
función que coincide con la densidad de la distribución <span class="math inline">\(N(\mu,\sigma)\)</span>.</p>
</div>
<h3 id="relación-entre-la-función-característica-y-los-momentos"><span class="header-section-number">2.2.6</span> Relación entre la función característica y los momentos</h3>
<p>La relación entre la <strong>función característica</strong> y los <strong>momentos</strong> es la siguiente:</p>
<p><l class="prop"> Proposición </l>
Sean <span class="math inline">\(X\)</span> una variable aleatoria con <strong>función característica</strong> <span class="math inline">\(\phi_X(w)\)</span>. Entonces, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> se puede obtener de la forma siguiente:
<span class="math display">\[
m_n =E\left(X^n\right)=\frac{1}{\mathrm{i}^n}\frac{d}{d w^n}\phi_X(w)|_{w=0} =\frac{1}{\mathrm{i}^n}\phi_X^{(n)}(0).
\]</span>
O sea, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> es la derivada <span class="math inline">\(n\)</span>-ésima de la función característica evaluada en <span class="math inline">\(w=0\)</span> dividido por <span class="math inline">\(\mathrm{i}^n\)</span>.</p>
<h3 id="relación-entre-la-función-característica-y-los-momentos-1"><span class="header-section-number">2.2.6</span> Relación entre la función característica y los momentos</h3>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>La demostración se realiza de forma similar a la demostración de la proposición que relaciona la función generatriz de momentos y los momentos.</p>
<p>Se deja como ejercicio al lector.</p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Realizar los mismos ejemplos que los realizados para la función generatriz de momentos. O sea:</p>
<ul>
<li><p>Si <span class="math inline">\(X\)</span> es una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>, demostrar usando la función característica que para todo <span class="math inline">\(n\)</span>, <span class="math inline">\(m_n = E\left(X^n\right)=p\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> es una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span>, demostrar usando la función característica que para todo <span class="math inline">\(n\)</span>, <span class="math inline">\(m_n = E\left(X^n\right)=\frac{n!}{\lambda^n}\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> es una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>, demostrar usando la función característica que <span class="math inline">\(E(X)=\mu\)</span>, <span class="math inline">\(E\left(X^2\right)=\mu^2+\sigma^2\)</span>, <span class="math inline">\(E\left(X^3\right)=3\sigma^2\mu\)</span> y <span class="math inline">\(E\left(X^4\right)=6\sigma^2\mu^2+\mu^4+3\sigma^4\)</span>.</p></li>
</ul>
</div>
<h2 id="fiabilidad"><span class="header-section-number">2.2.6</span> Fiabilidad</h2>
<h3 id="introducción-1"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Sea <span class="math inline">\(T\geq 0\)</span> una variable aleatoria que nos da, por ejemplo, el tiempo de vida de cierto componente o dispositivo.</p>
<p>Vamos a definir medidas para estudiar la fiabilidad de este tipo de variables aleatorias.</p>
<p><l class="definition">Definición:</l>
Sea <span class="math inline">\(T\geq 0\)</span> una variable aleatoria. La <strong>fiabilidad</strong> de <span class="math inline">\(T\)</span> en el tiempo <span class="math inline">\(t\)</span> se define como la probabilidad que el sistema, componente o dispositivo funcione en el tiempo <span class="math inline">\(t\)</span>: <span class="math inline">\(R(t)=P(T&gt;t)\)</span>.</p>
<p><l class="observ">Observación:</l>
Dada una variable <span class="math inline">\(T\geq 0\)</span>, la relación existente entre la <strong>fiabilidad</strong> <span class="math inline">\(R\)</span> y la <strong>función de distribución</strong> <span class="math inline">\(F_T\)</span> es la siguiente:
<span class="math display">\[
R(t)=P(T&gt;t)=1-P(T\leq t)=1-F_T (t)
\]</span></p>
<h3 id="tiempo-medio-de-vida"><span class="header-section-number">2.2.6</span> Tiempo medio de vida</h3>
<p><l class="observ">Observación:</l>
Dada una variable <span class="math inline">\(T\geq 0\)</span> continua, el <strong>tiempo medio de vida</strong> de la variable <span class="math inline">\(T\)</span> sería <span class="math inline">\(E(T)\)</span>. Entonces, este <strong>tiempo medio de vida</strong> se puede calcular como:
<span class="math inline">\(E(T)=\int_0^\infty R(t)\, dt.\)</span></p>
<p>Veámoslo. Para ello basta ver que <span class="math inline">\(E(T)=\int_0^\infty (1-F_T(t))\, dt\)</span>, donde <span class="math inline">\(F_T(t)\)</span> es la función de distribución de la variable <span class="math inline">\(T\)</span>:
<span class="math display">\[
\begin{array}{rl}
E(T) &amp; =\int_{t=0}^{t=\infty} 1-F_T(t)\, dt=\int_{t=0}^{t=\infty}\int_{u=t}^{u=\infty} f_T(u)\,du\,dt \\ &amp; =\int_{u=0}^{u=\infty} f_T(u)\int_{t=0}^{t=u} \, dt\, du =\int_{u=0}^{u=\infty} f_T(u)\cdot u\, du = E(T),
\end{array}
\]</span>
donde <span class="math inline">\(f_T(u)\)</span> seria la función de densidad de la variable <span class="math inline">\(T\)</span> en el valor <span class="math inline">\(u\)</span>.</p>
<h3 id="ejemplo-27"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Sea <span class="math inline">\(T\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
La fiabilidad de <span class="math inline">\(T\)</span> sería: <span class="math inline">\(R(t)=P(T&gt;t)=1-F_T(t)=\mathrm{e}^{-\lambda t}\)</span>:
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<h2 id="generación-de-muestras-de-variables-aleatorias-por-ordenador"><span class="header-section-number">2.2.6</span> Generación de muestras de variables aleatorias por ordenador</h2>
<h3 id="introducción-2"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>La simulación por <strong>computadora</strong> de cualquier fenómeno aleatorio implica la <strong>generación de variables aleatorias</strong> con distribuciones prefijadas de antemano.</p>
<p>Por ejemplo, la simulación de un sistema de colas implica generar el tiempo entre las llegadas de los clientes, así como los tiempos de servicio de cada cliente.</p>
<p>Fijémonos que fijar la variable aleatoria <span class="math inline">\(X\)</span> es equivalente a fijar la <strong>función de distribución <span class="math inline">\(F_X(x)\)</span></strong> o la <strong>función de densidad <span class="math inline">\(f_X(x)\)</span></strong> en el caso continuo o la <strong>función de probabilidad <span class="math inline">\(P_X(x)\)</span></strong> en el caso discreto.</p>
<p>Todos los métodos que vamos a describir presuponen que podemos generar <strong>números aleatorios</strong> que se distribuyen <strong>uniformemente</strong> entre 0 y 1. En <code>R</code> se puede hacer usando la función <code>runif(n)</code>, donde <code>n</code> es la cantidad de números aleatorios entre 0 y 1 a generar.</p>
<h3 id="método-de-transformación"><span class="header-section-number">2.2.6</span> Método de transformación</h3>
<p>El <strong>método de transformación</strong> se basa en el resultado siguiente:</p>
<p><l class="prop">Proposición: </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria con función de distribución <span class="math inline">\(F_X(x)\)</span>. Supongamos que <span class="math inline">\(F_X(x)\)</span> es estrictamente creciente o que existe <span class="math inline">\(F_X^{-1}(y)\)</span>, para todo <span class="math inline">\(y\in [0,1]\)</span>. Sea <span class="math inline">\(Y\)</span> la variable aleatoria definida como: <span class="math inline">\(Y=F_X(X)\)</span>. Entonces la distribución de <span class="math inline">\(Y\)</span> es uniforme en el intervalo <span class="math inline">\([0,1]\)</span>.</p>
<div class="dem">
<p><strong>Demostración:</strong></p>
<p>Claramente, por propia definición de <span class="math inline">\(Y\)</span>, tenemos que el dominio de <span class="math inline">\(Y\)</span> es <span class="math inline">\([0,1]\)</span> ya que el conjunto recorrido de la función de distribución de cualquier variable es el intervalo <span class="math inline">\([0,1]\)</span>.</p>
<p>Para ver que la distribución de <span class="math inline">\(Y\)</span> es <span class="math inline">\(U[0,1]\)</span> basta comprobar que <span class="math inline">\(F_Y(y)=y\)</span>, para todo <span class="math inline">\(y\in [0,1]\)</span>:
<span class="math display">\[
\begin{array}{rl}
F_Y(y) &amp; =P(Y\leq y)=P(F_X(X)\leq y)\stackrel{\mbox{usando que $F_X$ es estrictamente creciente}}{=} P(X\leq F_X^{-1}(y)) \\ &amp; =F_X(F_X^{-1}(y))=y.
\end{array}
\]</span></p>
</div>
<h3 id="método-de-transformación-1"><span class="header-section-number">2.2.6</span> Método de transformación</h3>
<p>Usando la proposición anterior, dada una variable <span class="math inline">\(X\)</span>, como la distribución de la variable aleatoria <span class="math inline">\(Y=F_X(X)\)</span> es <span class="math inline">\(U[0,1]\)</span>, si hacemos <span class="math inline">\(X=F_X^{-1}(Y)\)</span>, tendremos que si sabemos generar una muestra de <span class="math inline">\(Y\)</span>, aplicándole a la muestra la función <span class="math inline">\(F_X^{-1}\)</span> tendremos generada una muestra de <span class="math inline">\(X\)</span>.</p>
<h3 id="ejemplo-28"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: generar una muestra de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Recordemos que si <span class="math inline">\(X\)</span> es exponencial de parámetro <span class="math inline">\(\lambda\)</span>, su función de distribución es: <span class="math inline">\(F_X(x)=1-\mathrm{e}^{-\lambda x}\)</span>.</p>
<p>Hallemos a continuación <span class="math inline">\(F_X^{-1}\)</span>:
<span class="math display">\[
y=1-\mathrm{e}^{-\lambda x},\ \Leftrightarrow 1-y=\mathrm{e}^{-\lambda x},\ \Leftrightarrow \ln(1-y)=-\lambda x,\ \Leftrightarrow x=-\frac{1}{\lambda}\ln(1-y).
\]</span>
Por tanto, <span class="math inline">\(F_X^{-1}(y)=-\frac{1}{\lambda}\ln(1-y)\)</span>.</p>
</div>
<h3 id="ejemplo-29"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: generar una muestra de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Generemos una muestra con <code>R</code> de 25 valores de una variable exponencial de parámetro <span class="math inline">\(\lambda=2\)</span> usando el método anterior:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" data-line-number="1">n=<span class="dv">25</span></a>
<a class="sourceLine" id="cb52-2" data-line-number="2">lambda=<span class="dv">2</span></a>
<a class="sourceLine" id="cb52-3" data-line-number="3">muestra.y =<span class="st"> </span><span class="kw">runif</span>(n)</a>
<a class="sourceLine" id="cb52-4" data-line-number="4">muestra.x =<span class="st"> </span><span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>lambda)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>muestra.y)</a>
<a class="sourceLine" id="cb52-5" data-line-number="5">muestra.x</a></code></pre></div>
<pre><code>##  [1] 0.76622450 0.31847128 0.16489516 0.10395200 0.39973729 0.16154405
##  [7] 0.87268703 0.14770213 0.37699316 0.91788067 0.87034314 0.09843204
## [13] 0.34849525 0.20425737 0.12971050 0.86116841 0.29512710 0.18396357
## [19] 0.49335130 0.13530541 0.21354477 0.07805220 1.34093300 0.32649289
## [25] 0.40878747</code></pre>
</div>
<h3 id="ejemplo-30"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: generar una muestra de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Vamos a testear si nuestro método funciona.</p>
<p>Para ello generaremos una muestra de 500 valores usando el método de transformación y dibujaremos su <strong>histograma de frecuencias relativas</strong>.</p>
<p>Seguidamente dibujaremos la <strong>función de densidad de la variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong> y compararemos los resultados:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" data-line-number="1">n=<span class="dv">500</span></a>
<a class="sourceLine" id="cb54-2" data-line-number="2">lambda=<span class="dv">2</span></a>
<a class="sourceLine" id="cb54-3" data-line-number="3">muestra.y =<span class="st"> </span><span class="kw">runif</span>(n)</a>
<a class="sourceLine" id="cb54-4" data-line-number="4">muestra.x =<span class="st"> </span><span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>lambda)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>muestra.y)</a>
<a class="sourceLine" id="cb54-5" data-line-number="5"><span class="kw">hist</span>(muestra.x,<span class="dt">freq=</span><span class="ot">FALSE</span>,<span class="dt">main=</span><span class="st">&quot;Histograma de la muestra&quot;</span>)</a>
<a class="sourceLine" id="cb54-6" data-line-number="6">x2=<span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">0</span>,<span class="dt">to=</span><span class="fl">2.5</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb54-7" data-line-number="7"><span class="kw">lines</span>(x2,<span class="kw">dexp</span>(x2,lambda),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
</div>
<h3 id="ejemplo-31"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<strong>Ejemplo: generar una muestra de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
</div>
<h3 id="método-de-rechazo"><span class="header-section-number">2.2.6</span> Método de rechazo</h3>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria continua tal que su función de densidad verifica:</p>
<ul>
<li>Existen valores <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> tal que <span class="math inline">\(f_X(x)= 0\)</span> si <span class="math inline">\(x\not\in [a,b]\)</span>.</li>
<li>Existen valores <span class="math inline">\(c\)</span> y <span class="math inline">\(d\)</span> tal que <span class="math inline">\(f_X(x)\in [c,d]\)</span>, si <span class="math inline">\(x\in [a,b]\)</span>.</li>
</ul>
<p>En resumen, los puntos <span class="math inline">\((x,f(x))\)</span> pertenecen al rectángulo <span class="math inline">\([a,b]\times [c,d]\)</span> y en caso contrario <span class="math inline">\(f_X(x)=0\)</span>.</p>
<p>En el gráfico siguiente, <span class="math inline">\(a=0\)</span>, <span class="math inline">\(b=2\)</span>, <span class="math inline">\(c=0\)</span> y <span class="math inline">\(d=1\)</span>.</p>
<h3 id="método-de-rechazo-1"><span class="header-section-number">2.2.6</span> Método de rechazo</h3>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<h3 id="método-de-rechazo-2"><span class="header-section-number">2.2.6</span> Método de rechazo</h3>
<p>Para generar una <strong>muestra aleatoria</strong> de la variable <span class="math inline">\(X\)</span>, hacemos lo siguiente:</p>
<ol style="list-style-type: decimal">
<li><p>generamos un valor aleatorio <span class="math inline">\(x\)</span> entre <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span>.</p></li>
<li><p>generamos un valor aleatorio <span class="math inline">\(y\)</span> entre <span class="math inline">\(c\)</span> y <span class="math inline">\(d\)</span>.</p></li>
<li><p>si <span class="math inline">\(y\leq f_X(x)\)</span>, aceptamos <span class="math inline">\(x\)</span> como valor de la muestra. En caso contrario, volvemos a empezar en 1.</p></li>
</ol>
<h3 id="ejemplo-32"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>El gráfico de la figura anterior corresponde a la función de densidad siguiente:
<span class="math display">\[
f_X(x)=\begin{cases}
x, &amp; \mbox{ si }0\leq x\leq 1,\\
2-x, &amp; \mbox{ si }1\leq x\leq 2,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-33"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Vamos a generar una muestra de <span class="math inline">\(25\)</span> valores usando el <strong>método del rechazo</strong>:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1">a=<span class="dv">0</span>; b=<span class="dv">2</span>; c=<span class="dv">0</span>; d=<span class="dv">1</span>; n=<span class="dv">25</span>; i=<span class="dv">1</span>;</a>
<a class="sourceLine" id="cb55-2" data-line-number="2">f =<span class="st"> </span><span class="cf">function</span>(x){<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&lt;=</span><span class="dv">1</span>,x,<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">1</span><span class="op">&amp;</span>x<span class="op">&lt;=</span><span class="dv">2</span>,<span class="dv">2</span><span class="op">-</span>x,<span class="dv">0</span>))}</a>
<a class="sourceLine" id="cb55-3" data-line-number="3">muestra=<span class="kw">c</span>()</a>
<a class="sourceLine" id="cb55-4" data-line-number="4"><span class="cf">while</span>(i <span class="op">&lt;=</span>n){</a>
<a class="sourceLine" id="cb55-5" data-line-number="5">  x=<span class="kw">runif</span>(<span class="dv">1</span>,a,b)</a>
<a class="sourceLine" id="cb55-6" data-line-number="6">  y=<span class="kw">runif</span>(<span class="dv">1</span>,c,d)</a>
<a class="sourceLine" id="cb55-7" data-line-number="7">  <span class="cf">if</span>(y <span class="op">&lt;=</span><span class="st"> </span><span class="kw">f</span>(x)){muestra=<span class="kw">c</span>(muestra,x); i=i<span class="op">+</span><span class="dv">1</span>}</a>
<a class="sourceLine" id="cb55-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb55-9" data-line-number="9">muestra</a></code></pre></div>
<pre><code>##  [1] 0.8934245 1.0241151 0.7485943 1.1318743 1.1174486 0.8956145 1.6270601
##  [8] 0.6737312 0.9584882 0.5762514 0.7390811 0.4276671 1.5210227 0.8506008
## [15] 0.4126257 0.9897587 1.2166050 1.6098125 0.8727734 0.8756381 1.4853939
## [22] 1.1539542 1.2353779 0.5157756 1.2873756</code></pre>
</div>
<h3 id="ejemplo-34"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Como hicimos con el ejemplo del <strong>método de transformación</strong>, vamos a generar una muestra de 500 valores de la variable <span class="math inline">\(X\)</span>, vamos a dibujar el <strong>histograma de frecuencias relativas</strong> junto con la función de densidad para ver si ésta se aproxima a dicho histograma:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1">a=<span class="dv">0</span>; b=<span class="dv">2</span>; c=<span class="dv">0</span>; d=<span class="dv">1</span>; n=<span class="dv">500</span>; i=<span class="dv">1</span>;</a>
<a class="sourceLine" id="cb57-2" data-line-number="2">f =<span class="st"> </span><span class="cf">function</span>(x){<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&lt;=</span><span class="dv">1</span>,x,<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">1</span><span class="op">&amp;</span>x<span class="op">&lt;=</span><span class="dv">2</span>,<span class="dv">2</span><span class="op">-</span>x,<span class="dv">0</span>))}</a>
<a class="sourceLine" id="cb57-3" data-line-number="3">muestra=<span class="kw">c</span>()</a>
<a class="sourceLine" id="cb57-4" data-line-number="4"><span class="cf">while</span>(i <span class="op">&lt;=</span>n){</a>
<a class="sourceLine" id="cb57-5" data-line-number="5">  x=<span class="kw">runif</span>(<span class="dv">1</span>,a,b)</a>
<a class="sourceLine" id="cb57-6" data-line-number="6">  y=<span class="kw">runif</span>(<span class="dv">1</span>,c,d)</a>
<a class="sourceLine" id="cb57-7" data-line-number="7">  <span class="cf">if</span>(y <span class="op">&lt;=</span><span class="st"> </span><span class="kw">f</span>(x)){muestra=<span class="kw">c</span>(muestra,x); i=i<span class="op">+</span><span class="dv">1</span>}</a>
<a class="sourceLine" id="cb57-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb57-9" data-line-number="9"><span class="kw">hist</span>(muestra,<span class="dt">freq=</span><span class="ot">FALSE</span>,<span class="dt">main=</span><span class="st">&quot;Histograma de la muestra&quot;</span>)</a>
<a class="sourceLine" id="cb57-10" data-line-number="10">x2=<span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">0</span>,<span class="dt">to=</span><span class="dv">2</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb57-11" data-line-number="11"><span class="kw">lines</span>(x2,<span class="kw">f</span>(x2),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
</div>
<h3 id="ejemplo-35"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
<h2 id="entropía"><span class="header-section-number">2.2.6</span> Entropía</h2>
<h3 id="introducción-3"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>La <strong>entropía</strong> es una medida de la <strong>incertidumbre</strong> en un experimento aleatorio.</p>
<p>Veremos cómo la <strong>entropía</strong> cuantifica la <strong>incertidumbre</strong> por la cantidad de <strong>información</strong> requerida para especificar el resultado de un experimento aleatorio.</p>
<h3 id="entropía-de-una-variable-aleatoria"><span class="header-section-number">2.2.6</span> Entropía de una variable aleatoria</h3>
<p>Supongamos que tenemos una variable aleatoria <span class="math inline">\(X\)</span> discreta con valores enteros: <span class="math inline">\(D_X=\{1,2,\ldots,N\}\)</span>.</p>
<p>Sea <span class="math inline">\(k\in D_X\)</span> un valor de la variable. Estamos interesados en cuantificar la <strong>incertidumbre</strong> del suceso <span class="math inline">\(A_k =\{X=k\}\)</span>.</p>
<p>O sea, cuánta <strong>menos incertidumbre</strong> tenga <span class="math inline">\(A_k\)</span>, más <strong>alta será su probabilidad</strong>, y cuánta <strong>más incertidumbre</strong>, <strong>menos probabilidad</strong> de aparecer <span class="math inline">\(A_k\)</span>.</p>
<h3 id="entropía-de-una-variable-aleatoria-1"><span class="header-section-number">2.2.6</span> Entropía de una variable aleatoria</h3>
<p>Una medida que cumple las condiciones anteriores es la siguiente: <span class="math inline">\(I(A_k)=I(\{X=k\})=\ln\left(\frac{1}{P(X=k)}\right)=-\ln\left(P(X=k)\right).\)</span></p>
<p>Por ejemplo, si <span class="math inline">\(P(A_k)=1\)</span>, o sea, <span class="math inline">\(A_k\)</span> aparece “<strong>seguro</strong>”, entonces tiene incertidumbre <strong>nula</strong>, <span class="math inline">\(I(A_k)=0\)</span>, y si <span class="math inline">\(P(A_k)=0\)</span>, o sea, <span class="math inline">\(A_k\)</span> no aparece “<strong>nunca</strong>”, tiene incertidumbre <strong>máxima</strong>, <span class="math inline">\(I(A_k)=\infty\)</span>.</p>
<h3 id="entropía-de-una-variable-aleatoria-2"><span class="header-section-number">2.2.6</span> Entropía de una variable aleatoria</h3>
<p>La motivación anterior hace que definamos la <strong>entropía</strong> de una variable aleatoria de la forma siguiente:</p>
<p><l class="definition">Definición:</l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria con función de densidad <span class="math inline">\(f_X(x)\)</span> en el caso continuo o función de probabilidad <span class="math inline">\(P_X(x)\)</span> en el caso discreto. Definimos <strong>entropía de X</strong> como:
<span class="math inline">\(H_X = E\left(-\ln(f_X)\right)=\int_{-\infty}^\infty -\ln(f_X(x)) f_X(x)\, dx,\)</span> en el caso continuo y,
<span class="math inline">\(H_X = E\left(-\ln(P_X)\right)=\sum_{x_k\in D_X} -\ln(P_X(x_k)) P_X(x_k),\)</span> en el caso discreto.</p>
<h3 id="ejemplo-36"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: entropía de una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>.</p>
<p>Recordemos que su función de probabilidad <span class="math inline">\(P_X\)</span> es: <span class="math inline">\(P_X(0)=1-p=q,\)</span> <span class="math inline">\(P_X(1)=p\)</span>.</p>
<p>La entropía de <span class="math inline">\(X\)</span> será:
<span class="math display">\[
H_X = E\left(-\ln(P_X)\right) = -(1-p)\cdot \ln(1-p)-p\cdot \ln p.
\]</span>
El gráfico de la entropía se puede observar en el gráfico siguiente donde <span class="math inline">\(X\)</span> tiene entropía máxima cuando <span class="math inline">\(p=\frac{1}{2}\)</span> que sería cuando <span class="math inline">\(X\)</span> tiene incertidumbre máxima al tratar de adivinar el resultado de <span class="math inline">\(X\)</span> y <span class="math inline">\(X\)</span> tiene entropía mínima cuando <span class="math inline">\(p=0\)</span> o <span class="math inline">\(p=1\)</span> ya que en estos casos el resultado de <span class="math inline">\(X\)</span> sería siempre <span class="math inline">\(0\)</span> o <span class="math inline">\(1\)</span>, respectivamente.</p>
</div>
<h3 id="ejemplo-37"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<strong>Ejemplo: entropía de una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
</div>
</div>
<h3 id="ejemplo-38"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Entropía de una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x}\)</span>, si <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(f_X(x)=0\)</span>, en caso contrario.</p>
<p>Su entropía será:
<span class="math display">\[
\begin{array}{rl}
H_X &amp; = E\left(-\ln(f_X)\right)=-\int_0^\infty \ln\left(\lambda\mathrm{e}^{-\lambda x}\right)\lambda\mathrm{e}^{-\lambda x}\, dx = -\lambda \int_0^\infty (\ln(\lambda) -\lambda x)\mathrm{e}^{-\lambda x}\, dx \\ &amp; = -\ln (\lambda)\int_0^\infty \lambda\mathrm{e}^{-\lambda x}\, dx+\lambda \int_0^\infty \lambda x \mathrm{e}^{-\lambda x}\, dx =-\ln(\lambda)\int_0^\infty f_X(x)\, dx +\lambda E(X)\\ &amp; =-\ln(\lambda)+\lambda \frac{1}{\lambda} =1-\ln(\lambda).
\end{array}
\]</span>
El gráfico de la entropía se puede observar en el gráfico siguiente donde <span class="math inline">\(X\)</span> tiene entropía máxima cuando <span class="math inline">\(\lambda=0\)</span> que sería cuando <span class="math inline">\(X\)</span> tiene incertidumbre máxima al tratar de adivinar el resultado de <span class="math inline">\(X\)</span> al tener media <span class="math inline">\(E(X)=\frac{1}{\lambda}=\infty\)</span> y <span class="math inline">\(X\)</span> tiene entropía mínima cuando <span class="math inline">\(\lambda\)</span> tiende a <span class="math inline">\(\infty\)</span> ya que su media <span class="math inline">\(E(X)=\frac{1}{\lambda}\)</span> tendería a 0.</p>
</div>
<h3 id="ejemplo-39"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<strong>Ejemplo: entropía de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
</div>

<h1 id="vectores-aleatorios-bidimensionales"><span class="header-section-number">2.2.6</span> Vectores aleatorios bidimensionales</h1>
<h2 id="dos-variables-aleatorias"><span class="header-section-number">2.2.6</span> Dos variables aleatorias</h2>
<h3 id="introducción-4"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Muchos experimentos aleatorios involucran varias variables aleatorias.</p>
<p>Por ejemplo, dado un individuo de 30 años escogido al azar de una cierta población, medir su altura y su peso conjuntamente.</p>
<p>Otro ejemplo más complejo sería la medición continuada de un <em>fenómeno aleatorio</em> que se repite en el tiempo, como sería medir la temperatura media un día determinado del año, por ejemplo el día 1 de enero en un cierto lugar.</p>
<p>La variable aleatoria que nos da la medición en 10 años sería una variable aleatoria de varias variables que involucra 10 variables aleatorias supuestas independientes e idénticamente distribuidas, lo que en <strong>estadística inferencial</strong> se le llama una <strong>muestra aleatoria simple</strong>.</p>
<h3 id="dos-variables-aleatorias.-definición"><span class="header-section-number">2.2.6</span> Dos variables aleatorias. Definición</h3>
<p>Recordemos que una <strong>variable aleatoria</strong> <span class="math inline">\(X\)</span> es una aplicación que toma valores numéricos para cada resultado de un experimento aleatorio:
<span class="math display">\[
\begin{array}{rl}
(X,Y): \Omega &amp; \longrightarrow \mathbb{R}\\
w &amp; \longrightarrow X(w).
\end{array}
\]</span>
A partir de la definición anterior, generalizamos la noción de <strong>variable aleatoria unidimensional</strong> a <strong>variable aleatoria bidimensional</strong>:</p>
<p><l class="definition">Definición de variable aleatoria bidimensional:</l>
Dado un experimento aleatorio con <strong>espacio muestral</strong> <span class="math inline">\(\Omega\)</span>, definimos <strong>variable aleatoria bidimensional</strong> <span class="math inline">\((X,Y)\)</span> a toda aplicación
<span class="math display">\[
\begin{array}{rl}
X: \Omega &amp; \longrightarrow \mathbb{R}^2\\
w &amp; \longrightarrow (X(w),Y(w)).
\end{array}
\]</span></p>
<h3 id="dos-variables-aleatorias.-ejemplos"><span class="header-section-number">2.2.6</span> Dos variables aleatorias. Ejemplos</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos el experimento aleatorio de lanzar un dado no trucado dos veces.</p>
<p>Sea <span class="math inline">\(S\)</span> la suma de los resultados obtenidos y <span class="math inline">\(P\)</span> el producto de los mismos.</p>
<p>La variable aleatoria <span class="math inline">\((S,P)\)</span> que asigna a cada resultado <span class="math inline">\(w=(x_1,x_2)\)</span> donde <span class="math inline">\(x_1\)</span> es el resultado obtenido en el primer lanzamiento y <span class="math inline">\(x_2\)</span>, el resultado obtenido en el segundo, los valores: <span class="math inline">\(S(w)=x_1+x_2\)</span> y <span class="math inline">\(P(w)=x_1\cdot x_2\)</span> sería una variable aleatoria bidimensional.</p>
<p>El suceso <span class="math inline">\(\{2\leq S\leq 4,\ 3\leq P\leq 6\}\)</span> seria:
<span class="math display">\[
\{2\leq S\leq 4,\ 3\leq P\leq 6\} = \{(1,3),(3,1),(2,2)\}.
\]</span></p>
</div>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos el experimento aleatorio de elegir al azar un estudiante de primer curso de grado. Sea <span class="math inline">\(w\)</span> el estudiante elegido. Consideremos la variable aleatoria <span class="math inline">\((H,W)\)</span> que asigna a dicho estudiante <span class="math inline">\(w\)</span>, <span class="math inline">\(H(w):\)</span> la altura de dicho estudiante en cm. y <span class="math inline">\(W(w):\)</span> el peso de dicho estudiante en kg.</p>
<p>Estamos interesado en sucesos del tipo <span class="math inline">\(A=\{H\leq 176,\ W\leq 85\}\)</span>, o sea, el conjunto de estudiantes que miden menos de 1.76 m. y que pesan menos de 85 kg.</p>
</div>
<!-- ### Dos variables aleatorias. Introducción  -->
<!-- Los sucesos que se derivan de una **variable aleatoria bidimensional** estan especificados por regiones del plano. -->
<!-- Veamos algunos ejemplos: -->
<!-- * Suceso: $\{X+Y\leq 1\}$. Sería la zona sombreada del gráfico siguiente: -->
<!-- ```{r,echo=FALSE} -->
<!-- xmin=-2 -->
<!-- xmax=3 -->
<!-- ymin=-2 -->
<!-- ymax=3 -->
<!-- tolx=0.0075*(xmax-xmin) -->
<!-- toly=0.0075*(ymax-ymin) -->
<!-- quantsx=5 -->
<!-- quantsy=5 -->
<!-- f = function(x){1-x} -->
<!-- plot(c(xmin-tolx,xmax+tolx,xmin-tolx,xmax+tolx),c(ymin-toly,ymin-toly,ymax+toly,ymax+toly),type="n",xlab="",ylab="",xaxt="n",yaxt="n",axes=FALSE) -->
<!-- x=seq(from=xmin,to=xmax,by=0.01) -->
<!-- #points(x,f(x),type="l") -->
<!-- lines(c(0,0),c(ymin,ymax)) -->
<!-- lines(c(xmin,xmax),c(0,0)) -->
<!-- text(xmax-tolx,2*tolx,"x") -->
<!-- text(toly,ymax+toly/2,"y") -->
<!-- for (i in 0:(quantsx)){ -->
<!--   lines(rep(xmin+((xmax-xmin)/quantsx)*i,2),c(-0.5*toly,0.5*toly)) -->
<!--   text(xmin+((xmax-xmin)/quantsx)*i,-3*toly,xmin+((xmax-xmin)/quantsx)*i,cex=0.75)} -->
<!-- for (i in 0:(quantsy)){ -->
<!--   lines(c(-tolx,tolx),rep(ymin+((ymax-ymin)/quantsy)*i,2)) -->
<!--   text(-3*tolx,ymin+((ymax-ymin)/quantsy)*i,ymin+((ymax-ymin)/quantsy)*i,cex=0.75)} -->
<!-- x2=seq(from=xmin,to=xmax,by=0.01) -->
<!-- lines(x2,f(x2),col="red") -->
<!-- #for (i in 1:length(x2)){ -->
<!-- ##  lines(c(x2[i],x2[i]),c(-2,f(x2[i])),lty=1) -->
<!-- #} -->
<!-- polygon(c(xmax,xmin,x2),c(ymin,ymin,f(x2)),col="red") -->
<!-- #for (i in 1:(length(x2)-1)){ -->
<!-- ##  polygon(c(x2[i],x2[i+1],x2[i+1],x2[i]),c(-2,-2,f(x2[i+1]),f(x2[i])),col="blue") -->
<!-- #} -->
<!-- ``` -->
<h3 id="dos-variables-aleatorias.-introducción"><span class="header-section-number">2.2.6</span> Dos variables aleatorias. Introducción</h3>
<p>Los sucesos que se derivan de una <strong>variable aleatoria bidimensional</strong> estan especificados por regiones del plano.
Veamos algunos ejemplos:</p>
<p>Suceso: <span class="math inline">\(\{X+Y\leq 1\}\)</span>. Sería la zona sombreada del gráfico siguiente:</p>
<div class="center">
<p><img src="Images/Bidim1.png" width="450px" /></p>
</div>
<h3 id="dos-variables-aleatorias.-introducción-1"><span class="header-section-number">2.2.6</span> Dos variables aleatorias. Introducción</h3>
<p>Suceso: <span class="math inline">\(\{X^2+Y^2\leq 4\}\)</span>. Sería la zona sombreada del gráfico siguiente:</p>
<div class="center">
<p><img src="Images/Bidim2.png" width="450px" /></p>
</div>
<h3 id="dos-variables-aleatorias.-introducción-2"><span class="header-section-number">2.2.6</span> Dos variables aleatorias. Introducción</h3>
<p>Suceso: <span class="math inline">\(\{\max\{X,Y\}\geq 1\}\)</span>. Sería la zona sombreada del gráfico siguiente:</p>
<div class="center">
<p><img src="Images/Bidim3.png" width="450px" /></p>
</div>
<h3 id="dos-variables-aleatorias.-introducción-3"><span class="header-section-number">2.2.6</span> Dos variables aleatorias. Introducción</h3>
<p>La probabilidad de que la <strong>variable bidimensional</strong> pertenezca a una cierta <strong>región del plano <span class="math inline">\(B\)</span></strong> se define de la forma siguiente:
<span class="math display">\[
P((X,Y)\in B)=P\{w\in \Omega,\ |\ (X(w),Y(w))\in B\},
\]</span>
o sea, la probabilidad anterior es la probabilidad del suceso formado por los elementos de <span class="math inline">\(w\in\Omega\)</span> que cumplen que su <strong>imagen</strong> por la <strong>variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span></strong> esté en <span class="math inline">\(B\)</span>.</p>
<p>Por ejemplo, si consideramos <span class="math inline">\(B=\{X+Y\leq 1\}\)</span>, <span class="math inline">\(P((X,Y)\in B)\)</span> sería la probabilidad del suceso formado por los elementos <span class="math inline">\(w\)</span> de <span class="math inline">\(\Omega\)</span> tal que la suma de las imágenes por <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sea menor o igual que 1: <span class="math inline">\(X(w)+Y(w)\leq 1\)</span>.</p>
<h2 id="función-de-distribución-conjunta"><span class="header-section-number">2.2.6</span> Función de distribución conjunta</h2>
<h3 id="función-de-distribución-conjunta.-introducción"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Introducción</h3>
<p>Dada una <strong>variable aleatoria bidimensional</strong> <span class="math inline">\((X,Y)\)</span>, queremos estudiar cómo se distribuye la probabilidad de sucesos cualesquiera de la forma <span class="math inline">\(\{(X,Y)\in B\}\)</span>, donde <span class="math inline">\(B\)</span> es una región del plano.</p>
<p>Para ello, definimos la <strong>función de distribución conjunta</strong>:</p>
<p><l class="definition">Definición de función de distribución conjunta:</l>
Dada una variable bidimensional <span class="math inline">\((X,Y)\)</span>, definimos su <strong>función de distribución conjunta</strong> <span class="math inline">\(F_{XY}\)</span> a la función definida sobre <span class="math inline">\(\mathbb{R}^2\)</span> de la manera siguiente:
<span class="math display">\[
\begin{array}{rl}
F_{XY}: \mathbb{R}^2 &amp; \longrightarrow \mathbb{R}\\
(x,y) &amp; \longrightarrow F_{XY}(x,y)=P(X\leq x,\ Y\leq y).
\end{array}
\]</span></p>
<h3 id="función-de-distribución-conjunta.-introducción-1"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Introducción</h3>
O sea, dado un valor <span class="math inline">\((x,y)\in \mathbb{R}^2\)</span>, consideramos la región del plano <span class="math inline">\((-\infty,x]\times (-\infty,y]\)</span>:
<div class="center">
<p><img src="Images/Fxy.png" width="450px" /></p>
</div>
<h3 id="función-de-distribución-conjunta.-introducción-2"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Introducción</h3>
<p>Entonces la <strong>función de distribución conjunta</strong> en el valor <span class="math inline">\((x,y)\)</span> es la probabilidad del suceso formado por aquellos elementos tal que la imagen por la <strong>variable aleatoria bidimensional</strong> <span class="math inline">\((X,Y)\)</span> caen dentro de la región sombreada en el gráfico anterior:</p>
<p><span class="math display">\[
\begin{array}{rl}
F_{XY}(x,y) &amp; =P\{w\in\Omega,\ |\ (X(w),Y(w))\in (-\infty,x]\times (-\infty,y]\} \\ &amp; = P\{w\in\Omega,\ |\ X(w)\leq x,\ Y(w)\leq y\}.
\end{array}
\]</span></p>
<h3 id="función-de-distribución-conjunta.-propiedades"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<p>Sea <span class="math inline">\((X,Y)\)</span> una variable bidimensional. Sean <span class="math inline">\(F_{XY}\)</span> su <strong>función de distribución conjunta</strong>. Dicha función satisface las propiedades siguientes:</p>
<ul>
<li><p>La función de distribución conjunta es no decreciente en cada una de las variables:
<span class="math display">\[
\mbox{Si }x_1\leq x_2, \mbox{ y }y_1\leq y_2,\mbox{ entonces, }F_{XY}(x_1,y_1)\leq F_{XY}(x_2,y_2).
\]</span></p></li>
<li><p><span class="math inline">\(F_{XY}(x,-\infty)=F_{XY}(-\infty,y)=0,\)</span> <span class="math inline">\(F_{XY}(\infty,\infty)=1\)</span>, para todo <span class="math inline">\(x,y\in\mathbb{R}\)</span>.</p></li>
</ul>
<h3 id="función-de-distribución-conjunta.-propiedades-1"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<ul>
<li><p>Las variables aleatorias <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> se llaman <strong>variables aleatorias marginales</strong> y sus funciones de distribución <span class="math inline">\(F_X\)</span> y <span class="math inline">\(F_Y\)</span> pueden hallarse de la forma siguiente como función de la <strong>función de distribución conjunta</strong> <span class="math inline">\(F_{XY}\)</span>:
<span class="math display">\[
F_X(x)=F_{XY}(x,\infty),\ F_Y(y)=F_{XY}(\infty,y),
\]</span>
para todo <span class="math inline">\(x,y\in\mathbb{R}\)</span>.</p></li>
<li><p>La función de distribución conjunta es continua por el “norte” y por el “este”:
<span class="math display">\[
\begin{array}{rl}
\lim_{x\to a^+}F_{XY}(x,y) &amp; =\lim_{x\to a, x&gt; a}F_{XY}(x,y)=F_{XY}(a,y), \\
\lim_{y\to b^+}F_{XY}(x,y) &amp; =\lim_{y\to b, y&gt; b}F_{XY}(x,y)=F_{XY}(x,b),
\end{array}
\]</span>
para todo <span class="math inline">\(a,b\in\mathbb{R}\)</span>. Ver figura siguiente.</p></li>
</ul>
<h3 id="función-de-distribución-conjunta.-propiedades-2"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<div class="center">
<p><img src="Images/Fxy2.png" width="450px" /></p>
</div>
<h3 id="función-de-distribución-conjunta.-propiedades-3"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<ul>
<li>Dados <span class="math inline">\(x_1&lt;x_2\)</span> y <span class="math inline">\(y_1&lt;y_2\)</span>, consideramos <span class="math inline">\(B\)</span> el rectángulo de vértices <span class="math inline">\((x_1,y_1)\)</span>, <span class="math inline">\((x_1,y_2)\)</span>, <span class="math inline">\((x_2,y_1)\)</span> y <span class="math inline">\((x_2,y_2)\)</span>: <span class="math inline">\((x_1,x_2]\times (y_1,y_2]\)</span>. Entonces,
<span class="math display">\[
\begin{array}{rl}
P((X,Y)\in B)  = &amp; F_{XY}(x_2,y_2)-F_{XY}(x_2,y_1)-F_{XY}(x_1,y_2)\\ &amp; +F_{XY}(x_1,y_1).
\end{array}
\]</span></li>
</ul>
<h3 id="función-de-distribución-conjunta.-propiedades-4"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<div class="center">
<p><img src="Images/Fxy3.png" width="450px" /></p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos una variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span> con <strong>función de distribución conjunta</strong>:
<span class="math display">\[
F_{XY}(x,y)=\begin{cases}
0, &amp; \mbox{si }x&lt;0,\mbox{ o }y&lt;0,\\
xy, &amp; \mbox{si }0\leq x\leq 1,\ 0\leq y\leq 1, \\
x, &amp; \mbox{si }0\leq x\leq 1,\ y&gt; 1, \\
y, &amp; \mbox{si }0\leq y\leq 1,\ x&gt; 1, \\
1, &amp; x\geq 1,\ y\geq 1.
\end{cases}
\]</span>
En la figura siguiente, hemos representado por zonas cómo está definida <span class="math inline">\(F_{XY}\)</span>.</p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-1"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<div class="center">
<p><img src="Images/FxyEx.png" width="450px" /></p>
</div>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-2"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p>Comprobemos algunas de las propiedades que hemos enunciado anteriormente:</p>
<ul>
<li><p>Claramente <span class="math inline">\(F_{XY}(x,-\infty)=F_{XY}(-\infty,y)=0\)</span> ya que <span class="math inline">\(F_{XY}(x,y)=0\)</span> si <span class="math inline">\(x&lt;0\)</span> o <span class="math inline">\(y&lt;0\)</span>. Por tanto, si hacemos tender <span class="math inline">\(x\)</span> o <span class="math inline">\(y\)</span> hacia <span class="math inline">\(-\infty\)</span>, obtendremos que <span class="math inline">\(F_{XY}(x,-\infty)=F_{XY}(-\infty,y)=0\)</span>.</p></li>
<li><p>De la misma manera <span class="math inline">\(F_{XY}(\infty,\infty)=1\)</span> ya que <span class="math inline">\(F_{XY}(x,y)=1\)</span> para <span class="math inline">\(x&gt;1\)</span> e <span class="math inline">\(y&gt;1\)</span>. Por tanto, si hacemos tender <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> hacia <span class="math inline">\(\infty\)</span>, obtendremos <span class="math inline">\(F_{XY}(\infty,\infty)=1\)</span>.</p></li>
<li><p>Hallemos las marginales:
<span class="math display">\[
F_X(x)=F_{XY}(x,\infty)=\begin{cases}
0, &amp; \mbox{ si }x&lt;0,\\
x, &amp; \mbox{ si } 0\leq x\leq 1,\\
1, &amp; \mbox{ si } x&gt;1.
\end{cases}
\]</span>
Para ver la expresión anterior basta trazar la recta vertical <span class="math inline">\(X=x\)</span> en el gráfico anterior y ver hacia dónde tiende a medida que la <span class="math inline">\(y\)</span> se va hacia <span class="math inline">\(\infty\)</span>.</p></li>
</ul>
<p>¿Habéis averiguado cuál es la distribución de <span class="math inline">\(X\)</span>?</p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-3"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p>¡Efectivamente!, <span class="math inline">\(X\)</span> es la uniforme en el intervalo <span class="math inline">\((0,1)\)</span>.</p>
<p>Dejamos como ejercicio hallar la distribución marginal para la variable <span class="math inline">\(Y\)</span>.</p>
<ul>
<li>Comprobemos que <span class="math inline">\(F_{XY}\)</span> es continua por el “norte” y el “este” en el punto <span class="math inline">\((1,1)\)</span> que sería un punto problemático:
<span class="math display">\[
\lim_{x\to 1,x&gt; 1} F_{XY}(x,1)=\lim_{x\to 1,x&gt; 1} 1  = F_{XY}(1,1),\ \lim_{y\to 1,y&gt; 1} F_{XY}(1,y)=\lim_{y\to 1,y&gt; 1} 1  = F_{XY}(1,1).
\]</span></li>
</ul>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-con-r"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<p>Hagamos un gráfico 3D de la <strong>función de distribución conjunta</strong> usando la función <code>persp</code> de <code>R</code> para <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> entre -2 y 2.</p>
<p>Primero definimos la <strong>función</strong> y luego la dibujamos:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1">f.dist.con =<span class="st"> </span><span class="cf">function</span>(x,y){<span class="kw">ifelse</span>(x<span class="op">&lt;</span><span class="dv">0</span> <span class="op">|</span><span class="st"> </span>y<span class="op">&lt;</span><span class="dv">0</span>,<span class="dv">0</span>,</a>
<a class="sourceLine" id="cb58-2" data-line-number="2">                           <span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>y<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>y<span class="op">&lt;=</span><span class="dv">1</span>,x<span class="op">*</span>y,</a>
<a class="sourceLine" id="cb58-3" data-line-number="3">                           <span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>y <span class="op">&gt;</span><span class="dv">1</span>,x,<span class="kw">ifelse</span>(y<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>y<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&gt;</span><span class="dv">1</span>,y,<span class="dv">1</span>))))}</a>
<a class="sourceLine" id="cb58-4" data-line-number="4">x=<span class="kw">seq</span>(<span class="dt">from=</span><span class="op">-</span><span class="dv">2</span>,<span class="dt">to=</span><span class="dv">2</span>,<span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb58-5" data-line-number="5">y=<span class="kw">seq</span>(<span class="dt">from=</span><span class="op">-</span><span class="dv">2</span>,<span class="dt">to=</span><span class="dv">2</span>,<span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb58-6" data-line-number="6">z=<span class="kw">outer</span>(x,y,f.dist.con)</a>
<a class="sourceLine" id="cb58-7" data-line-number="7"><span class="kw">persp</span>(x,y,z,<span class="dt">theta=</span><span class="dv">50</span>,<span class="dt">phi=</span><span class="dv">40</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">shade=</span><span class="fl">0.25</span>,<span class="dt">ticktype=</span><span class="st">&quot;detailed&quot;</span>)</a></code></pre></div>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-con-r-1"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo con <code>R</code></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-30-1.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="función-de-distribución-conjunta.-ejemplo-4"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: dos lanzamientos de un dado no trucado</strong></p>
<p>Consideremos el experimento aleatorio de lanzar dos veces un dado no trucado.</p>
<p>Sea <span class="math inline">\((S,P)\)</span> la <strong>variable aleatoria bidimensional</strong> que nos da la suma y el producto de los resultados obtenidos, respectivamente.</p>
<p>La <strong>función de distribución conjunta</strong> en el valor <span class="math inline">\((3,4)\)</span> será:
<span class="math display">\[
F_{XY}(3,4) = P(S\leq 3,\ P\leq 4)=P\{(1,1), (1,2), (2,1) \}=\frac{3}{36}=\frac{1}{12}\approx 0.083, 
\]</span>
ya que <span class="math inline">\(\Omega\)</span> tiene en total <span class="math inline">\(36\)</span> resultados:
<span class="math display">\[
\Omega =\{(1,1),(1,2).\ldots, (6,6)\}.
\]</span>
y los únicos resultados en los que la suma es menor o igual que 3 y el producto menor o igual que 4 son <span class="math inline">\((1,1)\)</span> (suma 2 producto 1), <span class="math inline">\((1,2)\)</span> (suma 3 y producto 2) y <span class="math inline">\((2,1)\)</span> (suma 3 y producto 2).</p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-5"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Hallar el valor de la <strong>función de distribución conjunta</strong> para la <strong>variable aleatoria bidimensional</strong> anterior <span class="math inline">\((S,P)\)</span> en los valores <span class="math inline">\((i,j)\)</span> siguientes: <span class="math inline">\((4,5),\ (4,9),\ (5,9),\ (6,10)\)</span>.</p>
</div>
<h2 id="variables-aleatorias-bidimensionales-discretas"><span class="header-section-number">2.2.6</span> Variables aleatorias bidimensionales discretas</h2>
<h3 id="variables-aleatorias-bidimensionales-discretas.-introducción"><span class="header-section-number">2.2.6</span> Variables aleatorias bidimensionales discretas. Introducción</h3>
<p><l class="definition">Definición de variable aleatoria bidimensional discreta:</l>
Sea <span class="math inline">\((X,Y)\)</span> una <strong>variable aleatoria bidimensional</strong>. Diremos que es discreta cuando su conjunto de valores en <span class="math inline">\(\mathbb{R}^2\)</span>, <span class="math inline">\((X,Y)(\Omega)\)</span> es un conjunto finito o numerable.</p>
<p>En la mayoría de los casos, dicho conjunto será un subconjunto de los enteros naturales.</p>
<h3 id="variables-aleatorias-bidimensionales-discretas.-ejemplo"><span class="header-section-number">2.2.6</span> Variables aleatorias bidimensionales discretas. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>La variable aleatoria bidimensional anterior que nos daba la suma y el producto de los resultados obtenidos por los dos lanzamientos, respectivamente es discreta ya que:
<span class="math display">\[
\begin{array}{rl}
(S,P)(\Omega) &amp; =\{(2,1),(3,2),(4,3),(4,4),(5,4),(5,6),(6,5),(6,8),(6,9),(7,6),(7,10),(7,12),(8,12),\\ &amp; (8,15),(8,16),(9,18),(9,20),(10,24),(10,25),(11,30),(12,36)\}.
\end{array}
\]</span></p>
</div>
<h3 id="variables-aleatorias-bidimensionales-discretas.-ejemplo-1"><span class="header-section-number">2.2.6</span> Variables aleatorias bidimensionales discretas. Ejemplo</h3>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Comprobar que el conjunto <span class="math inline">\((S,P)(\Omega)\)</span> dado por el ejemplo coincide con la expresión dada.
O sea, hallar el conjunto <span class="math inline">\((S,P)(\Omega)\)</span>:
<span class="math display">\[
\begin{array}{rl}
(S,P): \Omega &amp; \longrightarrow \mathbb{R}^2\\
(1,1) &amp; \longrightarrow (S(1,1),P(1,1))=(2,1),\\
(1,2) &amp; \longrightarrow (S(1,2),P(1,2))=(3,2),\\
\vdots &amp; \vdots \\
(6,6) &amp; \longrightarrow (S(6,6),P(6,6))=(12,36).
\end{array}
\]</span></p>
</div>
<h3 id="función-de-probabilidad-conjunta"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta</h3>
<p><l class="definition">Definición de función de probabilidad conjunta:</l>
Dada una <strong>variable aleatoria bidimensional discreta</strong> <span class="math inline">\((X,Y)\)</span> con <span class="math inline">\((X,Y)(\Omega)=\{(x_i,y_j),\ i=1,2,\ldots,\ j=1,2,\ldots,\}\)</span>, definimos la función de probabilidad discreta <span class="math inline">\(P_{XY}\)</span> para un valor <span class="math inline">\((x,y)\in\mathbb{R}^2\)</span> de la siguiente forma:
<span class="math display">\[
\begin{array}{rl}
P_{XY}: \mathbb{R}^2 &amp; \longrightarrow \mathbb{R}\\
(x,y) &amp; \longrightarrow P_{XY}(x,y)=P(X= x,\ Y= y).
\end{array}
\]</span></p>
<p><l class="observ">Observación:</l>
Si <span class="math inline">\((x,y)\not\in (X,Y)(\Omega)\)</span>, el valor de la <strong>función de probabilidad conjunta</strong> en <span class="math inline">\((x,y)\)</span> en nulo: <span class="math inline">\(P_{XY}(x,y)=0\)</span>, ya que, en este caso, el conjunto <span class="math inline">\(\{w\in\Omega,\ | (X(w),Y(w))=(x,y)\}=\emptyset\)</span> ya que recordemos <span class="math inline">\((x,y)\not\in (X,Y)(\Omega)\)</span>.</p>
<h3 id="función-de-probabilidad-conjunta-1"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta</h3>
<p>Por tanto, de cara a calcular <span class="math inline">\(P_{XY}\)</span> basta calcular <span class="math inline">\(P_{XY}(x_i,y_j)\)</span> para <span class="math inline">\((x_i,y_j)\in (X,Y)(\Omega)\)</span>:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X/Y\)</span></th>
<th><span class="math inline">\(y_1\)</span></th>
<th><span class="math inline">\(y_2\)</span></th>
<th><span class="math inline">\(\ldots\)</span></th>
<th><span class="math inline">\(y_N\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x_1\)</span></td>
<td><span class="math inline">\(P_{XY}(x_1,y_1)\)</span></td>
<td><span class="math inline">\(P_{XY}(x_1,y_2)\)</span></td>
<td><span class="math inline">\(\ldots\)</span></td>
<td><span class="math inline">\(P_{XY}(x_1,y_N)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_2\)</span></td>
<td><span class="math inline">\(P_{XY}(x_2,y_1)\)</span></td>
<td><span class="math inline">\(P_{XY}(x_2,y_2)\)</span></td>
<td><span class="math inline">\(\ldots\)</span></td>
<td><span class="math inline">\(P_{XY}(x_2,y_N)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_M\)</span></td>
<td><span class="math inline">\(P_{XY}(x_M,y_1)\)</span></td>
<td><span class="math inline">\(P_{XY}(x_M,y_2)\)</span></td>
<td><span class="math inline">\(\ldots\)</span></td>
<td><span class="math inline">\(P_{XY}(x_M,y_N)\)</span></td>
</tr>
</tbody>
</table>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado</strong></p>
La <strong>función de probabilidad conjunta</strong> será:
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(S/P\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>12</th>
<th>15</th>
<th>16</th>
<th>18</th>
<th>20</th>
<th>24</th>
<th>25</th>
<th>30</th>
<th>36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>3</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>5</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-1"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo</h3>
<div class="example">
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(S/P\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>12</th>
<th>15</th>
<th>16</th>
<th>18</th>
<th>20</th>
<th>24</th>
<th>25</th>
<th>30</th>
<th>36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>7</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>8</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>10</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>11</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
</tr>
<tr class="even">
<td>12</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<p>Vamos a definir unas funciones en <code>R</code> para calcular la <strong>función de probabilidad conjunta</strong>.</p>
<p>La función <code>pdado</code> devuelve la probabilidad de que salga la cara <code>x</code> en un dado de <code>n</code> caras donde por defecto <span class="math inline">\(n=6\)</span>:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1">pdado =<span class="cf">function</span>(x,<span class="dt">n=</span><span class="dv">6</span>)  <span class="kw">sapply</span>(x,<span class="dt">FUN=</span><span class="cf">function</span>(x) </a>
<a class="sourceLine" id="cb59-2" data-line-number="2">  <span class="cf">if</span>( x <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>n))  {<span class="kw">return</span>(<span class="dv">1</span><span class="op">/</span>n)} <span class="cf">else</span> {<span class="kw">return</span>(<span class="dv">0</span>)})</a></code></pre></div>
<p>Vamos a probarla. La probabilidad de que salga la cara 4 en un dado de 6 caras vale:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1"><span class="kw">pdado</span>(<span class="dv">4</span>,<span class="dv">6</span>)</a></code></pre></div>
<pre><code>## [1] 0.1666667</code></pre>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-1"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<p>La función <code>pdado2</code> devuelve la probabilidad de que salgan las caras <code>x</code> e <code>y</code> cuando lanzamos un dado de <code>n</code> caras dos veces:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1">pdado2 =<span class="cf">function</span>(x,y,<span class="dt">n=</span><span class="dv">6</span>) {<span class="kw">pdado</span>(x,n)<span class="op">*</span><span class="kw">pdado</span>(y,n)}</a></code></pre></div>
<p>Por ejemplo la probabilidad de que salgan las caras 3 y 4 en un dado de 6 caras será:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1"><span class="kw">pdado2</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">6</span>)</a></code></pre></div>
<pre><code>## [1] 0.02777778</code></pre>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-2"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<p>La función <code>psum_prod</code> nos da la <strong>función de probabilidad conjunta</strong> de la suma y el producto cuando lanzamos dos dados de <code>n</code> caras:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1">psum_prod=<span class="cf">function</span>(x,y,<span class="dt">n=</span><span class="dv">6</span>){</a>
<a class="sourceLine" id="cb65-2" data-line-number="2">  Dxy=<span class="kw">data.frame</span>(<span class="dt">d1=</span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dt">each=</span>n),<span class="dt">d2=</span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dt">times=</span>n))</a>
<a class="sourceLine" id="cb65-3" data-line-number="3">  Dxy<span class="op">$</span>suma=Dxy<span class="op">$</span>d1<span class="op">+</span>Dxy<span class="op">$</span>d2</a>
<a class="sourceLine" id="cb65-4" data-line-number="4">  Dxy<span class="op">$</span>producto=Dxy<span class="op">$</span>d1<span class="op">*</span>Dxy<span class="op">$</span>d2</a>
<a class="sourceLine" id="cb65-5" data-line-number="5">  aux=Dxy[Dxy<span class="op">$</span>suma<span class="op">==</span>x<span class="op">&amp;</span><span class="st"> </span>Dxy<span class="op">$</span>producto<span class="op">==</span>y,]</a>
<a class="sourceLine" id="cb65-6" data-line-number="6">  <span class="kw">sum</span>(<span class="kw">apply</span>(aux[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],<span class="dt">FUN=</span><span class="cf">function</span>(x) {<span class="kw">pdado2</span>(x[<span class="dv">1</span>],x[<span class="dv">2</span>],<span class="dt">n=</span>n)},<span class="dv">1</span> ))</a>
<a class="sourceLine" id="cb65-7" data-line-number="7">}</a></code></pre></div>
<p>Por ejemplo, sabemos que <span class="math inline">\(P_{SP}(6,8)=\frac{2}{36}=0.0556\)</span>:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="kw">psum_prod</span>(<span class="dv">6</span>,<span class="dv">8</span>)</a></code></pre></div>
<pre><code>## [1] 0.05555556</code></pre>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-3"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<p>Para construir la tabla de la <strong>función de probabilidad conjunta</strong> para la variable <span class="math inline">\((S,P)\)</span> hacemos lo siguiente:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1">n=<span class="dv">6</span></a>
<a class="sourceLine" id="cb68-2" data-line-number="2">Dxy=<span class="kw">data.frame</span>(<span class="dt">d1=</span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dt">each=</span>n),<span class="dt">d2=</span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dt">times=</span>n))</a>
<a class="sourceLine" id="cb68-3" data-line-number="3">Dxy<span class="op">$</span>suma=Dxy<span class="op">$</span>d1<span class="op">+</span>Dxy<span class="op">$</span>d2</a>
<a class="sourceLine" id="cb68-4" data-line-number="4">Dxy<span class="op">$</span>producto=Dxy<span class="op">$</span>d1<span class="op">*</span>Dxy<span class="op">$</span>d2</a>
<a class="sourceLine" id="cb68-5" data-line-number="5">tabla.func.prob.conjunta=<span class="kw">prop.table</span>(<span class="kw">table</span>(Dxy<span class="op">$</span>suma,Dxy<span class="op">$</span>producto))</a>
<a class="sourceLine" id="cb68-6" data-line-number="6">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">round</span>(tabla.func.prob.conjunta[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>],<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb68-7" data-line-number="7">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">round</span>(tabla.func.prob.conjunta[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,<span class="dv">10</span><span class="op">:</span><span class="dv">18</span>],<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb68-8" data-line-number="8">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">round</span>(tabla.func.prob.conjunta[<span class="dv">7</span><span class="op">:</span><span class="dv">11</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>],<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb68-9" data-line-number="9">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">round</span>(tabla.func.prob.conjunta[<span class="dv">7</span><span class="op">:</span><span class="dv">11</span>,<span class="dv">10</span><span class="op">:</span><span class="dv">18</span>],<span class="dv">4</span>))</a></code></pre></div>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-4"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td align="right">0.0278</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td>4</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0278</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td>5</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td>6</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0278</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td>7</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
</tr>
</tbody>
</table>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-5"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">12</th>
<th align="right">15</th>
<th align="right">16</th>
<th align="right">18</th>
<th align="right">20</th>
<th align="right">24</th>
<th align="right">25</th>
<th align="right">30</th>
<th align="right">36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td align="right">0.0000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>4</td>
<td align="right">0.0000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>5</td>
<td align="right">0.0000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>6</td>
<td align="right">0.0000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>7</td>
<td align="right">0.0556</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-6"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>9</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>10</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>11</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>12</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-7"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">12</th>
<th align="right">15</th>
<th align="right">16</th>
<th align="right">18</th>
<th align="right">20</th>
<th align="right">24</th>
<th align="right">25</th>
<th align="right">30</th>
<th align="right">36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8</td>
<td align="right">0.0556</td>
<td align="right">0.0556</td>
<td align="right">0.0278</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td>9</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0556</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td>10</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0278</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td>11</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0556</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td>12</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0278</td>
</tr>
</tbody>
</table>
</div>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta</h3>
<p>Sea <span class="math inline">\((X,Y)\)</span> una <strong>variable aleatoria bidimensional discreta</strong> con conjunto de valores <span class="math inline">\((X,Y)(\Omega)=\{(x_i,y_j)\, i=1,2,\ldots,\ j=1,2,\ldots\}\)</span>. Entonces su <strong>función de probabilidad conjunta</strong> verifica las propiedades siguientes:</p>
<p>La suma de todos los valores de la <strong>función de probabilidad conjunta</strong> sobre el conjunto de valores siempre vale 1: <span class="math display">\[\sum_{i}\sum_j P_{XY}(x_i,y_j)=1.\]</span></p>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta-1"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta</h3>
<p>Sea <span class="math inline">\(B\)</span> una región del plano. El valor de la probabilidad <span class="math inline">\(P((X,Y)\in B)\)</span> se puede calcular de la forma siguiente:
<span class="math display">\[
P((X,Y)\in B) =\sum_{(x_i,y_j)\in B} P_{XY}(x_i,y_j).
\]</span>
O sea, la probabilidad de que la variable bidimensional coja valores en <span class="math inline">\(B\)</span> es igual a la suma de todos aquellos valores de la función de probabilidad conjunta que están en <span class="math inline">\(B\)</span>.</p>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta-2"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta</h3>
<p>En particular, tenemos la relación siguiente que relaciona la <strong>función de distribución conjunta</strong> con la <strong>función de probabilidad conjunta</strong>:
<span class="math display">\[
F_{XY}(x,y)=\sum_{x_i\leq x, y_j\leq y} P_{XY}(x_i,y_j).
\]</span>
Dicha expresión se deduce de la expresión anterior considerando <span class="math inline">\(B=(-\infty,x]\times (-\infty,y]\)</span>.</p>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta.-ejemplo"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado</strong></p>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Comprobad usando la tabla de la función de probabilidad conjunta que la suma de todos sus valores suma 1.</p>
</div>
<p>Apliquemos la fórmula que relaciona la función de distribución conjunta con la función de probabilidad conjunta para <span class="math inline">\((x,y)=(5,4)\)</span>.</p>
<p>Recordemos la tabla de la función de probabilidad conjunta hasta <span class="math inline">\(S=5\)</span> y <span class="math inline">\(P=4\)</span>:</p>
</div>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta.-ejemplo-1"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta. Ejemplo</h3>
<div class="example">
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(S/P\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\ldots\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\ldots\)</span></td>
</tr>
<tr class="odd">
<td>4</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td><span class="math inline">\(\ldots\)</span></td>
</tr>
<tr class="even">
<td>5</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\ldots\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta.-ejemplo-2"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta. Ejemplo</h3>
<div class="example">
<p>Observamos que los únicos valores <span class="math inline">\((x_i,y_j)\in (X,Y)(\Omega)\)</span> que verifican <span class="math inline">\(x_i\leq 5\)</span> y <span class="math inline">\(y_j\leq 4\)</span> son <span class="math inline">\((2,1)\)</span>, <span class="math inline">\((3,2)\)</span>, <span class="math inline">\((4,3)\)</span>, <span class="math inline">\((4,4)\)</span> y <span class="math inline">\((5,4)\)</span>. Por tanto,
<span class="math display">\[
\begin{array}{rl}
F_{SP}(5,4) &amp; = P_{SP}(2,1)+P_{SP}(3,2)+P_{SP}(4,3)+P_{SP}(4,4)+P_{SP}(5,4) \\ &amp; = \frac{1}{36}+\frac{2}{36}+\frac{2}{36}+\frac{1}{36}+\frac{2}{36} = \frac{8}{36}=\frac{2}{9}.
\end{array}
\]</span>
O sea, “a la larga”, de cada 9 ocasiones que lanzamos un dado dos veces, en 2 ocasiones obtenemos un resultado cuya suma es menor o igual que 5 y cuyo producto es menor o igual que 4.</p>
</div>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta.-ejemplo-con-r"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<p>Para definir la <strong>función de distribución conjunta</strong> definimos la función siguiente en <code>R</code>:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">func.dist.conj =<span class="st"> </span><span class="cf">function</span>(x,y,<span class="dt">n=</span><span class="dv">6</span>){</a>
<a class="sourceLine" id="cb69-2" data-line-number="2">  <span class="kw">sum</span>(tabla.func.prob.conjunta[<span class="kw">as.integer</span>(<span class="kw">rownames</span>(tabla.func.prob.conjunta))<span class="op">&lt;=</span>x,</a>
<a class="sourceLine" id="cb69-3" data-line-number="3">                            <span class="kw">as.integer</span>(<span class="kw">colnames</span>(tabla.func.prob.conjunta)) <span class="op">&lt;=</span>y])</a>
<a class="sourceLine" id="cb69-4" data-line-number="4">}</a></code></pre></div>
<p>Comprobemos que <span class="math inline">\(F_{SP}(5,4)=\frac{2}{9}=0.2222\)</span>:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1"><span class="kw">func.dist.conj</span>(<span class="dv">5</span>,<span class="dv">4</span>)</a></code></pre></div>
<pre><code>## [1] 0.2222222</code></pre>
</div>
<h3 id="variables-aleatorias-marginales"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales</h3>
<p>Consideremos una variable aleatoria <strong>bidimensional discreta <span class="math inline">\((X,Y)\)</span></strong> con <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}(x_i,y_j)\)</span>, con <span class="math inline">\((x_i,y_j)\in (X,Y)(\Omega)\)</span>, <span class="math inline">\(i=1,2,\ldots\)</span>, <span class="math inline">\(j=1,2,\ldots\)</span>.</p>
<p>La tabla de la <strong>función de probabilidad conjunta</strong> contiene suficiente información para obtener las <strong>funciones de probabilidad</strong> de las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<p>Dichas variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> se denominan <strong>distribuciones marginales</strong> y sus correspondientes <strong>funciones de probabilidad</strong>, <strong>funciones de probabilidad marginales</strong> <span class="math inline">\(P_X\)</span> de la variable <span class="math inline">\(X\)</span> y <span class="math inline">\(P_Y\)</span> de la variable <span class="math inline">\(Y\)</span>.</p>
<p>Veamos cómo obtener <span class="math inline">\(P_X\)</span> y <span class="math inline">\(P_Y\)</span> a partir de la tabla <span class="math inline">\(P_{XY}\)</span>.</p>
<h3 id="variables-aleatorias-marginales-1"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales</h3>
<p><l class="prop">Proposición. Expresión de las funciones de probabilidad marginales. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria <strong>bidimensional discreta</strong> con <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}(x_i,y_j)\)</span>, con <span class="math inline">\((x_i,y_j)\in (X,Y)(\Omega)\)</span>, <span class="math inline">\(i=1,2,\ldots\)</span>, <span class="math inline">\(j=1,2,\ldots\)</span>.</p>
<p>Las <strong>funciones de probabilidad marginales</strong> <span class="math inline">\(P_X(x_i)\)</span> y <span class="math inline">\(P_Y(y_j)\)</span> se calculan usando las expresiones siguientes:
<span class="math display">\[
\begin{array}{rl}
P_X(x_i)  &amp; = \sum_{j=1} P_{XY}(x_i,y_j),\  i=1,2,\ldots,\\ P_Y(y_j) &amp;  = \sum_{i=1} P_{XY}(x_i,y_j),\ \ j=1,2,\ldots
\end{array}
\]</span></p>
<h3 id="variables-aleatorias-marginales-2"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales</h3>
<p>O sea, si pensamos <span class="math inline">\(P_{XY}\)</span> como una tabla bidimensional donde en la primera fila están los valores de la variable <span class="math inline">\(Y\)</span> (<span class="math inline">\(y_1,y_2,\ldots\)</span>) y en la primera columna están los valores de la variable <span class="math inline">\(X\)</span> (<span class="math inline">\(x_1,x_2,\ldots\)</span>), para obtener la <strong>función de probabilidad marginal</strong> de la variable <span class="math inline">\(X\)</span> en el valor <span class="math inline">\(x_i\)</span>, <span class="math inline">\(P_X(x_i)\)</span>, hay que sumar todos los valores de <span class="math inline">\(P_{XY}(x_i,y_j)\)</span> correspondientes a la fila <span class="math inline">\(i\)</span>-ésima y para obtener la <strong>función de probabilidad marginal</strong> de la variable <span class="math inline">\(Y\)</span> en el valor <span class="math inline">\(y_j\)</span>, <span class="math inline">\(P_Y(y_j)\)</span>, hay que sumar todos los valores de <span class="math inline">\(P_{XY}(x_i,y_j)\)</span> correspondientes a la columna <span class="math inline">\(j\)</span>-ésima.</p>
<h3 id="variables-aleatorias-marginales.-ejemplo"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado</strong></p>
<p>Hallemos la función de probabilidad marginal para la suma de los resultados <span class="math inline">\(S\)</span> usando la expresión vista:
<span class="math display">\[
\begin{array}{rl}
P_S(2) &amp; = P_{SP}(2,1)=\frac{1}{36},\\
P_S(3) &amp; = P_{SP}(3,2)=\frac{2}{36},\\
P_S(4) &amp; = P_{SP}(4,3)+P_{SP}(4,4)=\frac{2}{36}+\frac{1}{36}=\frac{3}{36}=\frac{1}{12},\\
P_S(5) &amp; = P_{SP}(5,4)+P_{SP}(5,6)=\frac{2}{36}+\frac{2}{36}=\frac{4}{36}=\frac{1}{9},\\
P_S(6) &amp; = P_{SP}(6,5)+P_{SP}(6,8)+P_{SP}(6,9)=\frac{2}{36}+\frac{2}{36}+\frac{1}{36}=\frac{5}{36},\\
P_S(7) &amp; = P_{SP}(7,6)+P_{SP}(7,10)+P_{SP}(7,12)=\frac{2}{36}+\frac{2}{36}+\frac{2}{36}=\frac{6}{36}=\frac{1}{6},\\
P_S(8) &amp; = P_{SP}(8,12)+P_{SP}(8,15)+P_{SP}(8,16)=\frac{2}{36}+\frac{2}{36}+\frac{1}{36}=\frac{5}{36},\\
P_S(9) &amp; = P_{SP}(9,18)+P_{SP}(9,20)=\frac{2}{36}+\frac{2}{36}=\frac{4}{36}=\frac{1}{9},\\
P_S(10) &amp; = P_{SP}(10,24)+P_{SP}(10,25)=\frac{2}{36}+\frac{1}{36}=\frac{3}{36}=\frac{1}{12},\\
P_S(11) &amp; = P_{SP}(11,30)=\frac{2}{36},\\
P_S(12) &amp; = P_{SP}(12,36)=\frac{1}{36}.
\end{array}
\]</span></p>
</div>
<h3 id="variables-aleatorias-marginales.-ejemplo-1"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales. Ejemplo</h3>
<div class="example">
<p>La <strong>función de probabilidad marginal</strong> de la suma <span class="math inline">\(S\)</span> queda resumida en la tabla siguiente:</p>
<div class="center">
<table>
<colgroup>
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(S\)</span></th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_S\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{3}{36}\)</span></td>
<td><span class="math inline">\(\frac{4}{36}\)</span></td>
<td><span class="math inline">\(\frac{5}{36}\)</span></td>
<td><span class="math inline">\(\frac{6}{36}\)</span></td>
<td><span class="math inline">\(\frac{5}{36}\)</span></td>
<td><span class="math inline">\(\frac{4}{36}\)</span></td>
<td><span class="math inline">\(\frac{3}{36}\)</span></td>
<td><span class="math inline">\(\frac{2}{36}\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="variables-aleatorias-marginales.-ejemplo-con-r"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales. Ejemplo con <code>R</code></h3>
<div class="example">
<p>Para hallar la <strong>función de probabilidad marginal</strong> de la suma basta sumar las filas de la tabla que nos daba la <strong>función de probabilidad conjunta</strong>:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1">marginal.suma =<span class="st"> </span><span class="kw">apply</span>(tabla.func.prob.conjunta,<span class="dv">1</span>,sum)</a>
<a class="sourceLine" id="cb72-2" data-line-number="2">marginal.suma</a></code></pre></div>
<pre><code>##          2          3          4          5          6          7          8 
## 0.02777778 0.05555556 0.08333333 0.11111111 0.13888889 0.16666667 0.13888889 
##          9         10         11         12 
## 0.11111111 0.08333333 0.05555556 0.02777778</code></pre>
</div>
<h3 id="variables-aleatorias-marginales.-ejemplo-con-r-1"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales. Ejemplo con <code>R</code></h3>
<div class="example">
<p>De la misma manera, para hallar la <strong>función de probabilidad marginal</strong> del producto basta sumar las columnas de la tabla anterior:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1">marginal.producto =<span class="st"> </span><span class="kw">apply</span>(tabla.func.prob.conjunta,<span class="dv">2</span>,sum)</a>
<a class="sourceLine" id="cb74-2" data-line-number="2">marginal.producto</a></code></pre></div>
<pre><code>##          1          2          3          4          5          6          8 
## 0.02777778 0.05555556 0.05555556 0.08333333 0.05555556 0.11111111 0.05555556 
##          9         10         12         15         16         18         20 
## 0.02777778 0.05555556 0.11111111 0.05555556 0.02777778 0.05555556 0.05555556 
##         24         25         30         36 
## 0.05555556 0.02777778 0.05555556 0.02777778</code></pre>
</div>
<h2 id="variables-aleatorias-bidimensionales-continuas"><span class="header-section-number">2.2.6</span> Variables aleatorias bidimensionales continuas</h2>
<h3 id="introducción-5"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Recordemos la definición de <strong>variable continua unidimensional</strong>: <span class="math inline">\(X\)</span> es continua si existe una función <span class="math inline">\(f_X:\mathbb{R}\longrightarrow \mathbb{R}\)</span>, llamada <strong>función de densidad</strong> no negativa <span class="math inline">\(f_X(x)\geq 0\)</span>, para todo <span class="math inline">\(x\in\mathbb{R}\)</span> tal que para cualquier intervalo <span class="math inline">\((a,b)\)</span>, la probabilidad de que <span class="math inline">\(X\)</span> esté en <span class="math inline">\((a,b)\)</span> se calcula de la forma siguiente:
<span class="math display">\[
P(X\in B)=P(a&lt; X &lt; b)=\int_B f_{X}(x)\,du=\int_a^b f_{X}(x)\,dx.
\]</span></p>
<h3 id="introducción-6"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>La generalización natural será, entonces:</p>
<p><l class="definition">Definición de variable aleatoria bidimensional continua. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional. Diremos que <span class="math inline">\((X,Y)\)</span> es continua si existe una función
<span class="math inline">\(f_{XY}:\mathbb{R}^2\longrightarrow \mathbb{R}\)</span> llamada <strong>función de densidad</strong> no negativa <span class="math inline">\(f_{XY}(x,y)\geq 0\)</span> para todo <span class="math inline">\((x,y)\in\mathbb{R}^2\)</span> tal que dado cualquier región <span class="math inline">\(B\)</span> del plano, la probabilidad de que <span class="math inline">\((X,Y)\)</span> esté en <span class="math inline">\(B\)</span> se calcula de la forma siguiente:
<span class="math display">\[
P((X,Y)\in B)=\int\int_B f_{XY}(x,y)\,dx\,dy.
\]</span></p>
<h3 id="ejemplo-40"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos la <strong>función de densidad siguiente</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
1, &amp; \mbox{ si }0\leq x\leq 1,\ 0\leq y\leq 1, \\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
En este caso, si consideramos <span class="math inline">\(B=\left[-1,\frac{1}{2}\right]\times \left[-1,\frac{1}{2}\right]\)</span>, la probabilidad de que <span class="math inline">\((X,Y)\)</span> esté en <span class="math inline">\(B\)</span> se calcularía de la forma siguiente:
<span class="math display">\[
P((X,Y)\in B)=\int_{-1}^{\frac{1}{2}}\int_{-1}^{\frac{1}{2}} f_{XY}(x,y)\, dx\, dy =\int_0^{\frac{1}{2}}\int_0^{\frac{1}{2}} 1\, dx\,dy=\int_0^{\frac{1}{2}} 1\, dx\int_0^{\frac{1}{2}} 1\, dy=\frac{1}{2}\cdot\frac{1}{2}=\frac{1}{4}.
\]</span>
En la figura siguiente hemos dibujado en morado la región donde <span class="math inline">\(f_{XY}\)</span> no es cero, o sea <span class="math inline">\([0,1]\times [0,1]\)</span>, la región <span class="math inline">\(B\)</span> en verde y la región intersección de las dos anteriores que es donde tenemos que integrar la <strong>función de densidad</strong> dada.</p>
</div>
<h3 id="ejemplo-41"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/VaUniformeBidi.png" width="700px" /></p>
</div>
<h3 id="propiedades-de-la-función-de-densidad"><span class="header-section-number">2.2.6</span> Propiedades de la función de densidad</h3>
<p>Sea <span class="math inline">\((X,Y)\)</span> una <strong>variable aleatoria bidimensional continua</strong> con <strong>función de densidad</strong> <span class="math inline">\(f_{XY}\)</span>. Entonces dicha función verifica las propiedades siguientes:</p>
<ul>
<li>La integral de dicha función sobre todo el plano vale 1:
<span class="math display">\[
\int\int_{\mathbb{R}^2} f_{XY}(x,y)\,dx\,dy =1.
\]</span>
Para ver dicha propiedad, basta considerar <span class="math inline">\(B=\mathbb{R}^2\)</span>, tener en cuenta que el suceso <span class="math inline">\((X,Y)\in \mathbb{R}^2\)</span> es el total <span class="math inline">\(\Omega\)</span> y aplicar la definición de <span class="math inline">\(f_{XY}\)</span>:
<span class="math display">\[
P((X,Y)\in \mathbb{R}^2)=1= \int\int_{\mathbb{R}^2} f_{XY}(x,y)\,dx\,dy.
\]</span></li>
</ul>
<h3 id="propiedades-de-la-función-de-densidad-1"><span class="header-section-number">2.2.6</span> Propiedades de la función de densidad</h3>
<ul>
<li>La relación que hay entre la <strong>función de distribución</strong> <span class="math inline">\(F_{XY}\)</span> y la <strong>función de densidad</strong> <span class="math inline">\(f_{XY}\)</span> es la siguiente:
<span class="math display">\[
F_{XY}(x,y)=\int_{-\infty}^x\int_{-\infty}^y f_{XY}(u,v)\,du\,dv.
\]</span>
Para ver dicha propiedad, basta considerar <span class="math inline">\(B=(-\infty,x]\times (-\infty,y]\)</span> y aplicar la definición de <strong>función de distribución</strong>:
<span class="math display">\[
F_{XY}(x,y)=P((X,Y)\in (-\infty,x]\times (-\infty,y])=\int_{-\infty}^x\int_{-\infty}^y f_{XY}(u,v)\,du\,dv.
\]</span></li>
</ul>
<h3 id="propiedades-de-la-función-de-densidad-2"><span class="header-section-number">2.2.6</span> Propiedades de la función de densidad</h3>
<ul>
<li><p>La relación que hay entre la <strong>función de densidad</strong> <span class="math inline">\(f_{XY}\)</span> y la <strong>función de distribución</strong> <span class="math inline">\(F_{XY}\)</span> es la siguiente:
<span class="math display">\[
f_{XY}(x,y)=\frac{\partial^2 F_{XY}(x,y)}{\partial x\partial y}.
\]</span>
Dicha propiedad se deduce de la anterior, derivando primero respecto a <span class="math inline">\(x\)</span> y después respecto a <span class="math inline">\(y\)</span> para eliminar las dos integrales.</p></li>
<li><p>Las <strong>funciones de densidad marginales</strong> de las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, <span class="math inline">\(f_X(x)\)</span> y <span class="math inline">\(f_Y(y)\)</span> respectivamente, se calculan de la forma siguiente:
<span class="math display">\[
f_X(x)=\int_{-\infty}^\infty f_{XY}(x,y)\, dy,\ f_Y(y)=\int_{-\infty}^\infty f_{XY}(x,y)\, dx
\]</span></p></li>
</ul>
<h3 id="ejemplo-42"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo anterior</strong></p>
<p>Comprobemos las propiedades usando la <strong>función de densidad</strong> del ejemplo anterior:
<span class="math inline">\(f_{XY}(x,y)=\begin{cases} 1, &amp; \mbox{ si }0\leq x\leq 1,\ 0\leq y\leq 1, \\ 0, &amp; \mbox{en caso contrario.} \end{cases}\)</span></p>
<ul>
<li><p>La integral de <span class="math inline">\(f_{XY}\)</span> sobre todo el plano vale 1:
<span class="math display">\[
\int\int_{\mathbb{R}^2} f_{XY}(x,y)\,dx\, dy=\int_0^1\int_0^1 1\, dx\, dv=\int_0^1 1\, dx\int_0^1 1\, dy=1\cdot 1=1.
\]</span></p></li>
<li>Vamos a calcular la función de distribución <span class="math inline">\(F_{XY}\)</span>. Para ello dividimos el plano en 5 zonas tal como muestra la figura siguiente:</li>
</ul>
</div>
<h3 id="ejemplo-43"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/VaUniformeBidi2.png" width="700px" /></p>
</div>
<h3 id="ejemplo-44"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Sea <span class="math inline">\((x,y)\)</span> un punto cualquiera de <span class="math inline">\(\mathbb{R}^2\)</span>. De cara a calcular <span class="math inline">\(F_{XY}(x,y)\)</span> tenemos que averiguar el conjunto intersección siguiente: <span class="math inline">\(([0,1]\times [0,1])\cap ((-\infty,x]\times (-\infty,y])\)</span> ya que el dominio donde <span class="math inline">\(f_{XY}\)</span> es no nula es <span class="math inline">\([0,1]\times [0,1]\)</span> y la función de distribución <span class="math inline">\(F_{XY}(x,y)\)</span> valdrá:
<span class="math display">\[
F_{XY}(x,y)=\int_{-\infty}^x\int_{-\infty}^y f_{XY}(u,v)\,du\,dv =\int\int_{([0,1]\times [0,1])\cap ((-\infty,x]\times (-\infty,y])} f_{XY}(u,v)\,du\,dv.
\]</span></p>
<ul>
<li>Caso <span class="math inline">\((x,y)\in \mbox{Zona A}\)</span> o <span class="math inline">\(x&lt;0\)</span> o <span class="math inline">\(y&lt;0\)</span> En este caso: <span class="math inline">\(([0,1]\times [0,1])\cap ((-\infty,x]\times (-\infty,y])=\emptyset.\)</span> Ver figura siguiente donde la zona morada <span class="math inline">\(([0,1]\times [0,1]\)</span>) no se interseca con la zona verde (<span class="math inline">\((-\infty,x]\times (-\infty,y]\)</span>).</li>
</ul>
<p>Por tanto en este caso, <span class="math inline">\(F_{XY}(x,y)=0\)</span>.</p>
</div>
<h3 id="ejemplo-45"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/VaUniformeBidi3.png" width="700px" /></p>
</div>
<h3 id="ejemplo-46"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<ul>
<li>Caso <span class="math inline">\((x,y)\in \mbox{Zona B}\)</span>, o <span class="math inline">\((x,y)\in [0,1]\times [0,1]\)</span>. En este caso: <span class="math inline">\(([0,1]\times [0,1])\cap ((-\infty,x]\times (-\infty,y])=[0,x]\times [0,y].\)</span> Ver figura siguiente.</li>
</ul>
<p>Por tanto en este caso,
<span class="math display">\[
F_{XY}(x,y)=\int_0^x \int_0^y 1\,du\,dv =\int_0^x 1\, du\int_0^y 1\, dy =x\cdot y.
\]</span></p>
</div>
<h3 id="ejemplo-47"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/VaUniformeBidi4.png" width="700px" /></p>
</div>
<h3 id="ejemplo-48"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Dejamos como ejercicio los otros casos. En resumen:
<span class="math display">\[
F_{XY}(x,y)=\begin{cases}
0, &amp; \mbox{ si }x&lt;0, \mbox{ o }y&lt;0,\\
x y, &amp; \mbox{ si }(x,y)\in [0,1]\times [0,1],\\
x, &amp; \mbox{ si }0\leq x\leq 1,\ y&gt;1,\\
y, &amp; \mbox{ si }x&gt;1,\ 0\leq y\leq 1,\\
1, &amp; \mbox{ si } x&gt;1,\ y&gt;1.
\end{cases}
\]</span>
¿Os suena?</p>
<p>Ver el primer ejemplo que pusimos del tema. Es la misma variable aleatoria bidimensional.
Ahora sabemos que se trata de una <strong>variable aleatoria bidimensional continua</strong>.</p>
</div>
<h3 id="ejemplo-49"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Comprobemos seguidamente que si derivamos dos veces la expresión de <span class="math inline">\(F_{XY}\)</span>, primero respecto <span class="math inline">\(x\)</span> y después respecto <span class="math inline">\(y\)</span>, obtendremos la <strong>función de densidad</strong> <span class="math inline">\(f_{XY}\)</span>.</p>
<p>Si derivamos respecto <span class="math inline">\(x\)</span> obtenemos:
<span class="math display">\[
\frac{\partial F_{XY}(x,y)}{\partial x}=\begin{cases}
0, &amp; \mbox{ si }x&lt;0, \mbox{ o }y&lt;0,\\
y, &amp; \mbox{ si }(x,y)\in [0,1]\times [0,1],\\
1, &amp; \mbox{ si }0\leq x\leq 1,\ y&gt;1,\\
0, &amp; \mbox{ si }x&gt;1,\ 0\leq y\leq 1,\\
0, &amp; \mbox{ si } x&gt;1,\ y&gt;1.
\end{cases}
\]</span>
Si ahora derivamos respecto <span class="math inline">\(y\)</span> obtenemos:
<span class="math display">\[
\frac{\partial^2 F_{XY}(x,y)}{\partial y\partial x}=\begin{cases}
0, &amp; \mbox{ si }x&lt;0, \mbox{ o }y&lt;0,\\
1, &amp; \mbox{ si }(x,y)\in [0,1]\times [0,1],\\
0, &amp; \mbox{ si }0\leq x\leq 1,\ y&gt;1,\\
0, &amp; \mbox{ si }x&gt;1,\ 0\leq y\leq 1,\\
0, &amp; \mbox{ si } x&gt;1,\ y&gt;1,
\end{cases}
\]</span>
expresión que coincide con la <strong>función de densidad</strong> <span class="math inline">\(f_{XY}(x,y)\)</span>.</p>
</div>
<h3 id="ejemplo-50"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Hallemos para finalizar las <strong>funciones de densidad marginales</strong>. Empecemos con <span class="math inline">\(f_X(x)\)</span>:
<span class="math display">\[
f_X(x)=\int_{-\infty}^\infty  f_{XY}(x,y)\, dy.
\]</span>
Recordemos que la región donde no se anulaba la <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span> era el cuadrado <span class="math inline">\([0,1]\times [0,1]\)</span>. Por tanto, fijado <span class="math inline">\(x\)</span>, el valor de <span class="math inline">\(f_X(x)\)</span> será no nulo si la recta vertical <span class="math inline">\(X=x\)</span> interseca dicho cuadrado. Y esto ocurre siempre que <span class="math inline">\(x\in (0,1)\)</span>. Por tanto,
<span class="math display">\[
f_X(x)=\begin{cases}
\int_{0}^1  f_{XY}(x,y)\, dy=\int_{0}^1  1\, dy=1, &amp; \mbox{ si }x\in (0,1),\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
Por tanto la variable <span class="math inline">\(X\)</span> sigue la distribución uniforme en el intervalo <span class="math inline">\([0,1]\)</span>.</p>
<p>Dejamos como ejercicio comprobar que la variable <span class="math inline">\(Y\)</span> también sigue la distribución uniforme en el mismo intervalo.</p>
</div>
<h3 id="ejemplo-2-1"><span class="header-section-number">2.2.6</span> Ejemplo 2</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos la variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span> con <strong>función de densidad</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
c \mathrm{e}^{-x}\mathrm{e}^{-y}, &amp; 0\leq y\leq x &lt; \infty,\\
0, &amp; \mbox{ en caso contrario,}
\end{cases}
\]</span>
donde <span class="math inline">\(c\)</span> es un valor que se tiene que hallar para que <span class="math inline">\(f_{XY}\)</span> sea función de densidad.</p>
<p>Para hallar <span class="math inline">\(c\)</span>, hemos de imponer que la integral de la función anterior debe ser 1 sobre todo el plano <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
<p>Primero fijémonos en como es la región de integración (zona morada de la figura). Fijado un valor <span class="math inline">\(x\geq 0\)</span>, el valor <span class="math inline">\(y\)</span> va desde <span class="math inline">\(y=0\)</span> hasta <span class="math inline">\(y=x\)</span>. Por tanto, para calcular el valor de <span class="math inline">\(c\)</span>, hay que hacer lo siguiente:</p>
</div>
<h3 id="ejemplo-51"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Ejemplo2Bidi.png" width="700px" /></p>
</div>
<h3 id="ejemplo-2-2"><span class="header-section-number">2.2.6</span> Ejemplo 2</h3>
<div class="example">
<p><span class="math display">\[
\begin{array}{rl}
1 &amp; =\int\int_{\mathbb{R}^2}f_{XY}(x,y)\, dx\, dy=\int_{x=0}^{x=\infty}\int_{y=0}^{y=x} c \mathrm{e}^{-x}\mathrm{e}^{-y} \, dy\, dx =c \int_{x=0}^{x=\infty}\mathrm{e}^{-x}\int_{y=0}^{y=x}\mathrm{e}^{-y}\, dy\, dx \\ &amp; =
c \int_{x=0}^{x=\infty}\mathrm{e}^{-x}\left[-\mathrm{e}^{-y}\right]_{y=0}^{y=x}\, dx = c \int_{x=0}^{x=\infty}\mathrm{e}^{-x}\left(1-\mathrm{e}^{-x}\right)\, dx =c \int_{x=0}^{x=\infty}\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right)\, dx \\ &amp; = c \left[-\mathrm{e}^{-x}+\frac{1}{2}\mathrm{e}^{-2x}\right]_{x=0}^{x=\infty} = c\left(1-\frac{1}{2}\right)=\frac{c}{2}.
\end{array}
\]</span>
El valor de <span class="math inline">\(c\)</span> será <span class="math inline">\(c=2\)</span>.</p>
<p>Vamos a calcular seguidamente su función de distribución.</p>
<p>Fijémonos que, en este caso, si <span class="math inline">\(x&lt;0\)</span> o <span class="math inline">\(y&lt;0\)</span>, <span class="math inline">\(F_{XY}(x,y)=0\)</span>, ya que el dominio <span class="math inline">\(B=(-\infty,x]\times (-\infty,y]\)</span> no interseca la zona morada del gráfico anterior.</p>
<p>Suponemos entonces que <span class="math inline">\(x\geq 0\)</span> e <span class="math inline">\(y\geq 0\)</span>.</p>
<p>Vamos a considerar dos casos:</p>
<ul>
<li><p><span class="math inline">\(x\leq y\)</span>. Ver zona verde del gráfico siguiente.</p></li>
<li><span class="math inline">\(x\geq y\)</span>. Ver zona morada del gráfico siguiente.</li>
</ul>
</div>
<h3 id="ejemplo-52"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Ejemplo2Bidi2.png" width="700px" /></p>
</div>
<h3 id="ejemplo-2-3"><span class="header-section-number">2.2.6</span> Ejemplo 2</h3>
<div class="example">
<ul>
<li>Caso <span class="math inline">\(x\leq y\)</span> (zona verde de la figura adjunta). En este caso, si hacemos la intersección de la región <span class="math inline">\(B=(-\infty,x]\times (-\infty,y]\)</span> (zona azul) con la zona morada o región donde <span class="math inline">\(f_{XY}(x,y)\neq 0\)</span> obtenemos el triángulo <span class="math inline">\(T_{x,y}=\{(u,v)\in\mathbb{R}^2,\ 0\leq u\leq x,\ 0\leq v\leq u\}.\)</span> Ver figura adjunta.</li>
</ul>
<p>Por tanto,
<span class="math display">\[
\begin{array}{rl}
F_{XY}(x,y) &amp; =\int_{u=0}^{u=x}\int_{v=0}^{v=u} f_{XY}(u,v)\,dv\,du= 2 \int_{u=0}^{u=x} \mathrm{e}^{-u}\int_{v=0}^{v=u}  \mathrm{e}^{-v}\,dv\,du  =
2 \int_{u=0}^{u=x} \mathrm{e}^{-u}\left[-\mathrm{e}^{-v}\right]_{v=0}^{v=u}\, du \\ &amp; = 2 \int_{u=0}^{u=x} \mathrm{e}^{-u} (1-\mathrm{e}^{-u})\, du =2 \int_{u=0}^{u=x} \left(\mathrm{e}^{-u}-\mathrm{e}^{-2u}\right)\, du=2 \left[-\mathrm{e}^{-u}+\frac{1}{2}\mathrm{e}^{-2u}\right]_{u=0}^{u=x}  \\ &amp; =
2\left(-\mathrm{e}^{-x}+\frac{1}{2}\mathrm{e}^{-2x}+1-\frac{1}{2}\right) =1-2\mathrm{e}^{-x}+\mathrm{e}^{-2x}.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-53"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Ejemplo2Bidi3.png" width="700px" /></p>
</div>
<h3 id="ejemplo-2-4"><span class="header-section-number">2.2.6</span> Ejemplo 2</h3>
<div class="example">
<ul>
<li>Caso <span class="math inline">\(x\geq y\)</span> (zona morada de la figura adjunta). En este caso, si hacemos la intersección de la región <span class="math inline">\(B=(-\infty,x]\times (-\infty,y]\)</span> (zona azul) con la zona morada o región donde <span class="math inline">\(f_{XY}(x,y)\neq 0\)</span> obtenemos el trapecio <span class="math inline">\(T_{x,y}=\{(u,v)\in\mathbb{R}^2,\ 0\leq v\leq y,\ v\leq u\leq x\}.\)</span> Ver figura adjunta.</li>
</ul>
<p>Por tanto,
<span class="math display">\[
\begin{array}{rl}
F_{XY}(x,y) &amp; =\int_{v=0}^{v=y}\int_{u=v}^{u=x} f_{XY}(u,v)\,dv\,du= 2 \int_{v=0}^{v=y} \mathrm{e}^{-v}\int_{u=v}^{u=x} \mathrm{e}^{-u}\,du\,dv  =
2 \int_{v=0}^{v=y} \mathrm{e}^{-v}\left[-\mathrm{e}^{-u}\right]_{u=v}^{u=x}\, dv \\ &amp; = 2 \int_{v=0}^{v=y} \mathrm{e}^{-v} (\mathrm{e}^{-v}-\mathrm{e}^{-x})\, du =2 \int_{v=0}^{v=y} \left(\mathrm{e}^{-2v}-\mathrm{e}^{-v-x}\right)\, du=2 \left[-\frac{1}{2}\mathrm{e}^{-2v}+\mathrm{e}^{-v-x}\right]_{v=0}^{v=y}  \\ &amp; =
2\left(-\frac{1}{2}\mathrm{e}^{-2y}+\mathrm{e}^{-x-y}+\frac{1}{2}-\mathrm{e}^{-x}\right) =1-2\mathrm{e}^{-x}-\mathrm{e}^{-2y}+2\mathrm{e}^{-x-y}.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-54"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Ejemplo2Bidi4.png" width="700px" /></p>
</div>
<h3 id="ejemplo-2-5"><span class="header-section-number">2.2.6</span> Ejemplo 2</h3>
<div class="example">
<p>En resumen:
<span class="math display">\[
F_{XY}(x,y)=\begin{cases}
1-2\mathrm{e}^{-x}+\mathrm{e}^{-2x}, &amp; \mbox{si }x\geq 0,\ y\geq 0,\ x\leq y,\\
1-2\mathrm{e}^{-x}-\mathrm{e}^{-2y}+2\mathrm{e}^{-x-y}, &amp; \mbox{si }x\geq 0,\ y\geq 0,\ x\geq y,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-2-6"><span class="header-section-number">2.2.6</span> Ejemplo 2</h3>
<div class="example">
<p>Comprobemos a continuación que si derivamos dos veces la expresión de <span class="math inline">\(F_{XY}\)</span>, primero respecto <span class="math inline">\(x\)</span> y después respecto <span class="math inline">\(y\)</span>, obtendremos la <strong>función de densidad</strong> <span class="math inline">\(f_{XY}\)</span>.</p>
<p>Si derivamos respecto <span class="math inline">\(x\)</span> obtenemos:
<span class="math display">\[
\frac{\partial F_{XY}(x,y)}{\partial x}=\begin{cases}
2\mathrm{e}^{-x}-2\mathrm{e}^{-2x}, &amp; \mbox{si }x\geq 0,\ y\geq 0,\ x\leq y,\\
2\mathrm{e}^{-x}-2\mathrm{e}^{-x-y}, &amp; \mbox{si }x\geq 0,\ y\geq 0,\ x\geq y,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
Si ahora derivamos respecto <span class="math inline">\(y\)</span> obtenemos:
<span class="math display">\[
\frac{\partial^2 F_{XY}(x,y)}{\partial y\partial x}=\begin{cases}
0, &amp; \mbox{si }x\geq 0,\ y\geq 0,\ x\leq y,\\
2\mathrm{e}^{-x-y}, &amp; \mbox{si }x\geq 0,\ y\geq 0,\ x\geq y,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
expresión que coincide con la <strong>función de densidad</strong> <span class="math inline">\(f_{XY}(x,y)\)</span>.</p>
</div>
<h3 id="ejemplo-2-7"><span class="header-section-number">2.2.6</span> Ejemplo 2</h3>
<div class="example">
<p>Hallemos las <strong>funciones de densidad marginales</strong>. Fijémonos que basta tener en cuenta los casos en que <span class="math inline">\(x\geq 0\)</span> e <span class="math inline">\(y\geq 0\)</span> ya que en caso contrario tanto <span class="math inline">\(f_X(x)\)</span> como <span class="math inline">\(f_Y(y)\)</span> serán nulas.</p>
<p><span class="math display">\[
\begin{array}{rl}
f_X(x) &amp; = \int_{-\infty}^{\infty} f_{XY}(x,y)\, dy =\int_{y=0}^{y=x}2\mathrm{e}^{-x-y}\, dy = 2\left[-\mathrm{e}^{-x-y}\right]_{y=0}^{y=x} = 2\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right),\mbox{ si }x\geq 0, \\
f_Y(y) &amp; = \int_{-\infty}^{\infty} f_{XY}(x,y)\, dx =\int_{x=y}^{x=\infty}2\mathrm{e}^{-x-y}\, dx = 2\left[-\mathrm{e}^{-x-y}\right]_{x=y}^{x=\infty} = 2\mathrm{e}^{-2y}, \mbox{ si }y\geq 0.
\end{array}
\]</span>
Vemos que la variable <span class="math inline">\(Y\)</span> corresponde a una distribución exponencial de parámetro <span class="math inline">\(\lambda =2\)</span>.</p>
</div>
<h3 id="ejemplo-2-con-r"><span class="header-section-number">2.2.6</span> Ejemplo 2 con <code>R</code></h3>
<div class="example">
<p>Dibujemos la <strong>función de densidad conjunta</strong> y la <strong>función de distribución conjunta</strong> con <code>R</code>. Primero las definimos:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1">fun.den.con =<span class="st"> </span><span class="cf">function</span>(x,y){<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>y<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&gt;=</span>y,</a>
<a class="sourceLine" id="cb76-2" data-line-number="2">                                   <span class="dv">2</span><span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span>x<span class="op">-</span>y),<span class="dv">0</span>)}</a>
<a class="sourceLine" id="cb76-3" data-line-number="3">fun.dist.con =<span class="st"> </span><span class="cf">function</span>(x,y){<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>y<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&lt;=</span>y,</a>
<a class="sourceLine" id="cb76-4" data-line-number="4">                    <span class="dv">1-2</span><span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span>x)<span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>x),<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>y<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&gt;=</span>y,</a>
<a class="sourceLine" id="cb76-5" data-line-number="5">                    <span class="dv">1-2</span><span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span>x)<span class="op">-</span><span class="kw">exp</span>(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>y)<span class="op">+</span><span class="dv">2</span><span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span>x<span class="op">-</span>y),<span class="dv">0</span>))}</a></code></pre></div>
<p>A continuación las dibujamos para <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> entre <span class="math inline">\(-1\)</span> y <span class="math inline">\(4\)</span>:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1">x=<span class="kw">seq</span>(<span class="dt">from=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">to=</span><span class="dv">4</span>,<span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb77-2" data-line-number="2">y=<span class="kw">seq</span>(<span class="dt">from=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">to=</span><span class="dv">4</span>,<span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb77-3" data-line-number="3">z.fun.den.con=<span class="kw">outer</span>(x,y,fun.den.con)</a>
<a class="sourceLine" id="cb77-4" data-line-number="4">z.fun.dist.con =<span class="st"> </span><span class="kw">outer</span>(x,y,fun.dist.con)</a>
<a class="sourceLine" id="cb77-5" data-line-number="5"><span class="kw">persp</span>(x,y,z.fun.den.con,<span class="dt">theta=</span><span class="dv">50</span>,<span class="dt">phi=</span><span class="dv">40</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">shade=</span><span class="fl">0.25</span>,<span class="dt">ticktype=</span><span class="st">&quot;detailed&quot;</span>)</a>
<a class="sourceLine" id="cb77-6" data-line-number="6"><span class="kw">persp</span>(x,y,z.fun.dist.con,<span class="dt">theta=</span><span class="dv">50</span>,<span class="dt">phi=</span><span class="dv">40</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">shade=</span><span class="fl">0.25</span>,<span class="dt">ticktype=</span><span class="st">&quot;detailed&quot;</span>)</a></code></pre></div>
</div>
<h3 id="ejemplo-2-con-r-1"><span class="header-section-number">2.2.6</span> Ejemplo 2 con <code>R</code></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-48-1.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="ejemplo-2-con-r-2"><span class="header-section-number">2.2.6</span> Ejemplo 2 con <code>R</code></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-49-1.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="la-distribución-gaussiana-bidimensional"><span class="header-section-number">2.2.6</span> La distribución gaussiana bidimensional</h3>
<p>Vamos a generalizar la distribución normal a dos dimensiones.</p>
<p><l class="definition">Definición de distribución gaussiana bidimensional. </l>
Diremos que la distribución de la variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span> es <strong>gaussiana bidimensional</strong> dependiendo del parámetro <span class="math inline">\(\rho\)</span> si su <strong>función de densidad conjunta</strong> es:
<span class="math display">\[
f_{XY}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}},\ -\infty &lt;x,y&lt;\infty.
\]</span></p>
<h3 id="la-distribución-gaussiana-bidimensional-1"><span class="header-section-number">2.2.6</span> La distribución gaussiana bidimensional</h3>
<p>Propiedades de la <strong>función de densidad de la variable gaussiana bidimensional</strong>:</p>
<ul>
<li><p>Para cualquier punto <span class="math inline">\((x,y)\in\mathbb{R}^2\)</span>, la <strong>función de densidad</strong> es no nula: <span class="math inline">\(f_{XY}(x,y)&gt;0\)</span>.</p></li>
<li><p>La <strong>función de densidad</strong> tiene un único máximo absoluto en el punto <span class="math inline">\((0,0)\)</span> que vale <span class="math inline">\(f_{XY}(0,0)=\frac{1}{2\pi\sqrt{1-\rho^2}}.\)</span> Por tanto, para <span class="math inline">\(\rho=0\)</span>, dicho máximo alcanza el mínimo valor posible y si <span class="math inline">\(\rho\to \pm 1\)</span>, dicho máximo tiende a <span class="math inline">\(\infty\)</span>.</p></li>
</ul>
<h3 id="la-distribución-gaussiana-bidimensional-2"><span class="header-section-number">2.2.6</span> La distribución gaussiana bidimensional</h3>
<ul>
<li>Las densidades marginales <span class="math inline">\(f_X(x)\)</span> y <span class="math inline">\(f_Y(y)\)</span> son normales <span class="math inline">\(N(0,1)\)</span>.</li>
</ul>
<div class="dem">
<p>Veámoslo con <span class="math inline">\(f_X(x)\)</span>. Por simetría, quedaría deducido para <span class="math inline">\(f_Y(y)\)</span>:
<span class="math display">\[
\begin{array}{rl}
f_X(x) &amp; =\frac{1}{2\pi\sqrt{1-\rho^2}}\int_{-\infty}^\infty \mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}}\, dy =
\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{x^2}{2(1-\rho^2)}}\int_{-\infty}^\infty \mathrm{e}^{-\frac{(-2\rho xy+y^2)}{2(1-\rho^2)}}\, dy \\ &amp; = \frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{x^2}{2(1-\rho^2)}} \int_{-\infty}^\infty \mathrm{e}^{-\frac{(y-\rho x)^2}{2(1-\rho^2)}} \mathrm{e}^{\frac{\rho^2 x^2}{2(1-\rho^2)}}\, dy \\ &amp; =\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{x^2}{2}} \int_{-\infty}^\infty \mathrm{e}^{-\frac{(y-\rho x)^2}{2(1-\rho^2)}}\, dy,  \mbox{ hacemos cambio $z=\frac{y-\rho x}{\sqrt{1-\rho^2}}$}\\ &amp; = \frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{x^2}{2}} \int_{-\infty}^\infty \mathrm{e}^{-\frac{z^2}{2}}\sqrt{1-\rho^2}\, dy =\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}},
\end{array}
\]</span>
función que coincide con la <strong>función de densidad</strong> de la variable <span class="math inline">\(N(0,1)\)</span>.</p>
</div>
<h3 id="la-distribución-gaussiana-bidimensional-3"><span class="header-section-number">2.2.6</span> La distribución gaussiana bidimensional</h3>
<div class="dem">
<p>En el último paso hemos usado que
<span class="math display">\[
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \mathrm{e}^{-\frac{z^2}{2}}\, dz=1,
\]</span>
ya que correspondería al área de una <strong>función de densidad</strong> de una distribución <span class="math inline">\(N(0,1)\)</span>.</p>
</div>
<h3 id="la-distribución-gaussiana-bidimensional-en-r"><span class="header-section-number">2.2.6</span> La distribución gaussiana bidimensional en <code>R</code></h3>
<div class="example">
<p>En <code>R</code> existe el paquete <code>bivariate</code> para trabajar con algunas distribuciones conjuntas; en particular, con la <strong>distribución normal bidimensional</strong>.</p>
<p>La función que nos la densidad de la <strong>distribución normal bidimensional</strong> es <code>nbvpdf</code> y tiene 5 parámetros: la <strong>media</strong> de <span class="math inline">\(X\)</span> (<span class="math inline">\(\mu_X\)</span>), la <strong>media</strong> de <span class="math inline">\(Y\)</span> (<span class="math inline">\(\mu_Y\)</span>), la <strong>desviación típica</strong> de <span class="math inline">\(X\)</span> (<span class="math inline">\(\sigma_X\)</span>), la <strong>desviación típica</strong> de <span class="math inline">\(Y\)</span> (<span class="math inline">\(\sigma_Y\)</span>) y un concepto que veremos más adelante, la <strong>correlación</strong> entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> (<span class="math inline">\(\rho_{XY}\)</span>).</p>
<p>En el ejemplo que estamos tratando, los valores de los parámetros anteriores son: <span class="math inline">\(\mu_X=\mu_Y=0\)</span>, <span class="math inline">\(\sigma_X=\sigma_Y=1\)</span> y <span class="math inline">\(\rho_{XY}=\rho.\)</span></p>
<p>Vamos a hacer un gráfico de la <strong>distribución normal bidimensional</strong> para <span class="math inline">\(\rho=\frac{1}{2}.\)</span></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="kw">library</span>(bivariate)</a>
<a class="sourceLine" id="cb78-2" data-line-number="2">f =<span class="st"> </span><span class="kw">nbvpdf</span> (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb78-3" data-line-number="3"><span class="kw">plot</span>(f,<span class="ot">TRUE</span>)</a></code></pre></div>
</div>
<h3 id="la-distribución-gaussiana-bidimensional-en-r-1"><span class="header-section-number">2.2.6</span> La distribución gaussiana bidimensional en <code>R</code></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-51-1.png" width="672" style="display: block; margin: auto;" /></p>
<h2 id="independencia-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias</h2>
<h3 id="independencia-de-variables-aleatorias-discretas"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias discretas</h3>
<p>Recordemos que dos sucesos <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> son independientes si <span class="math inline">\(P(A\cap B)=P(A)\cdot P(B)\)</span>.</p>
<p>¿Cómo trasladar dicho concepto al caso de variables aleatorias?</p>
<p>En el caso de <strong>variables aleatorias discretas bidimensionales</strong> vimos que, dada una variable aleatoria bidimensional discreta <span class="math inline">\((X,Y)\)</span> con <span class="math inline">\((X,Y)(\Omega)=\{(x_i,y_j),\ i=1,2,\ldots,j=1,2,\ldots\}\)</span>, los sucesos de la forma <span class="math inline">\(\{X=x_i,\  Y=y_j\}\)</span> determinaban cómo se distribuían los valores de la variable <span class="math inline">\((X,Y)\)</span>. De ahí la definición siguiente:</p>
<h3 id="independencia-de-variables-aleatorias-discretas-1"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias discretas</h3>
<p><l class="definition">Definición de independencia para variables aleatorias bidimensionales discretas. </l>
Sean <span class="math inline">\((X,Y)\)</span> una <strong>variable aleatoria bidimensional discreta</strong> con <span class="math inline">\((X,Y)(\Omega)=\{(x_i,y_j),\ i=1,2,\ldots,j=1,2,\ldots\}\)</span> y <strong>función de probabilidad</strong> <span class="math inline">\(P_{XY}\)</span> y <strong>funciones de probabilidad marginales</strong> <span class="math inline">\(P_X\)</span> y <span class="math inline">\(P_Y\)</span>. Entonces <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes si:
<span class="math display">\[
P_{XY}(x_i,y_j)=P_X(x_i)\cdot P_Y(y_k),\ i=1,2,\ldots,j=1,2,\ldots
\]</span>
o dicho de otra forma:
<span class="math display">\[
P(X=x_i,\ Y=y_k)=P(X=x_i)\cdot P(Y=y_k),\ i=1,2,\ldots,j=1,2,\ldots
\]</span></p>
<h3 id="ejemplo-55"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado</strong></p>
<p>Consideramos la variable aleatoria <span class="math inline">\((S,P)\)</span> donde <span class="math inline">\(S\)</span> representa la suma de los valores obtenidos al lanzar dos veces un dado y <span class="math inline">\(P\)</span>, su producto.</p>
<p>En este caso <span class="math inline">\(S\)</span> y <span class="math inline">\(P\)</span> no son independientes ya que recordemos que por ejemplo <span class="math inline">\(P_{SP}(3,2)=\frac{2}{36}\)</span>, <span class="math inline">\(P_S(3)=\frac{2}{36}\)</span> y <span class="math inline">\(P_P(2)=\frac{2}{36}\)</span>, ya que en este último caso, sólo hay dos posibles resultados en los que el producto dé 2: el <span class="math inline">\((1,2)\)</span> y el <span class="math inline">\((2,1)\)</span>.</p>
<p>Entonces no se cumple que <span class="math inline">\(P_{SP}(3,2)=P_S(3)\cdot P_P(2)\)</span>, ya que <span class="math inline">\(\frac{2}{36}\neq \frac{2}{36}\cdot \frac{2}{36}\)</span>.</p>
<p>De ahí que no sean independientes ya que la condición anterior se debería cumplir para todos los valores <span class="math inline">\(x_i\)</span> e <span class="math inline">\(y_k\)</span> y hemos encontrado un contraejemplo en donde no se cumple.</p>
</div>
<p><l class="observ">Observación. </l>
Si la tabla de la <strong>función de probabilidad conjunta</strong> de <span class="math inline">\((X,Y)\)</span> contiene algún <span class="math inline">\(0\)</span>, <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> no pueden ser independientes. ¿Podéis decir por qué?</p>
<h3 id="ejemplo-56"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Veamos un caso de independencia.</p>
<p>Consideramos el experimento aleatorio de lanzar un dado dos veces. Sea <span class="math inline">\(X\)</span> el resultado del primer lanzamiento e <span class="math inline">\(Y\)</span>, el resultado del segundo lanzamiento.</p>
<p>Veamos que, en este caso, <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes.</p>
<p>El valor de <span class="math inline">\((X,Y)(\Omega)=\{(1,1),(1,2),\ldots,(6,6)\}\)</span>, en total 36 resultados.</p>
<p>La <strong>función de probabilidad conjunta</strong> en un valor cualquiera <span class="math inline">\((i,j)\)</span> con <span class="math inline">\(i,j\in\{1,2,3,4,5,6\}\)</span> será:
<span class="math inline">\(P_{XY}(i,j)=\frac{1}{36}\)</span> ya que la probabilidad que salga <span class="math inline">\(i\)</span> en el primer lanzamiento es <span class="math inline">\(\frac{1}{6}\)</span> y la probabilidad de que salga <span class="math inline">\(j\)</span> en el segundo lanzamiento, también. Por tanto, la probabilidad de que salga <span class="math inline">\(i\)</span> en el primer lanzamiento y <span class="math inline">\(j\)</span> en el segundo será: <span class="math inline">\(\frac{1}{6}\cdot \frac{1}{6}=\frac{1}{36}.\)</span></p>
</div>
<h3 id="ejemplo-57"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
Las <strong>funciones de densidad marginales</strong> de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> serán:
<div class="center">
<table style="width:100%;">
<colgroup>
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(X\)</span> o <span class="math inline">\(Y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_X\)</span> o <span class="math inline">\(P_Y\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Por tanto, para todo <span class="math inline">\((i,j)\)</span> con <span class="math inline">\(i,j\in\{1,2,3,4,5,6\}\)</span> se cumplirá:
<span class="math display">\[
P_{XY}(i,j)=\frac{1}{36}=\frac{1}{6}\cdot \frac{1}{6}=P_X(i)\cdot P_Y(j).
\]</span>
Deducimos que son independientes.</p>
</div>
<h3 id="ejemplo-con-r"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Para comprobar si dos variables aleatorias <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes o no en <code>R</code> en general, una vez calculada la tabla de la <strong>función de probabilidad</strong>, podemos calcular la tabla de <strong>independencia teórica</strong> <span class="math inline">\(P_T(x_i,y_j)\)</span> y compararlas. Ésta segunda tabla se define de la forma siguiente:
<span class="math display">\[
P_T(x_i,y_j)=P_X(x_i)\cdot P_Y(y_j),
\]</span>
donde <span class="math inline">\(P_X\)</span> y <span class="math inline">\(P_Y\)</span> son las distribuciones marginales.</p>
<p>La tabla de <strong>independencia teórica </strong> en el caso de la suma y el producto se calcularían de la forma siguiente:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1">tabla.ind.teor =<span class="st">  </span>marginal.suma<span class="op">%*%</span><span class="kw">t</span>(marginal.producto)</a>
<a class="sourceLine" id="cb79-2" data-line-number="2">tabla.ind.teor =<span class="st"> </span><span class="kw">as.data.frame</span>(tabla.ind.teor)</a>
<a class="sourceLine" id="cb79-3" data-line-number="3"><span class="kw">rownames</span>(tabla.ind.teor)=<span class="kw">rownames</span>(tabla.func.prob.conjunta)</a>
<a class="sourceLine" id="cb79-4" data-line-number="4"><span class="kw">colnames</span>(tabla.ind.teor)=<span class="kw">colnames</span>(tabla.func.prob.conjunta)</a></code></pre></div>
<p>Si comparáis los resultados de la tabla de <strong>independencia teórica</strong> mostrada en las transparencias siguientes con los resultados de la tabla de la <strong>función de probabilidad conjunta</strong>, veréis que no son iguales. Por tanto, <span class="math inline">\(S\)</span> y <span class="math inline">\(P\)</span> no son <strong>independientes</strong>.</p>
</div>
<h3 id="ejemplo-con-r-1"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
<td align="right">0.0015</td>
<td align="right">0.0023</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0031</td>
<td align="right">0.0046</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
</tr>
<tr class="odd">
<td>4</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
<td align="right">0.0046</td>
<td align="right">0.0069</td>
<td align="right">0.0046</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
</tr>
<tr class="even">
<td>5</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0062</td>
<td align="right">0.0093</td>
<td align="right">0.0062</td>
<td align="right">0.0123</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
</tr>
<tr class="odd">
<td>6</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
<td align="right">0.0077</td>
<td align="right">0.0116</td>
<td align="right">0.0077</td>
<td align="right">0.0154</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
</tr>
<tr class="even">
<td>7</td>
<td align="right">0.0046</td>
<td align="right">0.0093</td>
<td align="right">0.0093</td>
<td align="right">0.0139</td>
<td align="right">0.0093</td>
<td align="right">0.0185</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
<td align="right">0.0093</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-con-r-2"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">12</th>
<th align="right">15</th>
<th align="right">16</th>
<th align="right">18</th>
<th align="right">20</th>
<th align="right">24</th>
<th align="right">25</th>
<th align="right">30</th>
<th align="right">36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
<td align="right">0.0015</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0031</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
</tr>
<tr class="odd">
<td>4</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
<td align="right">0.0046</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
</tr>
<tr class="even">
<td>5</td>
<td align="right">0.0123</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0062</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
</tr>
<tr class="odd">
<td>6</td>
<td align="right">0.0154</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
<td align="right">0.0077</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
</tr>
<tr class="even">
<td>7</td>
<td align="right">0.0185</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
<td align="right">0.0093</td>
<td align="right">0.0093</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-con-r-3"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
<td align="right">0.0077</td>
<td align="right">0.0116</td>
<td align="right">0.0077</td>
<td align="right">0.0154</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
</tr>
<tr class="even">
<td>9</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0062</td>
<td align="right">0.0093</td>
<td align="right">0.0062</td>
<td align="right">0.0123</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
</tr>
<tr class="odd">
<td>10</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
<td align="right">0.0046</td>
<td align="right">0.0069</td>
<td align="right">0.0046</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
</tr>
<tr class="even">
<td>11</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0031</td>
<td align="right">0.0046</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
</tr>
<tr class="odd">
<td>12</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
<td align="right">0.0015</td>
<td align="right">0.0023</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-con-r-4"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">12</th>
<th align="right">15</th>
<th align="right">16</th>
<th align="right">18</th>
<th align="right">20</th>
<th align="right">24</th>
<th align="right">25</th>
<th align="right">30</th>
<th align="right">36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8</td>
<td align="right">0.0154</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
<td align="right">0.0077</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
<td align="right">0.0077</td>
<td align="right">0.0039</td>
</tr>
<tr class="even">
<td>9</td>
<td align="right">0.0123</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0062</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
</tr>
<tr class="odd">
<td>10</td>
<td align="right">0.0093</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
<td align="right">0.0046</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
<td align="right">0.0046</td>
<td align="right">0.0023</td>
</tr>
<tr class="even">
<td>11</td>
<td align="right">0.0062</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0031</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
</tr>
<tr class="odd">
<td>12</td>
<td align="right">0.0031</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
<td align="right">0.0015</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
<td align="right">0.0015</td>
<td align="right">0.0008</td>
</tr>
</tbody>
</table>
</div>
<h3 id="independencia-de-variables-aleatorias-continuas"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias continuas</h3>
<p>La definición dada para <strong>variables aleatorias discretas</strong> se traslada de forma natural a las <strong>variables aleatorias continuas</strong>:</p>
<p><l class="definition">Definición de independencia para variables aleatorias bidimensionales continuas. </l>
Sean <span class="math inline">\((X,Y)\)</span> una <strong>variable aleatoria bidimensional continua</strong> con <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span> y <strong>funciones de densidad marginales</strong> <span class="math inline">\(f_X\)</span> y <span class="math inline">\(f_Y\)</span>. Entonces <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes si:
<span class="math display">\[
f_{XY}(x,y)=f_X(x)\cdot f_Y(y),\ \mbox{para todo $x,y\in\mathbb{R}$.}
\]</span></p>
<h3 id="independencia-de-variables-aleatorias-continuas.-ejemplo"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias continuas. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos el ejemplo siguiente visto donde teníamos una <strong>variable aleatoria bidimensional continua</strong> <span class="math inline">\((X,Y)\)</span> con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
1, &amp; \mbox{ si }0\leq x\leq 1,\ 0\leq y\leq 1, \\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
y con densidad marginales:
<span class="math display">\[
f_{X}(x)=\begin{cases}
1, &amp; \mbox{ si }0\leq x\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}\quad f_{Y}(y)=\begin{cases}
1, &amp; \mbox{ si }0\leq y\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
</div>
<h3 id="independencia-de-variables-aleatorias-continuas.-ejemplo-1"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias continuas. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Veamos que son independientes.</p>
<p>Consideremos dos casos:</p>
<ul>
<li><p><span class="math inline">\((x,y)\in [0,1]\times [0,1]\)</span>. En este caso:
<span class="math display">\[
f_{XY}(x,y) =1 =1\cdot 1=f_X(x)\cdot f_Y(y).
\]</span></p></li>
<li><span class="math inline">\((x,y)\not\in [0,1]\times [0,1]\)</span>. En este caso:
<span class="math display">\[
f_{XY}(x,y) =0 = f_X(x)\cdot f_Y(y),
\]</span>
ya que si <span class="math inline">\((x,y)\not\in [0,1]\times [0,1]\)</span>, o <span class="math inline">\(x\not\in [0,1]\)</span> o <span class="math inline">\(y\not\in [0,1]\)</span>. Por tanto <span class="math inline">\(f_X(x)=0\)</span> o <span class="math inline">\(f_Y(y)=0\)</span>. En cualquier caso, <span class="math inline">\(f_X(x)\cdot f_Y(y)=0\)</span>.</li>
</ul>
</div>
<h3 id="independencia-de-variables-aleatorias-continuas.-ejemplo-2"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias continuas. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos el ejemplo siguiente visto donde teníamos una <strong>variable aleatoria bidimensional continua</strong> <span class="math inline">\((X,Y)\)</span> con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
2 \mathrm{e}^{-x}\mathrm{e}^{-y}, &amp; 0\leq y\leq x &lt; \infty,\\
0, &amp; \mbox{ en caso contrario,}
\end{cases}
\]</span>
y con densidad marginales:
<span class="math display">\[
f_X(x)  = 2\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right),\mbox{ si }x\geq 0, \quad
f_Y(y)  =  2\mathrm{e}^{-2y}, \mbox{ si }y\geq 0.
\]</span>
En este caso no son independientes ya que claramente <span class="math inline">\(f_{XY}(x,y)\neq f_X(x)\cdot f_Y(y)\)</span>.</p>
</div>
<h3 id="ejemplo-de-la-variable-gaussiana-bidimensional"><span class="header-section-number">2.2.6</span> Ejemplo de la variable gaussiana bidimensional</h3>
<p>En este caso, recordemos que la <strong>función de densidad conjunta</strong> de <span class="math inline">\((X,Y)\)</span> es:
<span class="math display">\[
f_{XY}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}},\ -\infty &lt;x,y&lt;\infty.
\]</span>
Las <strong>funciones de densidad marginales</strong> de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> correspondían a <span class="math inline">\(N(0,1)\)</span>:
<span class="math display">\[
\begin{array}{rl}
f_X(x) &amp; =\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}},\ -\infty &lt;x&lt;\infty,\\ f_Y(y) &amp; =\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{y^2}{2}},\ -\infty &lt;y&lt;\infty.
\end{array}
\]</span></p>
<h3 id="ejemplo-de-la-variable-gaussiana-bidimensional-1"><span class="header-section-number">2.2.6</span> Ejemplo de la variable gaussiana bidimensional</h3>
<p>¿Para qué valor(es) de <span class="math inline">\(\rho\)</span> las variables normales estándar <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> serían independientes?</p>
<p>o, ¿para qué valor(es) de <span class="math inline">\(\rho\)</span> se cumple?
<span class="math display">\[
f_X(x)\cdot f_Y(y)=\frac{1}{2\pi}\mathrm{e}^{-\frac{x^2+y^2}{2}} = \frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}}.
\]</span>
La respuesta es claramente para <span class="math inline">\(\rho=0\)</span>.</p>
<p>Por tanto, <span class="math inline">\(\rho\)</span> se puede interpretar como un parámetro de independencia, cuánto más cercano a cero esté, más cerca de la independencia estarán las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<h3 id="relación-de-la-independencia-y-la-función-de-distribución"><span class="header-section-number">2.2.6</span> Relación de la independencia y la función de distribución</h3>
<p>El siguiente resultado nos da la relación entre la <strong>independencia de variables aleatorias</strong> y su <strong>función de distribución conjunta</strong>:</p>
<p><l class="prop">Teorema. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional. Entonces
<span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes si, y sólo si, la <strong>función de distribución conjunta</strong> es el producto de las <strong>funciones de distribución marginales</strong> en todo valor <span class="math inline">\((x,y)\in\mathbb{R}^2\)</span>:
<span class="math display">\[
F_{XY}(x,y)=F_X(x)\cdot F_Y(y),\ (x,y)\in\mathbb{R}^2.
\]</span></p>
<h3 id="ejemplo-de-variables-aleatorias-discretas-independientes"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias discretas independientes</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideramos el experimento aleatorio de lanzar un dado dos veces. Sea <span class="math inline">\(X\)</span> el resultado del primer lanzamiento e <span class="math inline">\(Y\)</span>, el resultado del segundo lanzamiento.</p>
<p>Recordemos que, en este caso, <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes.</p>
<p>En primer lugar notemos que si <span class="math inline">\(x&lt;1\)</span> o <span class="math inline">\(y&lt;1\)</span>, <span class="math inline">\(F_{XY}(x,y)=0\)</span> ya que el suceso <span class="math inline">\(\{X\leq x,\ Y\leq y\}\)</span> es vacío.</p>
<p>De la misma forma como <span class="math inline">\(x&lt;1\)</span> o <span class="math inline">\(y&lt;1\)</span>, o el suceso <span class="math inline">\(\{X\leq x\}\)</span> o el suceso <span class="math inline">\(\{Y\leq y\}\)</span> son vacíos. Por tanto, o <span class="math inline">\(F_X(x)=0\)</span> o <span class="math inline">\(F_Y(y)=0\)</span>.</p>
<p>En cualquier caso, se cumple <span class="math inline">\(F_{XY}(x,y)=0=F_X(x)\cdot F_Y(y)\)</span>.</p>
<p>Podemos suponer, por tanto, que <span class="math inline">\(x\geq 1\)</span> e <span class="math inline">\(y\geq 1\)</span>.</p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-discretas-independientes-1"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias discretas independientes</h3>
<div class="example">
<p>Sea <span class="math inline">\((x,y)\in \mathbb{R}^2\)</span> con <span class="math inline">\(x\geq 1\)</span> e <span class="math inline">\(y\geq 1\)</span>. Podemos suponer tal que existen dos valores <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span> en <span class="math inline">\(\{1,2,\ldots\}\)</span> con <span class="math inline">\(i\leq x &lt; i+1\)</span> y <span class="math inline">\(j\leq y &lt;j+1\)</span>.</p>
<p>El valor de la <strong>función de distribución conjunta</strong> en <span class="math inline">\((x,y)\)</span> será:
<span class="math display">\[
F_{XY}(x,y)=\begin{cases}
\frac{i\cdot j}{36}, &amp; \mbox{si }i\leq 6, \ j\leq 6, \\
\frac{6 i}{36}, &amp; \mbox{si }i\leq 6,\ j\geq 6,\\
\frac{6 j}{36}, &amp; \mbox{si }i\geq 6,\ j\leq 6,\\
1, &amp; \mbox{ si }i\geq 6,\ j\geq 6,
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-discretas-independientes-2"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias discretas independientes</h3>
<div class="example">
<p>ya que:
<span class="math display">\[
\begin{array}{rl}
F_{XY}(x,y) &amp; =P(X\leq i,\ Y\leq j)=P(\{(k,l)\in \{1,2,3,4,5,6\}^2,\ |\ k\leq i,\ l\leq j\})\\ &amp; =P(\{(1,1),\ldots,(1,j),\ldots,(i,1),\ldots,(i,j)\})=\begin{cases}
\frac{i\cdot j}{36}, &amp; \mbox{si }i\leq 6, \ j\leq 6, \\
\frac{6 i}{36}, &amp; \mbox{si }i\leq 6,\ j\geq 6,\\
\frac{6 j}{36}, &amp; \mbox{si }i\geq 6,\ j\leq 6,\\
1, &amp; \mbox{ si }i\geq 6,\ j\geq 6,
\end{cases},
\end{array}
\]</span>
ya que claramente el cardinal del conjunto <span class="math inline">\(\{(1,1),\ldots,(1,j),\ldots,(i,1),\ldots,(i,j)\}\)</span> es <span class="math inline">\(\begin{cases} i\cdot j, &amp; \mbox{si }i\leq 6, \ j\leq 6, \\ 6 i, &amp; \mbox{si }i\leq 6,\ j\geq 6,\\ 6 j, &amp; \mbox{si }i\geq 6,\ j\leq 6,\\ 36, &amp; \mbox{ si }i\geq 6,\ j\geq 6. \end{cases}\)</span>.</p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-discretas-independientes-3"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias discretas independientes</h3>
<div class="example">
<p>Hallemos ahora la función de distribución de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> que consiste en el resultado del lanzamiento de un dado.</p>
<p>Dado <span class="math inline">\(x\in\mathbb{R}\)</span> con <span class="math inline">\(x\geq 1\)</span>, existe un <span class="math inline">\(i\)</span> con <span class="math inline">\(i\in\{1,2,\ldots,\}\)</span> con <span class="math inline">\(i\leq x &lt;i+1\)</span>. En este caso, el valor de <span class="math inline">\(F_X(x)\)</span> es:
<span class="math display">\[
F_X(x)=\begin{cases}
\frac{i}{6}, &amp;\mbox{si }i\leq 6,\\
1, &amp; \mbox{si }i\geq 6,
\end{cases}
\]</span>
ya que:
<span class="math display">\[
F_X(x)=F_X(i)=P(X\leq i)=P(\{k\in\{1,2,3,4,5,6\},\ |\ k\leq i\})=\begin{cases}
\frac{i}{6}, &amp;\mbox{si }i\leq 6,\\
1, &amp; \mbox{si }i\geq 6,
\end{cases},
\]</span>
ya que el cardinal del conjunto <span class="math inline">\(\{k\in\{1,2,3,4,5,6\},\ |\ k\leq i\}\)</span> es <span class="math inline">\(\begin{cases} i, &amp;\mbox{si }i\leq 6,\\ 6, &amp; \mbox{si }i\geq 6. \end{cases}\)</span></p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-discretas-independientes-4"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias discretas independientes</h3>
<div class="example">
<p>La función de distribución de <span class="math inline">\(Y\)</span> es de la misma forma.</p>
<p>Por último, comprobemos que se verifica que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>, si <span class="math inline">\(x\geq 1\)</span> e <span class="math inline">\(y\geq 1\)</span>.</p>
<p>Sea <span class="math inline">\((x,y)\in\mathbb{R}^2\)</span> y sean los enteros <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span> tales que <span class="math inline">\(i\leq x&lt;i+1\)</span> y <span class="math inline">\(j\leq y&lt;j+1\)</span>. Consideremos 4 casos:</p>
<ul>
<li><p><span class="math inline">\(i\leq 6, \ j\leq 6\)</span>. En este caso:
<span class="math display">\[
F_{XY}(x,y)=\frac{i\cdot j}{36}=\frac{i}{6}\cdot \frac{j}{6}=F_X(x)\cdot F_Y(y).
\]</span></p></li>
<li><p><span class="math inline">\(i\leq 6,\ j\geq 6\)</span>. En este caso:
<span class="math display">\[
F_{XY}(x,y)=\frac{6i}{36}=\frac{i}{6}\cdot 1=F_X(x)\cdot F_Y(y).
\]</span></p></li>
</ul>
</div>
<h3 id="ejemplo-de-variables-aleatorias-discretas-independientes-5"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias discretas independientes</h3>
<div class="example">
<ul>
<li><p><span class="math inline">\(i\geq 6,\ j\leq 6\)</span>. En este caso:
<span class="math display">\[
F_{XY}(x,y)=\frac{6j}{36}=1\cdot \frac{j}{6}=F_X(x)\cdot F_Y(y).
\]</span></p></li>
<li><p><span class="math inline">\(i\geq 6,\ j\geq 6\)</span>. En este caso:
<span class="math display">\[
F_{XY}(x,y)=1=1\cdot 1=F_X(x)\cdot F_Y(y).
\]</span></p></li>
</ul>
<p>En resumen, para todo <span class="math inline">\((x,y)\in \mathbb{R}^2\)</span> se verifica que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>, tal como queríamos ver.</p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-continuas-independientes"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias continuas independientes</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos la variable aleatoria bidimensional continua con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
1, &amp; \mbox{ si }0\leq x\leq 1,\ 0\leq y\leq 1, \\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
Su <strong>función de distribución conjunta</strong> es:
<span class="math display">\[
F_{XY}(x,y)=\begin{cases}
0, &amp; \mbox{si }x&lt;0,\mbox{ o }y&lt;0,\\
xy, &amp; \mbox{si }0\leq x\leq 1,\ 0\leq y\leq 1, \\
x, &amp; \mbox{si }0\leq x\leq 1,\ y&gt; 1, \\
y, &amp; \mbox{si }0\leq y\leq 1,\ x&gt; 1, \\
1, &amp; x\geq 1,\ y\geq 1.
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-continuas-independientes-1"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias continuas independientes</h3>
<div class="example">
<p>Recordemos también que las <strong>distribuciones marginales</strong> de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> eran uniformes en el intervalo <span class="math inline">\([0,1]\)</span>. Por tanto, las <strong>funciones de distribución marginales</strong> serán:
<span class="math display">\[
F_X(x)=\begin{cases}
0, &amp; \mbox{si }x\leq 0, \\
x, &amp; \mbox{si }0\leq x\leq 1, \\
1, &amp; \mbox{si }x\geq 1. \\
\end{cases},\quad 
F_Y(y)=\begin{cases}
0, &amp; \mbox{si }y\leq 0, \\
y, &amp; \mbox{si }0\leq y\leq 1, \\
1, &amp; \mbox{si }y\geq 1. \\
\end{cases}
\]</span>
Recordemos que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes. Verifiquemos que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>.</p>
<p>Distinguiremos cinco casos:</p>
<ul>
<li><p><span class="math inline">\(x&lt;0\)</span> o <span class="math inline">\(y&lt;0\)</span>. En este caso, <span class="math inline">\(F_{XY}(x,y)=0\)</span> y, o <span class="math inline">\(F_X(x)=0\)</span>, si <span class="math inline">\(x&lt;0\)</span>, o <span class="math inline">\(F_Y(y)=0\)</span>, si <span class="math inline">\(y&lt;0\)</span>. En cualquier caso, se cumple que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>.</p></li>
<li><p><span class="math inline">\(0\leq x\leq 1,\ 0\leq y\leq 1\)</span>. En este caso, <span class="math inline">\(F_{XY}(x,y)=xy\)</span>, <span class="math inline">\(F_X(x)=x\)</span> y <span class="math inline">\(F_Y(y)=y\)</span>. Claramente, se cumple que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>.</p></li>
</ul>
</div>
<h3 id="ejemplo-de-variables-aleatorias-continuas-independientes-2"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias continuas independientes</h3>
<div class="example">
<ul>
<li><p><span class="math inline">\(0\leq x\leq 1,\ y&gt; 1\)</span>. En este caso, <span class="math inline">\(F_{XY}(x,y)=x\)</span>, <span class="math inline">\(F_X(x)=x\)</span> y <span class="math inline">\(F_Y(y)=1\)</span>. Claramente, se cumple que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>.</p></li>
<li><p><span class="math inline">\(x &gt;1,\ 0\leq y\leq 1\)</span>. En este caso, <span class="math inline">\(F_{XY}(x,y)=y\)</span>, <span class="math inline">\(F_X(x)=1\)</span> y <span class="math inline">\(F_Y(y)=y\)</span>. Claramente, se cumple que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>.</p></li>
<li><p><span class="math inline">\(x\geq 1,\ y\geq 1\)</span>. En este caso, <span class="math inline">\(F_{XY}(x,y)=1\)</span>, <span class="math inline">\(F_X(x)=1\)</span> y <span class="math inline">\(F_Y(y)=1\)</span>. Claramente, se cumple que <span class="math inline">\(F_{XY}(x,y)=F_X(x)\cdot F_Y(y)\)</span>.</p></li>
</ul>
</div>
<h2 id="momentos-conjuntos-y-valores-esperados-conjuntos"><span class="header-section-number">2.2.6</span> Momentos conjuntos y valores esperados conjuntos</h2>
<h3 id="introducción-7"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>El <strong>valor esperado</strong> de una variable aleatoria <span class="math inline">\(X\)</span> se identifica con el <em>centro de masa de la distribución de <span class="math inline">\(X\)</span></em>.</p>
<p>La <strong>varianza</strong> proporciona una medida de la <em>extensión de la distribución</em>.</p>
<p>En el caso de dos variables aleatorias, estamos interesados en cómo <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> varían juntas.</p>
<p>En particular, nos interesa saber si la variación de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> está correlacionada. Por ejemplo, si <span class="math inline">\(X\)</span> aumenta, ¿Y tiende a aumentar o disminuir?</p>
<p>Los momentos conjuntos de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, que se definen como valores esperados de las funciones de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, proporcionan esta información.</p>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias</h3>
<p>Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional.</p>
<p>Sea <span class="math inline">\(P_{XY}\)</span> su <strong>función de probabilidad conjunta</strong> en el caso en que <span class="math inline">\((X,Y)\)</span> sea <strong>discreta</strong> y <span class="math inline">\(f_{XY}\)</span> su <strong>función de densidad conjunta</strong> en el caso en que <span class="math inline">\((X,Y)\)</span> sea <strong>continua</strong>.</p>
<p>Sea <span class="math inline">\(Z=g(X,Y)\)</span> una <strong>variable aleatoria unidimensional</strong> función de las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>. Por ejemplo:</p>
<ul>
<li>Suma de las dos variables <span class="math inline">\(g(x,y)=x+y\)</span>: <span class="math inline">\(Z=X+Y\)</span>.</li>
<li>Producto de las dos variables <span class="math inline">\(g(x,y)=x\cdot y\)</span>: <span class="math inline">\(Z=X\cdot Y\)</span>.</li>
<li>Suma de los cuadrados de las variables <span class="math inline">\(g(x,y)=x^2+y^2\)</span>: <span class="math inline">\(Z=X^2+Y^2\)</span>.</li>
</ul>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-1"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias</h3>
<p>Hay que tener en cuenta que <span class="math inline">\(Z\)</span>, como <strong>variable aleatoria unidimensional</strong> tiene una <strong>función de probabilidad</strong> <span class="math inline">\(P_Z\)</span> en el caso en que <span class="math inline">\((X,Y)\)</span> sea discreta y una <strong>función de densidad</strong> <span class="math inline">\(f_Z\)</span> en el caso en que <span class="math inline">\((X,Y)\)</span> sea continua.</p>
<p>El siguiente resultado nos dice cómo calcular el <strong>valor esperado</strong> de <span class="math inline">\(Z\)</span> sin tener que calcular <span class="math inline">\(P_Z\)</span> o <span class="math inline">\(f_Z\)</span>, sólo usando la información de la <strong>variable aleatoria conjunta</strong> <span class="math inline">\((X,Y)\)</span>:</p>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-2"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias</h3>
<p><l class="prop">Proposición. </l>
El valor esperado de <span class="math inline">\(Z\)</span> se puede hallar usando la expresión siguiente:</p>
<ul>
<li><p>en el caso en que <span class="math inline">\((X,Y)\)</span> sea discreta con <span class="math inline">\((X,Y)(\Omega)=\{(x_i,y_j),\ i=1,2,\ldots, j=1,2,\ldots\}\)</span>,
<span class="math display">\[
E(Z)  = E(g(X,Y))  =\sum_{x_i}\sum_{y_j}g(x_i,y_j)P(x_i,y_j),
\]</span></p></li>
<li><p>en el caso en que <span class="math inline">\((X,Y)\)</span> sea continua:
<span class="math display">\[
E(Z)=E(g(X,Y))=\int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y)f_{XY}(x,y)\, dx\, dy.
\]</span></p></li>
</ul>
<h3 id="ejemplo-58"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos el ejemplo de la variable <span class="math inline">\((S,P)\)</span> que nos daba la suma y el producto de los resultados cuando lanzábamos dos veces un dado.</p>
<p>Vamos a calcular <span class="math inline">\(E(S+P)\)</span>.</p>
<p>Recordemos que ya hemos calculado <span class="math inline">\(P_{SP}\)</span>. La expresión de <span class="math inline">\(E(S+P)\)</span> será:
<span class="math display">\[
\begin{array}{rll}
E(S+P) &amp; = &amp;(2+1)\cdot P_{SP}(2,1)+(3+2)\cdot P_{SP}(3,2)+(4+3)\cdot P_{SP}(4,3)+(4+4)\cdot P_{SP}(4,4)\\ &amp; &amp;
+ (5+4)\cdot P_{SP}(5,4)+(5+6)\cdot P_{SP}(5,6)+(6+5)\cdot P_{SP}(6,5)+(6+8)\cdot P_{SP}(6,8)\\ &amp; &amp; 
+ (6+9)\cdot P_{SP}(6,9)+(7+6)\cdot P_{SP}(7,6)+(7+10)\cdot P_{SP}(7,10)+(7+12)\cdot P_{SP}(7,12)\\ &amp; &amp; 
+ (8+12)\cdot P_{SP}(8,12)+(8+15)\cdot P_{SP}(8,15)+(8+16)\cdot P_{SP}(8,16)\\ &amp; &amp; +(9+18)\cdot P_{SP}(9,18)
+ (9+20)\cdot P_{SP}(9,20)+(10+24)\cdot P_{SP}(10,24)\\ &amp; &amp; +(10+25)\cdot P_{SP}(10,25)+(11+30)\cdot P_{SP}(11,30) 
+ (12+36)\cdot P_{SP}(12,36) \\ &amp; = &amp; 3\cdot \frac{1}{36}+5\cdot\frac{2}{36}+7\cdot \frac{2}{36}+8\cdot \frac{1}{36}+9\cdot \frac{2}{36}+11\cdot\frac{2}{36}+11\cdot \frac{2}{36}+14\cdot\frac{2}{36}+15\cdot\frac{1}{36}\\ &amp; &amp; 
+ 13\cdot\frac{2}{36}+17\cdot\frac{2}{36}+19\cdot\frac{2}{36}+20\cdot\frac{2}{36}+23\cdot\frac{2}{36}+24\cdot\frac{1}{36}+27\cdot\frac{2}{36}+29\cdot\frac{2}{36} \\ &amp; &amp; 
+ 34\cdot\frac{2}{36}+35\cdot\frac{1}{36}+41\cdot\frac{2}{36}+48\cdot\frac{1}{36}=\frac{693}{36}= 19.25.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-con-r-5"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Hallar el valor esperado de la suma <span class="math inline">\(E(S+P)\)</span> una vez hallada la tabla de la <strong>función de probabilidad conjunta</strong>, en <code>R</code> es bastante sencillo usando la función <code>outer</code>:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1">valores.suma =<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">rownames</span>(tabla.func.prob.conjunta))</a>
<a class="sourceLine" id="cb80-2" data-line-number="2">valores.producto =<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">colnames</span>(tabla.func.prob.conjunta))</a>
<a class="sourceLine" id="cb80-3" data-line-number="3">suma.valores =<span class="st"> </span><span class="kw">outer</span>(valores.suma,valores.producto,<span class="st">&quot;+&quot;</span>)</a>
<a class="sourceLine" id="cb80-4" data-line-number="4">(<span class="dt">valor.esperado.suma =</span> <span class="kw">sum</span>(suma.valores<span class="op">*</span>tabla.func.prob.conjunta))</a></code></pre></div>
<pre><code>## [1] 19.25</code></pre>
</div>
<p><l class="observ">Observación:</l>
En <code>R</code> para hallar el valor esperado de una función <span class="math inline">\(g(X,Y)\)</span>, <span class="math inline">\(E(g(X,Y))\)</span> de las variables aleatorias <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, basta sustituir el valor <code>+</code> en el script anterior por <code>FUN=g</code>, definiendo previamente la función <code>g</code>.</p>
<h3 id="ejemplo-59"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos el ejemplo donde <span class="math inline">\((X,Y)\)</span> era una variable aleatoria bidimensional continua con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
2 \mathrm{e}^{-x}\mathrm{e}^{-y}, &amp; 0\leq y\leq x &lt; \infty,\\
0, &amp; \mbox{ en caso contrario.}
\end{cases}
\]</span>
Calculemos <span class="math inline">\(E(X\cdot Y)\)</span>:
<span class="math display">\[
\begin{array}{ll}
E(X\cdot Y) &amp; =\int_{x=0}^{x=\infty} \int_{y=0}^{y=x} 2 x y \mathrm{e}^{-x}\mathrm{e}^{-y}\, dy\, dx=2\int_{x=0}^{x=\infty} x \mathrm{e}^{-x} \int_{y=0}^{y=x}  y \mathrm{e}^{-y}\, dy\, dx \\ &amp; = 2\int_{x=0}^{x=\infty}x \mathrm{e}^{-x} \left[-\mathrm{e}^{-y} (y+1)\right]_{y=0}^{y=x}\, dx =
2\int_{x=0}^{x=\infty}x \mathrm{e}^{-x} \left(1-\mathrm{e}^{-x}(x+1)\right)\, dx \\ &amp;= 2\int_{x=0}^{x=\infty}x\left( \mathrm{e}^{-x}-\mathrm{e}^{-2x}\right)-x^2\mathrm{e}^{-2x}\, dx \\ &amp; =
2\left[-\mathrm{e}^{-x}(x+1)+\frac{1}{4}\mathrm{e}^{-2 x}(1+2x)+\frac{1}{4} \mathrm{e}^{-2 x} \left(2 x^2+2
   x+1\right)\right]_{x=0}^{x=\infty} = 2\cdot \left(1-\frac{1}{4}-\frac{1}{4}\right)=1.
\end{array}
\]</span>
En el último cálculo hemos usado integración por partes para integrar <span class="math inline">\(\int x\mathrm{e}^{-x}\,dx\)</span>, <span class="math inline">\(\int x\mathrm{e}^{-2x}\,dx\)</span> y <span class="math inline">\(\int x^2\mathrm{e}^{-2x}\, dx\)</span>.</p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Hallar <span class="math inline">\(E(X+Y)\)</span> para el ejemplo anterior.</p>
</div>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-independientes"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias independientes</h3>
<p>El siguiente resultado nos simplifica el cálculo del <strong>valor esperado de una función de dos variables aleatorias</strong> en el caso en que sean <strong>independientes</strong>:</p>
<p><l class="prop">Proposición: cálculo del valor esperado de una función de dos variables aleatorias en el caso de independencia. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional donde suponemos que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes.
Sea <span class="math inline">\(Z=g(X,Y)\)</span> una variable aleatoria unidimensional función de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> donde suponemos que podemos “separar” las variables <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> en la función <span class="math inline">\(g\)</span>. O sea, existen dos funciones <span class="math inline">\(g_x\)</span> y <span class="math inline">\(g_y\)</span> tal que <span class="math inline">\(g(x,y)=g_x(x)\cdot g_y(y)\)</span> para todo valor <span class="math inline">\(x,y\in\mathbb{R}\)</span>. En este caso, el valor esperado de <span class="math inline">\(Z\)</span> se puede calcular como:
<span class="math display">\[
E(Z)=E(g(X,Y))=E_X(g_x(X))\cdot E_Y(g_y(Y)).
\]</span></p>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-independientes-1"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias independientes</h3>
<p>O sea, el cálculo de <span class="math inline">\(E(g(X,Y))\)</span> que sería una suma doble en el caso de que <span class="math inline">\((X,Y)\)</span> sea <strong>discreta</strong> o una integral doble en el caso en que <span class="math inline">\((X,Y)\)</span> sea continua se transforma en el producto de dos sumas simples (caso <strong>discreto</strong>) o el producto de dos integrales simples (caso <strong>continuo</strong>):
<span class="math display">\[
\begin{array}{rl}
E(Z) &amp; =E(g(X,Y))=\left(\sum_{x_i} g_x(x_i)\cdot P_X(x_i)\right)\cdot \left(\sum_{y_j} g_y(y_j)\cdot P_Y(y_j)\right),\\ &amp;\ \quad \mbox{caso discreto},\\
E(Z) &amp; =E(g(X,Y))=\left(\int_{-\infty}^\infty g_x(x)\cdot f_X(x)\, dx\right)\cdot \left(\int_{-\infty}^\infty g_y(y)\cdot f_Y(y)\right), \\  &amp;\ \quad \mbox{caso continuo}.
\end{array}
\]</span></p>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-independientes-2"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias independientes</h3>
<p>Un caso particular de aplicación de la proposición anterior sería cuando queramos calcular <span class="math inline">\(E(X\cdot Y)\)</span>. En este caso <span class="math inline">\(g(x,y)=x\cdot y\)</span>, <span class="math inline">\(g_x(x)=x\)</span>, y <span class="math inline">\(g_y(y)=y\)</span>.</p>
<p>Podemos escribir, por tanto:
<span class="math display">\[
E(X\cdot Y)=E_X(X)\cdot E_Y(Y),
\]</span>
si <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes.</p>
<h3 id="ejemplo-60"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos el experimento aleatorio que consiste en lanzar un dado dos veces. Sea <span class="math inline">\(X\)</span> el resultado del primer lanzamiento e <span class="math inline">\(Y\)</span>, el resultado del segundo lanzamiento.</p>
<p>Hemos visto que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes.</p>
Las marginales de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> recordemos que son las siguientes:
<div class="center">
<table style="width:100%;">
<colgroup>
<col width="18%" />
<col width="13%" />
<col width="13%" />
<col width="13%" />
<col width="13%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(X\)</span> o <span class="math inline">\(Y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_X(i)\)</span> o <span class="math inline">\(P_Y(i)\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Calculemos <span class="math inline">\(E(X\cdot Y)\)</span> usando la proposición anterior:
<span class="math display">\[
E(X\cdot Y)=E_X(X)\cdot E_Y(Y)=\left(\sum_{i=1}^6 i\cdot \frac{1}{6}\right)\cdot \left(\sum_{i=1}^6 i\cdot \frac{1}{6}\right)=\left(\frac{21}{6}\right)^2 = 12.25.
\]</span>
Dejamos como ejercicio el cálculo de <span class="math inline">\(E(X\cdot Y)\)</span> usando la <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}\)</span> y comprobar que da el mismo resultado.</p>
</div>
<h3 id="ejemplo-61"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos la variable aleatoria bidimensional continua con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
1, &amp; \mbox{ si }0\leq x\leq 1,\ 0\leq y\leq 1, \\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
donde vimos que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> eran independientes y de distribución uniforme en el intervalo <span class="math inline">\([0,1]\)</span>.</p>
<p>Calculemos <span class="math inline">\(E(X\cdot Y)\)</span> usando la proposición:
<span class="math display">\[
E(X\cdot Y)=E_X(X)\cdot E_Y(Y)=\int_0^1 x\cdot 1\, dx\cdot \int_0^1 y\cdot 1\, dy =\left.\frac{x^2}{2}\right]_{x=0}^{x=1}\cdot \left.\frac{y^2}{2}\right]_{y=0}^{y=1}=\frac{1}{2}\cdot \frac{1}{2}=\frac{1}{4}.
\]</span>
Dejamos como ejercicio el cálculo de <span class="math inline">\(E(X\cdot Y)\)</span> usando la <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span> y comprobar que da el mismo resultado.</p>
</div>
<h3 id="momentos-conjuntos"><span class="header-section-number">2.2.6</span> Momentos conjuntos</h3>
<p>A continuación vamos a definir el momento de orden <span class="math inline">\((k,l)\)</span> para una variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span> para intentar obtener información de su comportamiento conjunto:</p>
<p><l class="definition">Definición de momento conjunto. </l>
Sean <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional con <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}\)</span> en el caso discreto y <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span> en el caso continuo. Dados <span class="math inline">\(k\)</span> y <span class="math inline">\(l\)</span> números enteros positivos, definimos el <strong>momento conjunto de orden <span class="math inline">\((k,l)\)</span></strong> para la variable <span class="math inline">\((X,Y)\)</span> como:
<span class="math display">\[
E\left(X^k Y^l\right)=\begin{cases}
\sum_{x_i}\sum_{y_j} x_i^k y_j^l P_{XY}(x_i,y_j), &amp; \mbox{ caso discreto,} \\
\int_{-\infty}^\infty\int_{-\infty}^\infty x^k y^l f_{XY}(x,y)\, dx\, dy. &amp; \mbox{ caso continuo.}
\end{cases}
\]</span></p>
<h3 id="momentos-conjuntos-1"><span class="header-section-number">2.2.6</span> Momentos conjuntos</h3>
<p><l class="observ">Observación.</l>
Si consideramos <span class="math inline">\(l=0\)</span>, los momentos conjuntos de orden <span class="math inline">\((k,0)\)</span> coinciden con los momentos de orden <span class="math inline">\(k\)</span> de la variable aleatoria <span class="math inline">\(X\)</span>.</p>
<p>De la misma forma, considerando <span class="math inline">\(k=0\)</span>, los momentos conjuntos de orden <span class="math inline">\((0,l)\)</span> coinciden con los momentos de orden <span class="math inline">\(l\)</span> de la variable aleatoria <span class="math inline">\(Y\)</span>.</p>
<p>Para <span class="math inline">\(l=1\)</span> y <span class="math inline">\(k=1\)</span> obtenemos el momento de orden <span class="math inline">\((1,1)\)</span> ya visto anteriormente: <span class="math inline">\(E(X\cdot Y)\)</span>, denominado <strong>correlación entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span></strong>. Si dicha correlación es nula, <span class="math inline">\(E(X\cdot Y)=0\)</span>, se dice que las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son <strong>ortogonales</strong>.</p>
<h3 id="momentos-conjuntos-centrados-en-las-medias"><span class="header-section-number">2.2.6</span> Momentos conjuntos centrados en las medias</h3>
<p>A continuación definamos los <strong>momentos conjuntos centrados en las medias</strong>:</p>
<p><l class="definition">Definición de momento conjunto. </l>
Sean <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional con <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}\)</span> en el caso discreto y <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span> en el caso continuo. Sean <span class="math inline">\(\mu_X=E(X)\)</span> y <span class="math inline">\(\mu_Y=E(Y)\)</span> los <strong>valores esperados</strong> de las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, respectivamente. Dados <span class="math inline">\(k\)</span> y <span class="math inline">\(l\)</span> números enteros positivos, definimos el <strong>momento conjunto de orden <span class="math inline">\((k,l)\)</span> centrado en las medias</strong> para la variable <span class="math inline">\((X,Y)\)</span> como:
<span class="math display">\[
E\left((X-\mu_X)^k (Y-\mu_Y)^l\right)=\begin{cases}
\sum_{x_i}\sum_{y_j} (x_i-\mu_X)^k (y_j-\mu_Y)^l P_{XY}(x_i,y_j), &amp; \\\ \qquad \mbox{ caso discreto,}&amp; \\
\int_{-\infty}^\infty\int_{-\infty}^\infty (x-\mu_X)^k (y-\mu_Y)^l f_{XY}(x,y)\, dx\, dy. &amp; \\ \ \qquad\mbox{ caso continuo.} &amp;
\end{cases}
\]</span></p>
<h3 id="covariancia-entre-las-variables"><span class="header-section-number">2.2.6</span> Covariancia entre las variables</h3>
<p>El <strong>momento conjunto centrado en las medias para <span class="math inline">\(k=1\)</span> y <span class="math inline">\(l=1\)</span></strong> se denomina <strong>covariancia</strong> entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>:
<span class="math display">\[
\mathrm{Cov}(X,Y)=E((X-\mu_X)(Y-\mu_Y)).
\]</span>
La covariancia puede calcularse a partir de la <strong>correlación</strong> entre las variables:
<span class="math display">\[
\mathrm{Cov}(X,Y)=E((X-\mu_X)(Y-\mu_Y))=E(X\cdot Y)-\mu_X\cdot \mu_Y,
\]</span></p>
<h3 id="covariancia-entre-las-variables-1"><span class="header-section-number">2.2.6</span> Covariancia entre las variables</h3>
<p>ya que, usando las propiedades de la esperanza, tenemos:
<span class="math display">\[
\begin{array}{rl}
E((X-\mu_X)(Y-\mu_Y)) &amp; =E(X\cdot Y-\mu_Y X-\mu_X Y+\mu_X\cdot \mu_Y)\\ &amp; =E(X\cdot Y)-\mu_YE(X)-\mu_X E(Y)+\mu_X\cdot \mu_Y \\ &amp;  = E(X\cdot Y)-\mu_Y\cdot \mu_X-\mu_X \cdot \mu_Y+\mu_X\cdot \mu_Y \\ &amp; = E(X\cdot Y)-\mu_X\cdot \mu_Y.
\end{array}
\]</span></p>
<h3 id="covarianza-entre-las-variables"><span class="header-section-number">2.2.6</span> Covarianza entre las variables</h3>
<p><l class="observ">Observación. </l>
Si las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son <strong>independientes</strong>, su <strong>covarianza</strong> es nula ya que vimos que <span class="math inline">\(E(X\cdot Y)=\mu_X\cdot \mu_y\)</span>.</p>
<p>La <strong>covarianza</strong> es una medida de lo relacionadas están las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>:</p>
<ul>
<li><p>Si cuando <span class="math inline">\(X\geq \mu_X\)</span>, también ocurre que <span class="math inline">\(Y\geq \mu_Y\)</span> o viceversa, cuando <span class="math inline">\(X\leq \mu_X\)</span>, también ocurre que <span class="math inline">\(Y\leq \mu_Y\)</span>, el valor <span class="math inline">\((X-\mu_X)(Y-\mu_Y)\)</span> será positivo y la <strong>covarianza</strong> será positiva.</p></li>
<li><p>Si por el contrario, cuando <span class="math inline">\(X\geq \mu_X\)</span>, también ocurre que <span class="math inline">\(Y\leq \mu_Y\)</span> o viceversa, cuando <span class="math inline">\(X\leq \mu_X\)</span>, también ocurre que <span class="math inline">\(Y\geq \mu_Y\)</span>, el valor <span class="math inline">\((X-\mu_X)(Y-\mu_Y)\)</span> será negativo y la <strong>covarianza</strong> será negativa.</p></li>
<li><p>En cambio, si a veces ocurre una cosa y a veces ocurre otra, la <strong>covarianza</strong> va cambiando de signo y puede tener un valor cercano a 0.</p></li>
</ul>
<h3 id="propiedades-de-la-covarianza"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<ul>
<li>Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional. Entonces la <strong>varianza de la suma/resta</strong> se calcula usando la expresión siguiente:
<span class="math display">\[
\mathrm{Var}(X\pm Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)\pm 2 \mathrm{Cov}(X,Y).
\]</span></li>
</ul>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>La varianza de la suma/resta de las variables es, usando la propiedad de la <strong>varianza</strong>:
<span class="math display">\[
\mathrm{Var}(X\pm Y)=E\left((X\pm Y)^2\right)-\left(E(X\pm Y)\right)^2.
\]</span>
Desarrollando las expresiones anteriores, obtenemos:
<span class="math display">\[
\begin{array}{rl}
\mathrm{Var}(X\pm Y) &amp; =E\left(X^2+Y^2\pm 2XY\right)-\left(E(X)\pm E(Y)\right)^2 \\ &amp; =E(X^2)+E(Y^2)\pm 2E(XY)-\left(E(X)^2+E(Y)^2\pm 2E(X)E(Y)\right) 
\\ &amp; = E(X^2)-E(X)^2+E(Y^2)-E(Y)^2\pm 2(E(XY)-E(X)E(Y)) \\ &amp; = \mathrm{Var}(X)+\mathrm{Var}(Y)\pm 2\mathrm{Cov}(X,Y),
\end{array}
\]</span>
tal como queríamos ver.</p>
</div>
<h3 id="propiedades-de-la-covarianza-1"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<p>Una consecuencia de la propiedad anterior es el resultado siguiente:</p>
<p><l class="prop">Proposición: si las variables son independientes, la varianza de la suma es la suma de varianzas. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional donde las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son <strong>independientes</strong>.
Entonces:
<span class="math display">\[
\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y).
\]</span></p>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>La demostración es muy sencilla: basta aplicar la fórmula vista anteriormente de la varianza y tener en cuenta que, como <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes, su covarianza es cero: <span class="math inline">\(\mathrm{Cov}(X,Y)=0\)</span>.</p>
</div>
<h3 id="coeficiente-de-correlación-entre-las-variables"><span class="header-section-number">2.2.6</span> Coeficiente de correlación entre las variables</h3>
<p>La <strong>covarianza</strong> depende de las unidades en las que están las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> ya que si <span class="math inline">\(a&gt;0\)</span> y <span class="math inline">\(b&gt;0\)</span>, entonces:
<span class="math display">\[
\mathrm{Cov}(aX,bY)=a\cdot b\cdot \mathrm{Cov}(X,Y).
\]</span>
Por tanto, si queremos “medir” la relación que existe entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> tendremos que “normalizar” la <strong>covarianza</strong> definiendo el <strong>coeficiente de correlación</strong> entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>:</p>
<h3 id="coeficiente-de-correlación-entre-las-variables-1"><span class="header-section-number">2.2.6</span> Coeficiente de correlación entre las variables</h3>
<p><l class="definition">Definición del coeficiente de correlación. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional. Se define el <strong>coeficiente de correlación</strong> entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> como:
<span class="math display">\[
\rho_{XY}=\frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)}\cdot\sqrt{\mathrm{Var}(Y)}}=\frac{E(X\cdot Y)-\mu_X\cdot \mu_Y}{\sqrt{E\left(X^2\right)-\mu_X^2}\cdot \sqrt{E\left(Y^2\right)-\mu_Y^2}}.
\]</span></p>
<h3 id="coeficiente-de-correlación-entre-las-variables-2"><span class="header-section-number">2.2.6</span> Coeficiente de correlación entre las variables</h3>
<p><l class="observ">Observación. </l>
Si las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son <strong>independientes</strong>, su <strong>coeficiente de correlación</strong> <span class="math inline">\(\rho_{XY}=0\)</span> es nulo ya que su <strong>covarianza</strong> lo es.</p>
<p>Notemos también que la <strong>correlación</strong> no tiene unidades y es invariante a cambios de escala.</p>
<p>Además, la <strong>covarianza</strong> de las <strong>variables tipificadas</strong> <span class="math inline">\(\frac{X-\mu_X}{\sigma_X}\)</span> y <span class="math inline">\(\frac{Y-\mu_Y}{\sigma_Y}\)</span> coincide con la <strong>correlación</strong> de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<h3 id="coeficiente-de-correlación-entre-las-variables-3"><span class="header-section-number">2.2.6</span> Coeficiente de correlación entre las variables</h3>
<p>El <strong>coeficiente de correlación</strong> es un valor normalizado ya que siempre está entre -1 y 1: <span class="math inline">\(-1\leq\rho_{XY}\leq 1\)</span>.</p>
<div class="dem">
<p>Para ver la demostración de este hecho, sean <span class="math inline">\(\mu_X=E(X)\)</span>, <span class="math inline">\(\mu_Y=E(Y)\)</span>, <span class="math inline">\(\sigma_X=\sqrt{\mathrm{Var}(X)}\)</span> y <span class="math inline">\(\sigma_Y=\sqrt{\mathrm{Var}(Y)}\)</span>.</p>
<p>Consideremos la variable <span class="math inline">\(Z=\left(\frac{X-\mu_X}{\sigma_X}\pm \frac{Y-\mu_Y}{\sigma_Y}\right)^2\)</span>. Como <span class="math inline">\(Z\geq 0\)</span>, tenemos que <span class="math inline">\(E(Z)\geq 0\)</span>. Desarrollemos el valor de <span class="math inline">\(E(Z)\)</span>:</p>
<p><span class="math display">\[
\begin{array}{rl}
E(Z) &amp; = E\left(\frac{X-\mu_X}{\sigma_X}\pm \frac{Y-\mu_Y}{\sigma_Y}\right)^2 = E\left(\left(\frac{X-\mu_X}{\sigma_X}\right)^2+\left(\frac{Y-\mu_Y}{\sigma_Y}\right)^2\pm 2\left(\frac{X-\mu_X}{\sigma_X}\right) \left(\frac{Y-\mu_Y}{\sigma_Y}\right)\right) \\ &amp; =
E\left(\left(\frac{X-\mu_X}{\sigma_X}\right)^2\right)+E\left(\left(\frac{Y-\mu_Y}{\sigma_Y}\right)^2\right)\pm 2 E\left(\left(\frac{X-\mu_X}{\sigma_X}\right) \left(\frac{Y-\mu_Y}{\sigma_Y}\right)\right) \\ &amp; =
\frac{1}{\sigma_X^2}E\left(\left(X-\mu_X\right)^2\right)+\frac{1}{\sigma_Y^2}E\left(\left(Y-\mu_Y\right)^2\right)\pm \frac{2}{\sigma_X\sigma_Y}E\left(\left(X-\mu_X\right) \left(Y-\mu_Y\right)\right) \\ &amp; = \frac{1}{\sigma_X^2}\sigma_X^2+
\frac{1}{\sigma_Y^2}\sigma_Y^2 \pm\frac{2}{\sigma_X\sigma_Y} \mathrm{Cov}(X,Y) = 1+1\pm 2\frac{\mathrm{Cov}(X,Y)}{\sigma_X\sigma_Y}=2(1\pm\rho_{XY})
\end{array}
\]</span></p>
</div>
<h3 id="coeficiente-de-correlación-entre-las-variables-4"><span class="header-section-number">2.2.6</span> Coeficiente de correlación entre las variables</h3>
<div class="dem">
<p>Ahora, como <span class="math inline">\(E(Z)\geq 0\)</span>, tenemos que <span class="math inline">\(1\pm \rho_{XY}\geq 0\)</span>, lo que significa que, por un lado <span class="math inline">\(1+\rho_{XY}\geq 0\)</span> y, por otro, <span class="math inline">\(1-\rho_{XY}\geq 0\)</span>. De la primera inecuación, deducimos que <span class="math inline">\(\rho_{XY}\geq -1\)</span> y de la segunda, <span class="math inline">\(\rho_{XY}\leq 1\)</span>.</p>
<p>En resumen, <span class="math inline">\(-1\leq\rho_{XY}\leq 1\)</span>, tal como queríamos ver.</p>
</div>
<h3 id="ejemplo-62"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Hallemos el <strong>coeficiente de correlación</strong> para el ejemplo de la variable aleatoria bidimensional continua con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
2 \mathrm{e}^{-x}\mathrm{e}^{-y}, &amp; 0\leq y\leq x &lt; \infty,\\
0, &amp; \mbox{ en caso contrario,}
\end{cases}
\]</span>
Recordemos los cálculos realizados anteriormente:</p>
<ul>
<li><p><span class="math inline">\(E(X\cdot Y)=1.\)</span></p></li>
<li><span class="math inline">\(f_X(x)=2\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right)\)</span>, si <span class="math inline">\(x\geq 0\)</span>. Su esperanza será:
<span class="math display">\[
E(X)=\int_0^\infty x 2\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right)\, dx=2 \left[\frac{1}{4} \mathrm{e}^{-2 x} (2 x+1)-\mathrm{e}^{-x}(x+1)\right]_0^\infty = 2\left(1-\frac{1}{4}\right)=\frac{3}{2}.
\]</span></li>
</ul>
</div>
<h3 id="ejemplo-63"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Calculemos a continuación su varianza: <span class="math inline">\(\mathrm{Var}(X)=E\left(X^2\right)-\mu_X^2\)</span>. El valor de <span class="math inline">\(E\left(X^2\right)\)</span> será:
<span class="math display">\[
\begin{array}{rl}
E\left(X^2\right) &amp; =\int_0^\infty x^2 2\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right)\, dx=2 \left[\frac{1}{4} \mathrm{e}^{-2 x}  (2x^2+2x+1)- \mathrm{e}^{-x} (x^2+2x+2)\right]_0^\infty \\ &amp; = 2\left(2-\frac{1}{4}\right)=\frac{7}{2}.
\end{array}
\]</span>
El valor de la varianza de <span class="math inline">\(X\)</span> será: <span class="math inline">\(\mathrm{Var}(X)=\frac{7}{2}-\left(\frac{3}{2}\right)^2 = \frac{5}{4}.\)</span></p>
<ul>
<li>La variable <span class="math inline">\(Y\)</span> era exponencial de parámetro <span class="math inline">\(\lambda =2\)</span>. Por tanto, <span class="math inline">\(E(Y)=\frac{1}{2}\)</span>, <span class="math inline">\(\mathrm{Var}(Y)=\frac{1}{4}\)</span>.</li>
</ul>
<p>El <strong>coeficiente de correlación</strong> entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> será:
<span class="math display">\[
\rho_{XY}=\frac{E(X\cdot Y)-\mu_X\cdot \mu_Y}{\sqrt{\mathrm{Var}(X)}\cdot\sqrt{\mathrm{Var}(Y)}}=\frac{1-\frac{3}{2}\cdot \frac{1}{2}}{\sqrt{\frac{5}{4}}\cdot\sqrt{\frac{1}{4}}}=\frac{\sqrt{5}}{5}\approx 0.447.
\]</span>
Vemos que la <strong>correlación</strong> entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> es positiva pero no demasiado ya que su valor no está cercano a 1.</p>
</div>
<h3 id="ejemplo-normal-bidimensional"><span class="header-section-number">2.2.6</span> Ejemplo normal bidimensional</h3>
<div class="example">
<p>Recordemos que la <strong>función de densidad</strong> de la variable aleatoria <strong>normal bidimensional</strong> es:
<span class="math inline">\(f_{XY}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}},\ -\infty &lt;x,y&lt;\infty.\)</span></p>
<p>Las <strong>variables aleatorias marginales</strong> eran normales estándar o <span class="math inline">\(N(0,1)\)</span>.</p>
<p>Hallemos el <strong>coeficiente de correlación <span class="math inline">\(\rho_{XY}\)</span></strong> en este caso.</p>
<p>Calculemos <span class="math inline">\(E(X\cdot Y)\)</span>:
<span class="math display">\[
\begin{array}{rl}
E(X\cdot Y) &amp; = \int_{-\infty}^\infty x y \frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}}\, dy\, dx = \frac{1}{2\pi\sqrt{1-\rho^2}}\int_{x=-\infty}^{x=\infty}x  \mathrm{e}^{-\frac{x^2}{2(1-\rho^2)}}\int_{y=-\infty}^{y=\infty}y \mathrm{e}^{-\frac{(-2\rho xy+y^2)}{2(1-\rho^2)}}\, dy\, dx \\ &amp; = \frac{1}{2\pi\sqrt{1-\rho^2}}\int_{x=-\infty}^{x=\infty}x  \mathrm{e}^{-\frac{x^2}{2(1-\rho^2)}}  \mathrm{e}^{\frac{\rho^2 x^2}{2(1-\rho^2)}} \int_{y=-\infty}^{y=\infty}y \mathrm{e}^{-\frac{(y-\rho y)^2}{2(1-\rho^2)}}\, dy\, dx,\\ &amp;\ \qquad\mbox{ cambio de variable en la segunda integral $z=\frac{y-\rho x}{\sqrt{1-\rho^2}}$,}\\ &amp; = \frac{1}{2\pi\sqrt{1-\rho^2}}\int_{x=-\infty}^{x=\infty}x  \mathrm{e}^{-\frac{x^2}{2}}  \int_{z=-\infty}^{z=\infty} \left(z\sqrt{1-\rho^2}+\rho x\right)\sqrt{1-\rho^2}\mathrm{e}^{-\frac{z^2}{2}}\, dz\, \\ &amp; =
\frac{1}{2\pi} \int_{x=-\infty}^{x=\infty}x  \mathrm{e}^{-\frac{x^2}{2}} \left(\sqrt{1-\rho^2}\int_{z=-\infty}^{z=\infty} z \mathrm{e}^{-\frac{z^2}{2}}\, dz +\rho x \int_{z=-\infty}^{z=\infty}\mathrm{e}^{-\frac{z^2}{2}}\, dz \right)\, dx
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-normal-bidimensional-1"><span class="header-section-number">2.2.6</span> Ejemplo normal bidimensional</h3>
<div class="example">
<p>Ahora, usando que el valor esperado de una variable <span class="math inline">\(N(0,1)\)</span> es cero tenemos que:
<span class="math inline">\(\int_{z=-\infty}^{z=\infty} z \mathrm{e}^{-\frac{z^2}{2}}\, dz =0,\)</span> y usando que la integral de la <strong>función de densidad</strong> de la <span class="math inline">\(N(0,1)\)</span> (<span class="math inline">\(\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{z^2}{2}}\)</span>) sobre todo <span class="math inline">\(\mathbb{R}\)</span> es 1, tenemos que:
<span class="math inline">\(\int_{z=-\infty}^{z=\infty} \mathrm{e}^{-\frac{z^2}{2}}\, dz =\sqrt{2\pi}.\)</span></p>
<p>Por tanto,
<span class="math display">\[
E(X\cdot Y)=\frac{\rho}{2\pi} \int_{x=-\infty}^{x=\infty} x^2  \mathrm{e}^{-\frac{x^2}{2}}\sqrt{2\pi}\, dx=\frac{\rho}{\sqrt{2\pi}}\int_{x=-\infty}^{x=\infty} x^2  \mathrm{e}^{-\frac{x^2}{2}}\, dx.
\]</span>
Por último, usando que la varianza de la distribución <span class="math inline">\(Z=N(0,1)\)</span> es 1, tenemos que <span class="math inline">\(\mathrm{Var}(Z)=E\left(Z^2\right)-E(Z)^2\)</span>. Como <span class="math inline">\(E(Z)=0\)</span>, deducimos que <span class="math inline">\(E\left(Z^2\right)=1\)</span>:
<span class="math display">\[
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x^2\mathrm{e}^{-\frac{x^2}{2}}\, dx=1,\ \Rightarrow \int_{-\infty}^\infty x^2\mathrm{e}^{-\frac{x^2}{2}}\, dx=\sqrt{2\pi}.
\]</span>
El valor de <span class="math inline">\(E(X\cdot Y)\)</span> será:
<span class="math display">\[
E(X\cdot Y)=\frac{\rho}{\sqrt{2\pi}}\sqrt{2\pi}=\rho.
\]</span></p>
</div>
<h3 id="ejemplo-normal-bidimensional-2"><span class="header-section-number">2.2.6</span> Ejemplo normal bidimensional</h3>
<div class="example">
<p>La correlación entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> es precisamente <span class="math inline">\(\rho\)</span>.</p>
<p>Ahora, usando que <span class="math inline">\(\mu_X=\mu_Y=0\)</span> y <span class="math inline">\(\sigma_X=\sigma_Y=1\)</span> ya que recordemos que las marginales son <span class="math inline">\(N(0,1)\)</span>, el <strong>coeficiente de correlación</strong> entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> será:
<span class="math display">\[
\rho_{XY}=\frac{E(X\cdot Y)-\mu_X\cdot \mu_Y}{\sqrt{\mathrm{Var}(X)}\cdot\sqrt{\mathrm{Var}(Y)}}=\frac{\rho-0\cdot 0}{1\cdot 1}=\rho.
\]</span>
Por tanto, <span class="math inline">\(\rho\)</span> es el <strong>coeficiente de correlación</strong> entre las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> y mide lo correlacionadas que están dichas variables.</p>
</div>
<h3 id="incorrelación-e-independencia"><span class="header-section-number">2.2.6</span> Incorrelación e independencia</h3>
<p>Hemos visto que si dos variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son <strong>independientes</strong>, entonces son <strong>incorreladas</strong>, o sea, la <strong>covarianza</strong> es 0 (<span class="math inline">\(E(X\cdot Y)=E(X)\cdot E(Y)\)</span>).</p>
<p>El recíproco, sin embargo, es falso. Veamos un ejemplo de variables <strong>incorreladas</strong> que no son independientes.</p>
<div class="example">
<p><strong>Ejemplo de variables aleatorias incorreladas pero no independientes</strong></p>
<p>Consideremos la variable aleatoria bidimensional continua con <strong>función de densidad</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
\frac{3}{8}(x^2+y^2), &amp; \mbox{si }(x,y)\in [-1,1]\times [-1,1],\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
<p>Dejamos como ejercicio comprobar que es una <strong>función de densidad</strong>. O sea, que es positiva y que la integral sobre todo el plano vale 1.</p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-incorreladas-pero-no-independientes"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias incorreladas pero no independientes</h3>
<div class="example">
<p>Calculemos las <strong>densidades marginales</strong>:
<span class="math display">\[
\begin{array}{rl}
f_X(x) &amp; = \int_{-1}^{1} \frac{3}{8}(x^2+y^2)\, dy = \frac{3}{8}\left[x^2 y+\frac{y^3}{3}\right]_{-1}^1 =\frac{3}{8}\left(2 x^2+\frac{2}{3}\right)=\frac{3}{4} x^2+\frac{1}{4}, \\
f_Y(y) &amp; = \int_{-1}^{1} \frac{3}{8}(x^2+y^2)\, dx = \frac{3}{8}\left[\frac{x^3}{3}+y^2 x\right]_{-1}^1 =\frac{3}{8}\left(\frac{2}{3}+2 y^2+\right)=\frac{3}{4} y^2+\frac{1}{4}.
\end{array}
\]</span></p>
<p>Los valores esperados de cada variable <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> serán:
<span class="math display">\[
\begin{array}{rl}
E(X) &amp; =\int_{-1}^1 x \left(\frac{3}{4} x^2+\frac{1}{4}\right)\, dx =0, \mbox{al integrar una función impar,}\\
E(Y) &amp; =\int_{-1}^1 x \left(\frac{3}{4} y^2+\frac{1}{4}\right)\, dx =0, \mbox{al integrar una función impar.}
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-de-variables-aleatorias-incorreladas-pero-no-independientes-1"><span class="header-section-number">2.2.6</span> Ejemplo de variables aleatorias incorreladas pero no independientes</h3>
<div class="example">
<p>El valor de la <strong>correlación</strong> entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> será:
<span class="math display">\[
\begin{array}{rl}
E(X\cdot Y) &amp; =\int_{-1}^1\int_{-1}^1 x y \frac{3}{8}(x^2+y^2)\, dy\, dx\\ &amp; =\frac{3}{8}\left(\int_{-1}^1\int_{-1}^1 x^3 y\, dy \, dx+\int_{-1}^1\int_{-1}^1 x y^3\, dy \, dx\right) \\ &amp; = \frac{3}{8} \left(\int_{x=-1}^{x=1}x^3 \left[\frac{y^2}{2}\right]_{y=-1}^{y=1}\, dx + \int_{y=-1}^{y=1}y^3 \left[\frac{x^2}{2}\right]_{x=-1}^{x=1}\right)=0.
\end{array}
\]</span>
El <strong>coeficiente de correlación</strong> entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> será: <span class="math inline">\(\rho_{XY}=E(X\cdot Y)-E(X)\cdot E(Y)=0-0\cdot 0=0\)</span>. Por tanto, son <strong>incorreladas</strong>.</p>
<p>En cambio no son <strong>independientes</strong> ya que claramente si <span class="math inline">\((x,y)\in [-1,1]\times [-1,1]\)</span>,
<span class="math display">\[
f_{XY}(x,y)=\frac{3}{8}(x^2+y^2) \neq f_X(x)\cdot f_Y(y)=\left(\frac{3}{4} x^2+\frac{1}{4}\right)\cdot \left(\frac{3}{4} y^2+\frac{1}{4}\right).
\]</span></p>
</div>
<h2 id="variables-aleatorias-condicionales-y-valor-esperado-condicional"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales y valor esperado condicional</h2>
<h3 id="introducción-8"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Muchas <strong>variables aleatorias bidimensionales</strong> de interés práctico no son independientes.</p>
<p>Por ejemplo, la salida <span class="math inline">\(Y\)</span> de un canal de comunicación debe depender de la entrada <span class="math inline">\(X\)</span> para transmitir información.</p>
<p>En esta sección vamos a introducir variables aleatorias <span class="math inline">\(Y\)</span> cuya distribución depende de otras <span class="math inline">\(X\)</span>. Dichas variables se denominan <strong>variables aleatorias condicionales</strong>.</p>
<p>También nos interesa el valor esperado de la <strong>variable condicional</strong> <span class="math inline">\(Y\)</span> suponiendo que conocemos <span class="math inline">\(X=x\)</span>.</p>
<h3 id="variables-aleatorias-condicionales-discretas"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales discretas</h3>
<p>Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional. Sea <span class="math inline">\(B\)</span> un subconjunto de los números reales <span class="math inline">\(\mathbb{R}\)</span>. Recordemos que la <strong>probabilidad condicional</strong> del suceso <span class="math inline">\(\{Y\in B\}\)</span> suponiendo que <span class="math inline">\(X=x\)</span> se definía de la forma siguiente:
<span class="math display">\[
P(Y\in B|X=x)=\frac{P(Y\in B,\ X=x)}{P(X=x)}, \mbox{ siempre que }P(X=x)&gt;0.
\]</span></p>
<h3 id="variables-aleatorias-condicionales-discretas-1"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales discretas</h3>
<p>La definición anterior motiva la definición siguiente de <strong>variable aleatoria condicional discreta</strong>:</p>
<p><l class="definition">Definición de variable aleatoria condicional discreta. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional discreta con conjunto de valores <span class="math inline">\((X,Y)(\Omega)=\{(x_i,y_j)\ i=1,2,\ldots, j=1,2,\ldots\}\)</span> y <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}\)</span>. Sean <span class="math inline">\(x_i\)</span> un valor de <span class="math inline">\(X(\Omega)\)</span> con <span class="math inline">\(P(X=x_i)&gt;0\)</span>. Entonces definimos la <strong>función de probabilidad</strong> de la <strong>variable aleatoria condicional discreta</strong> <span class="math inline">\(Y|X=x_i\)</span> como:
<span class="math display">\[
P_{Y|X=x_i}(y_j)=P(Y=y_j|X=x_i)=\frac{P(X=x_i,\ Y=y_j)}{P(X=x_i)}=\frac{P_{XY}(x_i,y_j)}{P_X(x_i)}.
\]</span></p>
<h3 id="variables-aleatorias-condicionales-discretas-2"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales discretas</h3>
<p><l class="observ">Observación. </l>
La <strong>función de probabilidad</strong> de la <strong>variable aleatoria condicional <span class="math inline">\(Y|X=x_i\)</span></strong> depende únicamente de la <strong>función de probabilidad conjunta</strong> de la variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span>.</p>
<l class="observ">Observación. </l>
Al ser <span class="math inline">\(Y|X=x_i\)</span> una variable aleatoria unidimensional, su <strong>función de probabilidad</strong> tiene que verificar que la suma de todos sus valores tiene que dar 1. O sea:
<span class="math display">\[
\sum_{y_j} P(Y=y_j|X=x_i)=1.
\]</span>
Veámoslo:
<div class="dem">
<p><span class="math display">\[
\sum_{y_j} P(Y=y_j|X=x_i)=\sum_{y_j} \frac{P_{XY}(x_i,y_j)}{P_X(x_i)}=\frac{1}{P_X(x_i)}\sum_{y_j} P_{XY}(x_i,y_j) =\frac{1}{P_X(x_i)}\cdot P_X(x_i)=1.
\]</span></p>
</div>
<h3 id="variables-aleatorias-condicionales-discretas-3"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales discretas</h3>
<p><l class="observ">Observación. </l>
Si <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes, <span class="math inline">\(Y|X=x_i =Y\)</span>, o sea, la <strong>variable aleatoria condicional <span class="math inline">\(Y|X=x_i\)</span></strong> coincide con <span class="math inline">\(Y\)</span>. O sea, condicionar con <span class="math inline">\(X=x_i\)</span> no tiene ningún efecto sobre <span class="math inline">\(Y\)</span>.</p>
<div class="dem">
<p>Efectivamente, veamos que <span class="math inline">\(P_{Y|X=x_i}(y_j)=P_Y(y_j)\)</span> para todo valor <span class="math inline">\(y_j\)</span> de <span class="math inline">\(Y(\Omega)\)</span>:
<span class="math display">\[
P_{Y|X=x_i}(y_j) =\frac{P_{XY}(x_i,y_j)}{P_X(x_i)} \stackrel{\mbox{Por ser independientes}}{=}\frac{P_Y(y_j)\cdot P_X(x_i)}{P_X(x_i)}=P_Y(y_j).
\]</span></p>
</div>
<p><l class="observ">Observación. </l>
La definición de la <strong>función de probabilidad</strong> de la <strong>variable aleatoria condicional <span class="math inline">\(X|Y=y_j\)</span></strong> se definiría de forma similar:
<span class="math display">\[
P_{X|Y=y_j}(x_i)=P(X=x_i|Y=y_j)=\frac{P(X=x_i,\ Y=y_j)}{P(Y=y_j)}=\frac{P_{XY}(x_i,y_j)}{P_Y(y_j)}, 
\]</span>
para todo <span class="math inline">\(x_i\in X(\Omega)\)</span>.</p>
<h3 id="variables-aleatorias-condicionales-discretas-4"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales discretas</h3>
<l class="observ">Observación.</l>
Si tenemos la tabla de la <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}\)</span>, para hallar la <strong>función de distribución de la variable <span class="math inline">\(Y|X=x_i\)</span></strong> es equivalente a considerar la fila del valor <span class="math inline">\(x_i\)</span> a la tabla y dividir todos los valores de la fila por la suma de los valores en dicha fila:
<div class="center">
<table>
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(Y|X=x_i\)</span></th>
<th><span class="math inline">\(y_1\)</span></th>
<th><span class="math inline">\(y_2\)</span></th>
<th><span class="math inline">\(\ldots\)</span></th>
<th><span class="math inline">\(y_N\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_{Y|X=x_i}\)</span></td>
<td><span class="math inline">\(\frac{P_{XY}(x_i,y_1)}{P_X(x_i)}\)</span></td>
<td><span class="math inline">\(\frac{P_{XY}(x_i,y_2)}{P_X(x_i)}\)</span></td>
<td><span class="math inline">\(\ldots\)</span></td>
<td><span class="math inline">\(\frac{P_{XY}(x_i,y_N)}{P_X(x_i)}\)</span></td>
</tr>
</tbody>
</table>
</div>
<h3 id="variables-aleatorias-condicionales-discretas-5"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales discretas</h3>
<l class="observ">Observación.</l>
De la misma manera, si tenemos la tabla de la <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}\)</span>, para hallar la <strong>función de distribución de la variable <span class="math inline">\(X|Y=y_j\)</span></strong> es equivalente a considerar la columna del valor <span class="math inline">\(y=y_j\)</span> a la tabla y dividir todos los valores de la columna por la suma de los valores en dicha columna:
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X|Y=y_j\)</span></th>
<th><span class="math inline">\(P_{X|Y=y_j}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x_1\)</span></td>
<td><span class="math inline">\(\frac{P_{XY}(x_1,y_j)}{P_Y(y_j)}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x_M\)</span></td>
<td><span class="math inline">\(\frac{P_{XY}(x_M,y_j)}{P_Y(y_j)}\)</span></td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-64"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado</strong></p>
<p>Vamos a hallar la <strong>variable aleatoria condicional <span class="math inline">\(S|P=12\)</span></strong>.</p>
<p>Tenemos calculada la tabla de la <strong>función de probabilidad conjunta <span class="math inline">\(P_{SP}\)</span></strong>.</p>
<p>Si <span class="math inline">\(P=12\)</span>, los únicos valores <span class="math inline">\(x_i\)</span> de <span class="math inline">\(S(\Omega)\)</span> para los que se verifica <span class="math inline">\(P_{SP}(x_i,12)\neq 0\)</span> son 7 y 8.</p>
<p>Además si calculamos <span class="math inline">\(P_P(12)\)</span>, obtenemos <span class="math inline">\(P(P=12)=\frac{4}{36}\)</span> ya que hay 4 casos en que el producto da 12: <span class="math inline">\((3,4), (4,3), (2,6)\)</span> y <span class="math inline">\((6,2)\)</span>.</p>
</div>
<h3 id="ejemplo-65"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Por tanto, la tabla de la <strong>función de probabilidad condicional</strong> de la variable <span class="math inline">\(S|P=12\)</span> será:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(S|P=12\)</span></th>
<th><span class="math inline">\(P_{S|P=12}\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(7\)</span></td>
<td><span class="math inline">\(\frac{\frac{2}{36}}{\frac{4}{36}}=\frac{1}{2}\)</span></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\(8\)</span></td>
<td><span class="math inline">\(\frac{\frac{2}{36}}{\frac{4}{36}}=\frac{1}{2}\)</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="ejemplo-66"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado</strong></p>
<p>Vamos a hallar la <strong>variable aleatoria condicional <span class="math inline">\(P|S=8\)</span></strong>.</p>
<p>Si <span class="math inline">\(S=8\)</span>, los únicos valores <span class="math inline">\(y_j\)</span> de <span class="math inline">\(P(\Omega)\)</span> para los que se verifica <span class="math inline">\(P_{SP}(8,y_j)\neq 0\)</span> son 12 y 15 y 16.</p>
<p>El valor de <span class="math inline">\(P_S(8)\)</span> recordemos que valía: <span class="math inline">\(P_S(8)=\frac{5}{36}\)</span>.</p>
<p>Por tanto, la tabla de la <strong>función de probabilidad condicional</strong> de la variable <span class="math inline">\(P|S=8\)</span> será:</p>
<div class="center">
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(P|S=8\)</span></th>
<th><span class="math inline">\(12\)</span></th>
<th><span class="math inline">\(15\)</span></th>
<th><span class="math inline">\(16\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_{P|S=8}\)</span></td>
<td><span class="math inline">\(\frac{\frac{2}{36}}{\frac{5}{36}}=\frac{2}{5}\)</span></td>
<td><span class="math inline">\(\frac{\frac{2}{36}}{\frac{5}{36}}=\frac{2}{5}\)</span></td>
<td><span class="math inline">\(\frac{\frac{1}{36}}{\frac{5}{36}}=\frac{1}{5}\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="ejemplo-con-r-6"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Para hallar la <strong>variable aleatoria condicional</strong> <span class="math inline">\(S|P=12\)</span> hemos de condicionar por la columna <span class="math inline">\(P=12\)</span> en la tabla de la <strong>función de probabilidad conjunta</strong>:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1">prob.cond.p12=tabla.func.prob.conjunta[,valores.producto<span class="op">==</span><span class="dv">12</span>]<span class="op">/</span></a>
<a class="sourceLine" id="cb82-2" data-line-number="2"><span class="st">  </span><span class="kw">sum</span>(tabla.func.prob.conjunta[,valores.producto<span class="op">==</span><span class="dv">12</span>])</a>
<a class="sourceLine" id="cb82-3" data-line-number="3">prob.cond.p12</a></code></pre></div>
<pre><code>##   2   3   4   5   6   7   8   9  10  11  12 
## 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0</code></pre>
</div>
<h3 id="ejemplo-con-r-7"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>El problema es que aparecen valores con <strong>función de probabilidad marginal</strong> nulos. Para eliminarlos hacemos lo siguiente:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1">prob.cond.p12.buena =<span class="st"> </span>prob.cond.p12[prob.cond.p12<span class="op">!=</span><span class="dv">0</span>]</a>
<a class="sourceLine" id="cb84-2" data-line-number="2">prob.cond.p12.buena</a></code></pre></div>
<pre><code>##   7   8 
## 0.5 0.5</code></pre>
<p>Para hallar la <strong>función de probabilidad marginal</strong> <span class="math inline">\(P|S=8\)</span>, haríamos lo siguiente:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1">prob.cond.s8=tabla.func.prob.conjunta[valores.suma<span class="op">==</span><span class="dv">8</span>,]<span class="op">/</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2"><span class="st">  </span><span class="kw">sum</span>(tabla.func.prob.conjunta[valores.suma<span class="op">==</span><span class="dv">8</span>,])</a>
<a class="sourceLine" id="cb86-3" data-line-number="3">(<span class="dt">prob.cond.s8.buena =</span> prob.cond.s8[prob.cond.s8<span class="op">!=</span><span class="dv">0</span>])</a></code></pre></div>
<pre><code>##  12  15  16 
## 0.4 0.4 0.2</code></pre>
</div>
<h3 id="variables-aleatorias-condicionales-continuas"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales continuas</h3>
<p>La definición en el caso continua se hace cambiando la <strong>función de probabilidad</strong> por la <strong>función de densidad</strong>:</p>
<p><l class="definition">Definición de variable aleatoria condicional discreta. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional continua con <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span>. Sean <span class="math inline">\(x\in\mathbb{R}\)</span> con <span class="math inline">\(f_X(x)&gt;0\)</span>. Entonces definimos la <strong>función de densidad</strong> de la <strong>variable aleatoria condicional continua</strong> <span class="math inline">\(Y|X=x\)</span> como:
<span class="math display">\[
f_{Y|X=x}(y)=\frac{f_{XY}(x,y)}{f_X(x)}.
\]</span></p>
<h3 id="variables-aleatorias-condicionales-continuas-1"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales continuas</h3>
<p><l class="observ">Observación. </l>
La <strong>función de densidad</strong> de la <strong>variable aleatoria condicional continua<span class="math inline">\(Y|X\)</span></strong> depende únicamente de la <strong>función de densidad conjunta</strong> de la variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span>.</p>
<l class="observ">Observación. </l>
Al ser <span class="math inline">\(Y|X=x\)</span> una variable aleatoria unidimensional, su <strong>función de densidad</strong> tiene que verificar que la integral de dicha función sobre todo <span class="math inline">\(\mathbb{R}\)</span> tiene que ser 1. O sea:
<span class="math display">\[
\int_{-\infty}^\infty f_{Y|X=x}(y)\, dy=1.
\]</span>
Veámoslo:
<div class="dem">
<p><span class="math display">\[
\int_{-\infty}^\infty f_{Y|X=x}(y)\, dy =\int_{-\infty}^\infty \frac{f_{XY}(x,y)}{f_X(x)}\, dy=\frac{1}{f_X(x)}\int_{-\infty}^\infty f_{XY}(x,y)\, dy= \frac{1}{f_X(x)}\cdot f_X(x) =1.
\]</span></p>
</div>
<h3 id="variables-aleatorias-condicionales-continuas-2"><span class="header-section-number">2.2.6</span> Variables aleatorias condicionales continuas</h3>
<p><l class="observ">Observación. </l>
Si <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes, <span class="math inline">\(Y|X=x =Y\)</span>, o sea, la <strong>variable aleatoria condicional <span class="math inline">\(Y|X=x\)</span></strong> coincide con <span class="math inline">\(Y\)</span>. O sea, condicionar con <span class="math inline">\(X=x\)</span> no tiene ningún efecto sobre <span class="math inline">\(Y\)</span>.</p>
<div class="dem">
<p>Efectivamente, veamos que <span class="math inline">\(f_{Y|X=x}(y)=f_Y(y)\)</span> para todo valor <span class="math inline">\(y\in\mathbb{R}.\)</span>
<span class="math display">\[
f_{Y|X=x}(y) =\frac{f_{XY}(x,y)}{f_X(x)} \stackrel{\mbox{Por ser independientes}}{=}\frac{f_Y(y)\cdot f_X(x)}{f_X(x)}=f_Y(y).
\]</span></p>
</div>
<p><l class="observ">Observación. </l>
La definición de la <strong>función de densidad</strong> de la <strong>variable aleatoria condicional <span class="math inline">\(X|Y=y\)</span></strong> se definiría de forma similar:
<span class="math display">\[
f_{X|Y=y}(x)=\frac{f_{XY}(x,y)}{f_Y(y)},
\]</span>
para todo <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<h3 id="ejemplo-67"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos el ejemplo de la variable aleatoria bidimensional continua con <strong>función de densidad</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
2 \mathrm{e}^{-x}\mathrm{e}^{-y}, &amp; 0\leq y\leq x &lt; \infty,\\
0, &amp; \mbox{ en caso contrario,}
\end{cases}
\]</span>
Dado un valor <span class="math inline">\(x_0\geq 0\)</span> cualquiera, vamos a hallar la <strong>función de densidad</strong> de la <strong>variable aleatoria condicional</strong> <span class="math inline">\(Y|X=x_0\)</span>.</p>
<p>Fijémonos que, fijado un valor <span class="math inline">\(x_0\)</span>, los valores <span class="math inline">\(y\)</span> para los cuales <span class="math inline">\(f_{XY}(x_0,y)\neq 0\)</span> cumplen <span class="math inline">\(0\leq y\leq x_0\)</span>.
Por tanto,
<span class="math display">\[
f_{Y|X=x_0}(y)=\frac{f_{XY}(x_0,y)}{f_X(x_0)}=\frac{2\mathrm{e}^{-x_0}\mathrm{e}^{-y}}{f_X(x_0)},
\]</span>
si <span class="math inline">\(0\leq y\leq x_0\)</span>, y <span class="math inline">\(f_{Y|X=x_0}(y)=0\)</span>, en caso contrario.</p>
</div>
<h3 id="ejemplo-68"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Recordemos que la <strong>densidad marginal</strong> de la variable <span class="math inline">\(X\)</span> era: <span class="math inline">\(f_X(x_0)=2\left(\mathrm{e}^{-x_0}-\mathrm{e}^{-2x_0}\right)\)</span>.</p>
<p>La <strong>función de densidad marginal</strong> de la variable <span class="math inline">\(Y|X=x_0\)</span> será:
<span class="math display">\[
f_{Y|X=x_0}(y)=\frac{2\mathrm{e}^{-x_0}\mathrm{e}^{-y}}{2\left(\mathrm{e}^{-x_0}-\mathrm{e}^{-2x_0}\right)}=\frac{e^{-y}}{1-\mathrm{e}^{-x_0}},
\]</span>
si <span class="math inline">\(0\leq y\leq x_0\)</span>, y <span class="math inline">\(f_{Y|X=x_0}(y)=0\)</span>, en caso contrario.</p>
<p>Sea ahora <span class="math inline">\(y_0&gt;0\)</span>. Calculemos ahora la <strong>densidad marginal</strong> de la variable <span class="math inline">\(X|Y=y_0\)</span>.</p>
<p>Fijémonos que, fijado un valor <span class="math inline">\(y_0\)</span>, los valores <span class="math inline">\(x\)</span> para los cuales <span class="math inline">\(f_{XY}(x,y_0)\neq 0\)</span> cumplen <span class="math inline">\(y_0\leq x\leq \infty\)</span>. Por tanto,
<span class="math display">\[
f_{X|Y=y_0}(x)=\frac{f_{XY}(x,y_0)}{f_Y(y_0)}=\frac{2\mathrm{e}^{-x}\mathrm{e}^{-y_0}}{f_Y(y_0)},
\]</span>
si <span class="math inline">\(y_0\leq x\leq \infty\)</span>, y <span class="math inline">\(f_{X|Y=y_0}(x)=0\)</span>, en caso contrario.</p>
</div>
<h3 id="ejemplo-69"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Recordemos que la variable <span class="math inline">\(Y\)</span> era exponencial de parámetro <span class="math inline">\(\lambda=2\)</span>. Por tanto, <span class="math inline">\(f_Y(y_0)=2\mathrm{e}^{-2y_0}\)</span>.</p>
<p>La <strong>función de densidad marginal</strong> de la variable <span class="math inline">\(X|Y=y_0\)</span> será:
<span class="math display">\[
f_{X|Y=y_0}(x)=\frac{2\mathrm{e}^{-x}\mathrm{e}^{-y_0}}{2\mathrm{e}^{-2y_0}}=\frac{\mathrm{e}^{-x}}{\mathrm{e}^{-y_0}},
\]</span>
si <span class="math inline">\(y_0\leq x\leq \infty\)</span>, y <span class="math inline">\(f_{X|Y=y_0}(x)=0\)</span>, en caso contrario.</p>
</div>
<h3 id="ejemplo-de-la-variable-aleatoria-normal-bidimensional"><span class="header-section-number">2.2.6</span> Ejemplo de la variable aleatoria normal bidimensional</h3>
<div class="example">
<p><strong>Ejemplo de la normal bidimensional</strong></p>
<p>Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional normal bidimensional con <strong>densidad conjunta</strong>:
<span class="math display">\[
f_{XY}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}},\ -\infty &lt;x,y&lt;\infty.
\]</span>
Sea <span class="math inline">\(x\in\mathbb{R}\)</span>. Hallemos la <strong>función de densidad</strong> de la <strong>variable aleatoria condicionada</strong> <span class="math inline">\(Y|X=x\)</span>.</p>
<p>Recordemos que las <strong>marginales</strong> eran <span class="math inline">\(N(0,1)\)</span>. Por tanto, <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}.\)</span></p>
</div>
<h3 id="ejemplo-de-la-variable-aleatoria-normal-bidimensional-1"><span class="header-section-number">2.2.6</span> Ejemplo de la variable aleatoria normal bidimensional</h3>
<div class="example">
<p>La <strong>función de densidad</strong> de la variable condicional <span class="math inline">\(Y|X=x\)</span> será:
<span class="math display">\[
f_{Y|X=x}(y)=\frac{f_{XY}(x,y)}{f_X(x)}=\frac{\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}}}{\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}}=\frac{1}{\sqrt{2\pi (1-\rho^2)}}\mathrm{e}^{-\frac{(y-\rho x)^2}{2(1-\rho^2)}},\ y\in\mathbb{R}.
\]</span>
Concluimos que la <strong>variable aleatoria condicional <span class="math inline">\(Y|X=x\)</span></strong> es una normal de parámetros <span class="math inline">\(\mu_{Y|X=x}=\rho x\)</span> y <span class="math inline">\(\sigma_{Y|X=x}^2 =1-\rho^2\)</span>.</p>
<p>Tenemos dos observaciones con respecto al resultado obtenido:</p>
<ul>
<li><p>La varianza de la <strong>variable aleatoria condicional</strong> no depende de la <span class="math inline">\(x\)</span> que se ha fijado. Sólo depende del parámetro <span class="math inline">\(\rho\)</span>. La <span class="math inline">\(x\)</span> sólo influye en la media de dicha variable.</p></li>
<li>En el caso en que <span class="math inline">\(\rho=0\)</span>, que significa que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes, la distribución condicional de <span class="math inline">\(Y|X=x\)</span> es una <span class="math inline">\(N(0,1)\)</span>, distribución que coincide con la distribución de la <strong>variable aleatoria marginal</strong> <span class="math inline">\(Y\)</span>.</li>
</ul>
</div>
<h3 id="valores-esperados-condicionales"><span class="header-section-number">2.2.6</span> Valores esperados condicionales</h3>
<p><l class="definition">Definición de valor esperado condicional.</l>
Dada una variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span>, definimos el <strong>valor esperado de la variable <span class="math inline">\(Y\)</span> dado que <span class="math inline">\(X=x\)</span></strong> como <span class="math inline">\(E(Y|x)\)</span>, o sea, el valor esperado de la <strong>variable aleatoria condicional <span class="math inline">\(Y|X=x\)</span></strong>:
<span class="math display">\[
E(Y|x)=\begin{cases}
\sum_{y_j} y_j P_{Y|X=x}(y_j), &amp; \mbox{ caso discreto,}\\
\int_{-\infty}^\infty y f_{Y|X=x}(y)\,dy, &amp; \mbox{ caso continuo.}
\end{cases}
\]</span></p>
<h3 id="valores-esperados-condicionales-1"><span class="header-section-number">2.2.6</span> Valores esperados condicionales</h3>
<p>Tenemos el siguiente resultado relacionado con los valores esperados: el valor esperado respecto <span class="math inline">\(x\)</span> del valor esperado de la <strong>variable condicional <span class="math inline">\(Y|X=x\)</span></strong> coincide con el valor esperado de la variable <span class="math inline">\(Y\)</span>:</p>
<p><l class="prop">Proposición. </l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional. Sean <span class="math inline">\(E(Y|x)\)</span> el <strong>valor esperado condicional de <span class="math inline">\(Y\)</span></strong> respecto <span class="math inline">\(x\)</span>. Entonces el valor esperado de la <em>variable aleatoria</em> <span class="math inline">\(E(Y|X)\)</span> como función de la variable <span class="math inline">\(X\)</span> es el valor esperado de la variable <span class="math inline">\(Y\)</span>:
<span class="math display">\[
E_X(E(Y|X))=E(Y).
\]</span></p>
<h3 id="valores-esperados-condicionales-2"><span class="header-section-number">2.2.6</span> Valores esperados condicionales</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Haremos la demostración en el caso continuo. Dejamos como ejercicio la demostración para el caso discreto.</p>
<p>Sea <span class="math inline">\(f_{XY}\)</span> la <strong>función de densidad conjunta</strong> y <span class="math inline">\(f_X\)</span> y <span class="math inline">\(f_Y\)</span> las <strong>funciones de densidad marginales</strong>.</p>
<p>El valor de <span class="math inline">\(E_X(E(Y|X))\)</span> será:
<span class="math display">\[
\begin{array}{rl}
E_X(E(Y|X)) &amp; =\int_{x=-\infty}^{x=\infty} E(Y|x)f_X(x)\, dx=\int_{x=-\infty}^{x=\infty}\int_{y=-\infty}^{y=\infty} y f_{Y|X=x}(y)\, dy f_X(x)\, dx \\ &amp; = \int_{x=-\infty}^{x=\infty}\int_{y=-\infty}^{y=\infty} y \frac{f_{XY}(x,y)}{f_X(x)}f_X(x)\, dy\, dx = \int_{y=-\infty}^{y=\infty} y \int_{x=-\infty}^{x=\infty}f_{XY}(x,y)\, dx\, dy \\ &amp;  = \int_{y=-\infty}^{y=\infty} y f_Y(y)\, dy = E(Y),
\end{array}
\]</span>
tal como queríamos ver.</p>
</div>
<h3 id="relación-con-el-problema-de-la-regresión-general"><span class="header-section-number">2.2.6</span> Relación con el problema de la regresión general</h3>
<p>El problema de la <strong>regresión general</strong> es el siguiente:</p>
<p>Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional. Queremos hallar una función <span class="math inline">\(g\)</span> tal que la variable <span class="math inline">\(\hat{Y}=g(X)\)</span> explique mejor la variable <span class="math inline">\(Y\)</span>.</p>
<p>Dicho de forma más explícita, queremos hallar una función <span class="math inline">\(g\)</span> tal que minimice el error cometido al aproximar <span class="math inline">\(Y\)</span> por <span class="math inline">\(\hat{Y}=g(X)\)</span>. Dicho error se definede forma natural como el valor esperado de la variable <span class="math inline">\((Y-g(X))^2\)</span>:
<span class="math display">\[
\min_g E\left((Y-g(X))^2\right).
\]</span></p>
<h3 id="relación-con-el-problema-de-la-regresión-general-1"><span class="header-section-number">2.2.6</span> Relación con el problema de la regresión general</h3>
<p>El siguiente resultado nos dice cuál es la función <span class="math inline">\(g\)</span>:</p>
<p><l class="prop">Proposición: </l>
La función <span class="math inline">\(g\)</span> solución del problema de <strong>regresión general</strong> es la siguiente: <span class="math inline">\(g(x)=E(Y|X=x)\)</span>.</p>
<p>O sea, la función <span class="math inline">\(g\)</span> asigna a cada valor <span class="math inline">\(x\)</span> de la variable aleatoria <span class="math inline">\(X\)</span>, el valor esperado de la <strong>variable condicional</strong> <span class="math inline">\(Y|X=x\)</span>.</p>
<p>En resumen, la función <span class="math inline">\(g(x)=E(Y|X=x)\)</span> es la función que minimiza el error. A la curva <span class="math inline">\(y=g(x)\)</span> se la denomina <strong>curva general de regresión de <span class="math inline">\(Y\)</span> sobre <span class="math inline">\(X\)</span></strong>.</p>
<h3 id="valores-esperados-condicionales.-caso-general"><span class="header-section-number">2.2.6</span> Valores esperados condicionales. Caso general</h3>
<p>Podemos generalizar los valores esperados condicionales en el sentido que en lugar de hallar <span class="math inline">\(E(Y|X=x)\)</span>, hallar <span class="math inline">\(E(g(Y)|X=x)\)</span>, donde <span class="math inline">\(g\)</span> es una función de la variable aleatoria <span class="math inline">\(Y\)</span>:</p>
<p><l class="definition">Definición de valor esperado condicional.</l>
Dada una variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span> y una función <span class="math inline">\(g\)</span>, definimos el <strong>valor esperado de la variable <span class="math inline">\(g(Y)\)</span> dado que <span class="math inline">\(X=x\)</span></strong> como <span class="math inline">\(E(g(Y)|x)\)</span>, o sea, el valor esperado de la <strong>variable aleatoria condicional <span class="math inline">\(g(Y)|X=x\)</span></strong>:
<span class="math display">\[
E(g(Y)|x)=\begin{cases}
\sum_{y_j} g(y_j) P_{Y|X=x}(y_j), &amp; \mbox{ caso discreto,}\\
\int_{-\infty}^\infty g(y) f_{Y|X=x}(y)\,dy, &amp; \mbox{ caso continuo.}
\end{cases}
\]</span></p>
<p><l class="observ">Observación:</l> cuando <span class="math inline">\(g(y)=y^k\)</span>, tenemos definidos los <strong>momentos condicionados de orden <span class="math inline">\(k\)</span></strong> de la variable <span class="math inline">\(Y|X=x\)</span>.</p>
<h3 id="ejemplo-70"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado</strong></p>
<p>Vamos a hallar el valor esperado de la <strong>variable aleatoria condicional <span class="math inline">\(P|S=8\)</span></strong>.</p>
<p>Recordemos su <strong>función de probabilidad</strong>:</p>
<div class="center">
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(P|S=8\)</span></th>
<th><span class="math inline">\(12\)</span></th>
<th><span class="math inline">\(15\)</span></th>
<th><span class="math inline">\(16\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_{P|S=8}\)</span></td>
<td><span class="math inline">\(\frac{\frac{2}{36}}{\frac{5}{36}}=\frac{2}{5}\)</span></td>
<td><span class="math inline">\(\frac{\frac{2}{36}}{\frac{5}{36}}=\frac{2}{5}\)</span></td>
<td><span class="math inline">\(\frac{\frac{1}{36}}{\frac{5}{36}}=\frac{1}{5}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Su valor esperado será, pues:
<span class="math display">\[
E(P|S=8)=12\cdot \frac{2}{5}+15\cdot \frac{2}{5}+16\cdot \frac{1}{5}=\frac{70}{5}=14.
\]</span>
El valor medio del producto de los resultados al lanzar un dado dos veces cuando la suma de dichos resultados es 8 vale 14.</p>
</div>
<h3 id="ejemplo-con-r-8"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>El valor esperado de la variable <span class="math inline">\(E(P|S=8)\)</span> será:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1">valores.cond.s8=<span class="kw">as.integer</span>(<span class="kw">names</span>(prob.cond.s8.buena))</a>
<a class="sourceLine" id="cb88-2" data-line-number="2"><span class="kw">sum</span>(valores.cond.s8<span class="op">*</span>prob.cond.s8.buena)</a></code></pre></div>
<pre><code>## [1] 14</code></pre>
</div>
<h3 id="ejemplo-71"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Recordemos el ejemplo de la variable aleatoria bidimensional continua con <strong>función de densidad</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
2 \mathrm{e}^{-x}\mathrm{e}^{-y}, &amp; 0\leq y\leq x &lt; \infty,\\
0, &amp; \mbox{ en caso contrario,}
\end{cases}
\]</span></p>
<p>Vimos que si fijamos <span class="math inline">\(x_0&gt;0\)</span>, la <strong>función de densidad</strong> de la <strong>variable aleatoria condicionada</strong> <span class="math inline">\(Y|X=x_0\)</span> era:
<span class="math display">\[
f_{Y|X=x_0}(y)=\begin{cases}
\frac{e^{-y}}{1-\mathrm{e}^{-x_0}}, &amp; \mbox{ si }0\leq y\leq x_0, \\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
<p>Hallemos su valor esperado:
<span class="math display">\[
E(Y|X=x_0)=\int_0^{x_0} y \frac{e^{-y}}{(1-\mathrm{e}^{-x_0})}\, dy=\frac{1}{(1-\mathrm{e}^{-x_0})}\left[-\mathrm{e}^{-y} (y+1)\right]_0^{x_0} = \frac{1-\mathrm{e}^{-x_0}(1+x_0)}{1-\mathrm{e}^{-x_0}}.
\]</span></p>
</div>
<h3 id="ejemplo-72"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Verifiquemos la propiedad vista anteriormente <span class="math inline">\(E_X(E(Y|x))=E(Y)\)</span>. Recordemos que la <strong>función de densidad marginal</strong> de la variable <span class="math inline">\(X\)</span> era: <span class="math inline">\(f_X(x)=2\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right)\)</span>, para <span class="math inline">\(x&gt;0\)</span>:
<span class="math display">\[
\begin{array}{rl}
E_X(E(Y|x)) &amp; =\int_0^\infty E(Y|x)\cdot f_X(x)\, dx = \int_0^\infty \frac{1-\mathrm{e}^{-x}(1+x)}{1-\mathrm{e}^{-x}}\cdot 2\left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}\right)\, dx 
\\ &amp; =  2\int_0^\infty \frac{1-\mathrm{e}^{-x}(1+x)}{1-\mathrm{e}^{-x}} \mathrm{e}^{-x}\left(1-\mathrm{e}^{-x}\right)\, dx = 2 \int_0^\infty \left(\mathrm{e}^{-x}-\mathrm{e}^{-2x}(1+x)\right)\, dx \\ &amp; = 2\left[-\mathrm{e}^{-x}+\mathrm{e}^{-2 x}
   \left(\frac{x}{2}+\frac{3}{4}\right)\right]_0^\infty = 2 \left(1-\frac{3}{4}\right)=\frac{1}{2}.
\end{array}
\]</span></p>
<p>Recordemos que la variable <span class="math inline">\(Y\)</span> era exponencial de parámetro <span class="math inline">\(\lambda=2\)</span>. Por tanto <span class="math inline">\(E(Y)=\frac{1}{\lambda}=\frac{1}{2}\)</span>, valor que coincide con el hallado, tal como queríamos ver.</p>
</div>
<h2 id="variables-aleatorias-definidas-como-función-de-dos-variables-aleatorias-conjuntas"><span class="header-section-number">2.2.6</span> Variables aleatorias definidas como función de dos variables aleatorias conjuntas</h2>
<h3 id="introducción-9"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Dado un experimento aleatorio, a veces estaremos interesados en una o más funciones de las variables asociadas con el experimento.</p>
<p>Por ejemplo, si consideramos el experimento aleatorio de lanzar un dado dos veces y definimos la <strong>variable aleatoria bidimensional</strong> <span class="math inline">\((X_1,X_2)\)</span> como la variable que nos da el resultado de cada lanzamiento, podemos expresar la suma y el producto como <span class="math inline">\(S=X_1+X_2\)</span>, <span class="math inline">\(P=X_1\cdot X_2\)</span>.</p>
<p>Otros ejemplos podrían ser considerar el experimento aleatoria de realizar mediciones repetidas de la misma cantidad aleatoria. Entonces, podríamos estar interesados en el valor máximo y mínimo en el conjunto, así como la media muestral y la varianza muestral.</p>
<h3 id="introducción-10"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>En esta sección presentamos métodos para determinar las probabilidades de eventos que involucran <strong>funciones de dos variables aleatorias</strong>.</p>
<p>Daremos métodos de cómo hallar la <strong>función de distribución</strong> y la <strong>función de probabilidad</strong> (caso discreto) o la <strong>función de densidad</strong> (caso continuo) de la variable aleatoria definida como función de la <strong>variable aleatoria bidimensional</strong>.</p>
<h3 id="variable-aleatoria-función-de-la-variable-aleatoria-bidimensional"><span class="header-section-number">2.2.6</span> Variable aleatoria función de la variable aleatoria bidimensional</h3>
<p><l class="prop">Proposición.</l>
Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional con <strong>función de probabilidad</strong> <span class="math inline">\(P_{XY}\)</span> (caso discreto) o <strong>función de densidad</strong> (caso continuo). Sea <span class="math inline">\(g\)</span> una función y definimos la <strong>variable aleatoria unidimensional</strong> <span class="math inline">\(Z\)</span> como <span class="math inline">\(Z=g(X,Y)\)</span>. Entonces la función de distribución de <span class="math inline">\(Z\)</span> será:
<span class="math display">\[
\begin{array}{rl}
F_Z(z) &amp; = P(Z\leq z)=\sum\sum_{(x_i,y_j),\ |\ g(x_i,y_j)\leq z} P_{XY}(x_i,y_j),\ z\in\mathbb{R},\\ &amp;\ \qquad\mbox{ (caso discreto),}\\
F_Z(z) &amp; = P(Z\leq z)=\int\int_{(x,y)\in\mathbb{R}^2,\ |\ g(x,y)\leq z} f_{XY}(x,y)\,dy\, dx, \ z\in\mathbb{R},\\ &amp;\ \qquad\mbox{ (caso continuo).}
\end{array}
\]</span></p>
<h3 id="variable-aleatoria-función-de-la-variable-aleatoria-bidimensional-1"><span class="header-section-number">2.2.6</span> Variable aleatoria función de la variable aleatoria bidimensional</h3>
<p><l class="observ">Observación.</l>
En el caso discreto, la variable aleatoria será discreta con valores <span class="math inline">\(Z(\Omega)=\{z_{ij}=g(x_i,y_j),\ |\ (x_i,y_j)\in (X,Y)(\Omega)\}\)</span>.
Hay que tener en cuenta que en dicho conjunto puede haber repeticiones, o sea, pueden existir dos parejas <span class="math inline">\((i,j)\)</span> y <span class="math inline">\((i&#39;,j&#39;)\)</span> tal que <span class="math inline">\(z_{ij}=z_{i&#39;j&#39;}\)</span>.</p>
<p>La expresión de la <strong>función de probabilidad</strong> en el caso discreto se complica mucho debido a dichas repeticiones y es mejor hallarla en cada caso concreto.</p>
<p>La última observación se puede aplicar también en el caso continuo: la expresión de la <strong>función de densidad</strong> se halla en cada caso concreto.</p>
<h3 id="ejemplo-variables-aleatorias-discretas"><span class="header-section-number">2.2.6</span> Ejemplo variables aleatorias discretas</h3>
<div class="example">
<p><strong>Ejemplo del lanzamiento de un dado dos veces.</strong></p>
<p>Consideremos el experimento aleatorio de lanzar dos veces un dado.</p>
<p>Sea <span class="math inline">\((X,Y)\)</span> la <strong>variable aleatoria</strong> bidimensional discreta ya estudiada anteriormente donde <span class="math inline">\(X\)</span> nos da el resultado del primer lanzamiento e <span class="math inline">\(Y\)</span>, el resultado del segundo lanzamiento.</p>
<p>Vimos que <span class="math inline">\((X,Y)(\Omega)=\{(i,j),\ i=1,2,3,4,5,6,\ j=1,2,3,4,5,6\}\)</span> con <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{XY}(i,j)=\frac{1}{36}\)</span>, <span class="math inline">\(i=1,2,3,4,5,6,\ j=1,2,3,4,5,6.\)</span></p>
<p>Anteriormente hemos estudiado la suma <span class="math inline">\(S\)</span> de los resultados. En este caso podemos interpretar <span class="math inline">\(S=g(X,Y)\)</span> donde <span class="math inline">\(g(x,y)=x+y\)</span>.</p>
<p>Como la función <span class="math inline">\(S\)</span> ya ha sido estudiada y el producto se ha dejado como ejercicio, estudiaremos la siguiente variable aleatoria función de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>: <span class="math inline">\(Z=X^2+Y^2\)</span>.</p>
<p>Realizaremos los cálculos con ayuda de <code>R</code> ya que hacerlos a mano es bastante tedioso.</p>
<p>Los valores de <span class="math inline">\(Z(\Omega)\)</span> serán: <span class="math inline">\(Z(\Omega)=\{z_{ij}=i^2+j^2,\ i=1,2,3,4,5,6,\ j=1,2,3,4,5,6\}\)</span>. Observad que hay parejas <span class="math inline">\((i,j)\)</span> que dan lugar a los mismos valores, por ejemplo <span class="math inline">\(1^2+2^2 = 2^2+1^2\)</span>, y, en general, si <span class="math inline">\(i\neq j\)</span>, <span class="math inline">\(z_{ij}=i^2+j^2=z_{ji}=j^2+i^2\)</span>.</p>
</div>
<h3 id="ejemplo-variables-aleatorias-discretas-1"><span class="header-section-number">2.2.6</span> Ejemplo variables aleatorias discretas</h3>
<div class="example">
<p>Para hallar el conjunto <span class="math inline">\(Z(\Omega)\)</span> usamos la función <code>outer</code> de <code>R</code>:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1">g=<span class="cf">function</span>(x,y){x<span class="op">^</span><span class="dv">2</span><span class="op">+</span>y<span class="op">^</span><span class="dv">2</span>}  <span class="co">## definimos la función g</span></a>
<a class="sourceLine" id="cb90-2" data-line-number="2"><span class="kw">sort</span>(<span class="kw">unique</span>(<span class="kw">as.vector</span>(<span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,g))))</a></code></pre></div>
<pre><code>##  [1]  2  5  8 10 13 17 18 20 25 26 29 32 34 37 40 41 45 50 52 61 72</code></pre>
<p>Vemos que hay 21 valores distintos de la variable <span class="math inline">\(Z\)</span>.</p>
<p>Para hallar la <strong>función de probabilidad</strong> de <span class="math inline">\(Z\)</span> hemos de calcular para cada valor <span class="math inline">\(z_k\)</span>, las parejas <span class="math inline">\((i,j)\)</span> tal que <span class="math inline">\(i^2+j^2=z_k\)</span>:</p>
</div>
<h3 id="ejemplo-variables-aleatorias-discretas-2"><span class="header-section-number">2.2.6</span> Ejemplo variables aleatorias discretas</h3>
<div class="example">
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1">valores.variable.Z =<span class="st"> </span><span class="kw">sort</span>(<span class="kw">unique</span>(<span class="kw">as.vector</span>(<span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,g))))  </a>
<a class="sourceLine" id="cb92-2" data-line-number="2">matriz.valores =<span class="st"> </span><span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,g) <span class="co">## aplicamos la función g a </span></a>
<a class="sourceLine" id="cb92-3" data-line-number="3"><span class="co">##  todas las parejas (i,j), i,j=1,2,3,4,5,6</span></a>
<a class="sourceLine" id="cb92-4" data-line-number="4">frecuencias =<span class="st"> </span><span class="kw">c</span>()  <span class="co">## vector donde guardaremos las frecuencias de los valores de Z</span></a>
<a class="sourceLine" id="cb92-5" data-line-number="5"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(valores.variable.Z)){</a>
<a class="sourceLine" id="cb92-6" data-line-number="6">  z=valores.variable.Z[i]</a>
<a class="sourceLine" id="cb92-7" data-line-number="7">  frecuencias=<span class="kw">c</span>(frecuencias,<span class="kw">length</span>(matriz.valores[matriz.valores<span class="op">==</span>z]))</a>
<a class="sourceLine" id="cb92-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb92-9" data-line-number="9">frecuencias</a></code></pre></div>
<pre><code>##  [1] 1 2 1 2 2 2 1 2 2 2 2 1 2 2 2 2 2 1 2 2 1</code></pre>
</div>
<h3 id="ejemplo-variables-aleatorias-discretas-3"><span class="header-section-number">2.2.6</span> Ejemplo variables aleatorias discretas</h3>
<div class="example">
<p>La <strong>función de probabilidad</strong> de <span class="math inline">\(Z\)</span> será:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1">función.probabilidad.Z=<span class="kw">data.frame</span>(<span class="kw">rbind</span>(valores.variable.Z,<span class="kw">round</span>(frecuencias<span class="op">/</span><span class="dv">36</span>,<span class="dv">3</span>)))</a>
<a class="sourceLine" id="cb94-2" data-line-number="2"><span class="kw">rownames</span>(función.probabilidad.Z)=<span class="kw">c</span>(<span class="st">&quot;Z&quot;</span>,<span class="st">&quot;P_Z&quot;</span>)</a>
<a class="sourceLine" id="cb94-3" data-line-number="3">función.probabilidad.Z</a></code></pre></div>
<pre><code>##        X1    X2    X3     X4     X5     X6     X7     X8     X9    X10    X11
## Z   2.000 5.000 8.000 10.000 13.000 17.000 18.000 20.000 25.000 26.000 29.000
## P_Z 0.028 0.056 0.028  0.056  0.056  0.056  0.028  0.056  0.056  0.056  0.056
##        X12    X13    X14    X15    X16    X17    X18    X19    X20    X21
## Z   32.000 34.000 37.000 40.000 41.000 45.000 50.000 52.000 61.000 72.000
## P_Z  0.028  0.056  0.056  0.056  0.056  0.056  0.028  0.056  0.056  0.028</code></pre>
</div>
<h3 id="ejemplo-variables-aleatorias-continuas"><span class="header-section-number">2.2.6</span> Ejemplo variables aleatorias continuas</h3>
<div class="example">
<p>Recordemos la variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span> con <strong>función de densidad</strong>:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
2 \mathrm{e}^{-x}\mathrm{e}^{-y}, &amp; 0\leq y\leq x &lt; \infty,\\
0, &amp; \mbox{ en caso contrario,}
\end{cases}
\]</span>
Consideremos la variable aleatoria <span class="math inline">\(Z=X+Y\)</span>. Vamos a calcular la <strong>función de densidad</strong> de <span class="math inline">\(Z\)</span>.</p>
<p>En primer lugar, los valores de <span class="math inline">\(Z\)</span> para los que <span class="math inline">\(f_Z(z)\neq 0\)</span> cumplen <span class="math inline">\(z\geq 0\)</span> ya que <span class="math inline">\(X\geq 0\)</span> e <span class="math inline">\(Y\geq 0\)</span>.</p>
<p>Calculemos la <strong>función de distribución</strong> de la variable <span class="math inline">\(Z\)</span>. Sean <span class="math inline">\(z\in\mathbb{R}\)</span> con <span class="math inline">\(z\geq 0\)</span>:
<span class="math display">\[
F_Z(z)=P(Z\leq z)=P(X+Y\leq z)=\int\int_{\{(x,y)\mathbb{R}^2,\ |\ x+y\leq z\}\cap \{(x,y)\in \mathbb{R}^2,\ |\ 0\leq y\leq x&lt;\infty\}} 2 \mathrm{e}^{-x}\mathrm{e}^{-y}\, dy\, dx
\]</span>
El gráfico siguiente muestra en color violeta la región de integración para hallar <span class="math inline">\(F_Z(z)\)</span> dado un <span class="math inline">\(z\geq 0\)</span>.</p>
</div>
<h3 id="ejemplo-variables-aleatorias-continuas-1"><span class="header-section-number">2.2.6</span> Ejemplo variables aleatorias continuas</h3>
<div class="center">
<p><img src="Images/EjSumaXY.png" width="750px" /></p>
</div>
<h3 id="ejemplo-variables-aleatorias-continuas-2"><span class="header-section-number">2.2.6</span> Ejemplo variables aleatorias continuas</h3>
<div class="example">
<p>El valor de <span class="math inline">\(F_Z(z)\)</span> será: (fijémonos que primero fijamos la <span class="math inline">\(y\)</span> y para cada <span class="math inline">\(y\)</span> la <span class="math inline">\(x\)</span> va desde la recta <span class="math inline">\(x=y\)</span> hasta la recta <span class="math inline">\(x=z-y\)</span>)
<span class="math display">\[
\begin{array}{rl}
F_Z(z) &amp; =\int_{y=0}^{y=\frac{z}{2}}\int_{x=y}^{x=z-y}2 \mathrm{e}^{-x}\mathrm{e}^{-y}\, dx\, dy = 2 \int_{y=0}^{y=\frac{z}{2}} \mathrm{e}^{-y} \left[-\mathrm{e}^{-x}\right]_{x=y}^{x=z-y}\, dy \\ &amp; = 2 \int_{y=0}^{y=\frac{z}{2}} \mathrm{e}^{-y} \left(\mathrm{e}^{-y}-\mathrm{e}^{y-z}\right)\, dy = 2 \int_{y=0}^{y=\frac{z}{2}} \left(\mathrm{e}^{-2y}-\mathrm{e}^{-z} \right)\, dy  = 2\left[-\frac{1}{2}\mathrm{e}^{-2y}-\mathrm{e}^{-z} y\right]_{y=0}^{y=\frac{z}{2}} \\ &amp; = 2\left(\frac{1}{2}-\frac{1}{2}\mathrm{e}^{-z}-\frac{z}{2}\mathrm{e}^{-z}\right) = 1-\mathrm{e}^{-z}(1+z),\ z\geq 0.
\end{array}
\]</span>
La <strong>función de densidad</strong> de <span class="math inline">\(Z\)</span> será:
<span class="math display">\[
f_Z(z)=F&#39;_Z(z)=z \mathrm{e}^{-z},\ z\geq 0,
\]</span>
y <span class="math inline">\(f_Z(z)=0\)</span> en caso contrario.</p>
</div>
<h3 id="ejemplo-de-la-suma-de-dos-normales"><span class="header-section-number">2.2.6</span> Ejemplo de la suma de dos normales</h3>
<div class="example">
<p>Consideremos el caso en que la variable aleatoria <span class="math inline">\((X,Y)\)</span> tenga distribución <strong>normal bidimensional</strong>.</p>
<p>Recordemos que su <strong>función de densidad conjunta</strong> era:
<span class="math display">\[
f_{XY}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}},\ -\infty &lt;x,y&lt;\infty.
\]</span>
Consideremos <span class="math inline">\(S=X+Y\)</span>. Estudiemos qué distribución tiene <span class="math inline">\(S\)</span>.</p>
<p>Dado un valor <span class="math inline">\(z\in\mathbb{R}\)</span>, la <strong>función de distribución</strong> de <span class="math inline">\(S\)</span> en <span class="math inline">\(s\)</span> será:
<span class="math display">\[
\begin{array}{rl}
F_S(s) &amp; =P(S\leq s)=\int\int_{\{(x,y)\in\mathbb{R}^2,\ |\ x+y\leq s\}}\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}}\, dy\, dx \\ &amp; =
\frac{1}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty}\int_{y=-\infty}^{y=s-x}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}}\, dy\, dx = \frac{1}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{x^2}{2(1-\rho^2)}} \int_{y=-\infty}^{y=s-x}\mathrm{e}^{-\frac{(-2\rho xy+y^2)}{2(1-\rho^2)}}\, dy\, dx  \\ &amp;\ \qquad\mbox{hacemos el cambio siguiente en la segunda integral $t=y+x$}\\ &amp; = \frac{1}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{x^2}{2(1-\rho^2)}} \int_{t=-\infty}^{t=s}\mathrm{e}^{-\frac{(-2\rho x(t-x)+(t-x)^2)}{2(1-\rho^2)}}\, dt\, dx 
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-de-la-suma-de-dos-normales-1"><span class="header-section-number">2.2.6</span> Ejemplo de la suma de dos normales</h3>
<div class="example">
<p><span class="math display">\[
\begin{array}{rl}
F_S(s) &amp; =  \frac{1}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{(1+\rho)x^2}{1-\rho^2}}\int_{t=-\infty}^{t=s} \mathrm{e}^{-\frac{(t^2-2(1+\rho) t x)}{2(1-\rho^2)}}\, dt\, dx \\ &amp; = \frac{1}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{(1+\rho)x^2}{1-\rho^2}}\int_{t=-\infty}^{t=s} \mathrm{e}^{-\frac{(t-(1+\rho)x)^2}{2(1-\rho^2)}} \mathrm{e}^{\frac{(\rho+1)^2 x^2}{2(1-\rho^2)}}\, dt\, dx  \\ &amp; = \frac{1}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{x^2}{2}}\cdot \sqrt{2\pi (1-\rho^2)} F_X(s)\, dx, \\ &amp; \mbox{ donde $F_X(s)$ es la función de distribución de una variable $X$ normal de parámetros} \\ &amp; \mbox{ $\mu =(1+\rho)x$ y $\sigma^2=1-\rho^2$.} \\ &amp; = \frac{1}{\sqrt{2\pi}}\int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{x^2}{2}}\cdot F_X(s)\, dx.
\end{array}
\]</span>
Para calcular la <strong>función de densidad</strong> <span class="math inline">\(f_S(s)\)</span> aplicamos la expresión <span class="math inline">\(f_S(s)=F&#39;_S(s)\)</span> y la derivación bajo el signo integral:
<span class="math display">\[
\begin{array}{rl}
f_S(s) &amp; = \frac{1}{\sqrt{2\pi}}\int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{x^2}{2}}\cdot f_X(s)\, dx = \frac{1}{\sqrt{2\pi}}\int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{x^2}{2}}\cdot \frac{1}{\sqrt{2\pi (1-\rho^2)}}\mathrm{e}^{-\frac{(s-(1+\rho)x)^2}{2(1-\rho^2)}}\, dx \\ &amp; = \frac{\mathrm{e}^{-\frac{s^2}{2(1-\rho^2)}}}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{(2(1+\rho) x^2-2(1+\rho)xs)}{2(1-\rho^2)}}\, dx= \frac{\mathrm{e}^{-\frac{s^2}{2(1-\rho^2)}}}{2\pi\sqrt{1-\rho^2}} \int_{x=-\infty}^{x=\infty} \mathrm{e}^{-\frac{\left(x-\frac{s}{2}\right)^2}{1-\rho}}\mathrm{e}^{\frac{s^2}{4(1-\rho)}}\, dx
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-de-la-suma-de-dos-normales-2"><span class="header-section-number">2.2.6</span> Ejemplo de la suma de dos normales</h3>
<div class="example">
<p>En la última integral hacemos el cambio <span class="math inline">\(u=x-\frac{z}{2}\)</span>:
<span class="math display">\[
f_S(s)  =\frac{\mathrm{e}^{-\frac{s^2}{4(1+\rho)}}}{2\pi\sqrt{1-\rho^2}} \int_{u=-\infty}^{u=\infty} \mathrm{e}^{-\frac{u^2}{1-\rho}}\, du.
\]</span>
A continuación usando que <span class="math inline">\(f_Z(z)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\)</span> es la función de densidad de la distribución <span class="math inline">\(Z=N(0,1)\)</span>, podemos escribir: <span class="math inline">\(\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}=1,\ \Rightarrow \int_{-\infty}^\infty \mathrm{e}^{-\frac{x^2}{2}}=\sqrt{2\pi}.\)</span></p>
<p>Si en la última integral hacemos el cambio <span class="math inline">\(v=\sqrt{\frac{2}{1-\rho}}u\)</span>, obtenemos:
<span class="math display">\[
\begin{array}{rl}
f_S(s)  &amp; = \frac{\mathrm{e}^{-\frac{s^2}{4(1+\rho)}}}{2\pi\sqrt{1-\rho^2}}\int_{v=-\infty}^{v=\infty}\mathrm{e}^{-\frac{v^2}{2}} \sqrt{\frac{1-\rho}{2}}\, dv= \frac{\mathrm{e}^{-\frac{s^2}{4(1+\rho)}}}{2\pi\sqrt{2(1+\rho)}}\int_{v=-\infty}^{v=\infty}\mathrm{e}^{-\frac{v^2}{2}} \, dv \\ &amp; = \frac{\mathrm{e}^{-\frac{s^2}{4(1+\rho)}}}{2\pi\sqrt{2(1+\rho)}} \sqrt{2\pi}= \frac{1}{\sqrt{2\pi 2(1+\rho)}}\mathrm{e}^{-\frac{s^2}{4(1+\rho)}},\ s\in\mathbb{R}.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-de-la-suma-de-dos-normales-3"><span class="header-section-number">2.2.6</span> Ejemplo de la suma de dos normales</h3>
<div class="example">
<p>Dicha función de densidad corresponde a una distribución normal de parámetros <span class="math inline">\(\mu =0\)</span> y <span class="math inline">\(\sigma = \sqrt{2(1+\rho)}\)</span>.</p>
<p>En resumen, la distribución de la suma de dos normales es una normal de parámetros <span class="math inline">\(S=N(\mu=0,\sigma = \sqrt{2(1+\rho)})\)</span>.</p>
</div>
<h3 id="transformaciones-lineales-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Transformaciones lineales de variables aleatorias</h3>
<p>Consideremos una variable aleatoria bidimensional continua <span class="math inline">\((X,Y)\)</span> con <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span>.</p>
<p>Definimos la variable aleatoria bidimensional continua <span class="math inline">\((U,V)\)</span> a partir de una transformación lineal de la variable <span class="math inline">\((X,Y)\)</span>. O sea, existe una matriz <span class="math inline">\(\mathbf{M}=\begin{pmatrix}a &amp; b\\ c&amp; d\end{pmatrix}\)</span> y un vector <span class="math inline">\(\mathbf{n}=\begin{pmatrix}\alpha\\\beta \end{pmatrix}\)</span> tal que:
<span class="math display">\[
\begin{array}{rl}
\begin{pmatrix}U\\ V\end{pmatrix} &amp; =\mathbf{M}\cdot \begin{pmatrix}X\\ Y\end{pmatrix}+\mathbf{n}=\begin{pmatrix}a &amp; b\\ c&amp; d\end{pmatrix}\cdot\begin{pmatrix}X\\ Y\end{pmatrix}+\begin{pmatrix}\alpha\\\beta \end{pmatrix},\\  &amp; \Rightarrow \left.\begin{array}{rl}U &amp; = aX+bY+\alpha,\\ V &amp; =cX+dY+\beta.\end{array}\right\}
\end{array}
\]</span></p>
<h3 id="transformaciones-lineales-de-variables-aleatorias-1"><span class="header-section-number">2.2.6</span> Transformaciones lineales de variables aleatorias</h3>
<p>Para que <span class="math inline">\((U,V)\)</span> sea una variable aleatoria bidimensional, necesitamos que la matriz <span class="math inline">\(\mathbf{M}\)</span> sea no singular, o <span class="math inline">\(\mathrm{det}(\mathbf{M})\neq 0\)</span>.</p>
<p>Nos preguntamos cuál es la relación entre la <strong>función de densidad</strong> de la variable <span class="math inline">\((U,V)\)</span>, <span class="math inline">\(f_{UV}\)</span> y la <strong>función de densidad</strong> de la variable <span class="math inline">\((X,Y)\)</span>, <span class="math inline">\(f_{XY}\)</span>. La expresión siguiente nos da dicha relación:
<span class="math display">\[
f_{UV}(u,v)=\frac{1}{|\mathrm{det}(\mathbf{M})|}f_{XY}\left(\mathbf{M}^{-1}\begin{pmatrix}u-\alpha\\ v-\beta\end{pmatrix}\right), \ (u,v)\in\mathbb{R}^2.
\]</span></p>
<h3 id="transformaciones-lineales-de-variables-aleatorias-2"><span class="header-section-number">2.2.6</span> Transformaciones lineales de variables aleatorias</h3>
<p><l class="observ">Observación. </l>
Si la variable <span class="math inline">\((X,Y)\)</span> tiene una región <span class="math inline">\(D\)</span> donde <span class="math inline">\(f_{XY}(x,y)\neq 0\)</span>, para todo <span class="math inline">\((x,y)\in D\)</span>, antes de aplicar la expresión anterior para hallar la <strong>función de densidad</strong> de la variable <span class="math inline">\((U,V)\)</span> hemos de calcular cómo se transforma <span class="math inline">\(D\)</span> con la matriz <span class="math inline">\(\mathbf{M}\)</span>. O sea, hay que hallar la región
<span class="math display">\[
D&#39;=\mathbf{M}(D)=\{(u,v)\in\mathbb{R}^2,\ \mbox{existe $(x,y)\in D$ con } (u,v)=\mathbf{M}(x,y)+\mathbf{n}\}.
\]</span></p>
<h3 id="ejemplo-73"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos la variable <span class="math inline">\((X,Y)\)</span> continua con función de densidad:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
\frac{1}{2}(1+x+y), &amp; \mbox{ si }(x,y)\in R, \\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
donde <span class="math inline">\(R\)</span> es el rombo de vértices <span class="math inline">\((1,0)\)</span>, <span class="math inline">\((0,1)\)</span>, <span class="math inline">\((-1,0)\)</span> y <span class="math inline">\((0,-1)\)</span>, ver figura adjunta.</p>
<p>Otra forma de definir la función anterior sería:
<span class="math display">\[
f_{XY}(x,y)=\begin{cases}
\frac{1}{2}(1+x+y), &amp; -1\leq x\leq 0,\ -1-x\leq y\leq x+1, \\
\frac{1}{2}(1+x+y), &amp; 0\leq x\leq 0,\ x-1\leq y\leq 1-x, \\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
Dejamos como ejercicio al lector comprobar que la función anterior es una <strong>función de densidad</strong>.</p>
</div>
<h3 id="ejemplo-74"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/EjTranLineal.png" width="750px" /></p>
</div>
<h3 id="ejemplo-75"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Consideramos la variable aleatoria bidimensional <span class="math inline">\((U,V)\)</span> definida a partir de la variable <span class="math inline">\((X,Y)\)</span>:
<span class="math display">\[
\begin{pmatrix}U\\ V\end{pmatrix}=\begin{pmatrix}1 &amp; -1\\ 1&amp; 1\end{pmatrix}\cdot\begin{pmatrix}X\\ Y\end{pmatrix},\ \Rightarrow \left.\begin{array}{rl}U &amp; = X-Y,\\ V &amp; =X+Y.\end{array}\right\}
\]</span>
La región <span class="math inline">\(R\)</span> se transforma en el cuadrado <span class="math inline">\(C\)</span> de vértices <span class="math inline">\((1,1)\)</span>, <span class="math inline">\((-1,1)\)</span>, <span class="math inline">\((-1,-1)\)</span> y <span class="math inline">\((1,-1)\)</span> ya que si aplicamos la matriz a los vértices del rombo, obtenemos los vértices de cuadrado:
<span class="math display">\[
\begin{array}{rl}
\begin{pmatrix}1 &amp; -1\\ 1&amp; 1\end{pmatrix}\cdot \begin{pmatrix}1\\ 0\end{pmatrix} &amp; =\begin{pmatrix}1\\ 1\end{pmatrix},\qquad 
\begin{pmatrix}1 &amp; -1\\ 1&amp; 1\end{pmatrix}\cdot \begin{pmatrix}0\\ 1\end{pmatrix}=\begin{pmatrix}-1\\ 1\end{pmatrix},\\ 
\begin{pmatrix}1 &amp; -1\\ 1&amp; 1\end{pmatrix}\cdot \begin{pmatrix}-1\\ 0\end{pmatrix} &amp; =\begin{pmatrix}-1\\ -1\end{pmatrix},\qquad 
\begin{pmatrix}1 &amp; -1\\ 1&amp; 1\end{pmatrix}\cdot \begin{pmatrix}0\\ -1\end{pmatrix}=\begin{pmatrix}1\\ -1\end{pmatrix}.
\end{array}
\]</span>
Ver la figura adjunta.</p>
<p>Para hallar la <strong>función de densidad</strong> <span class="math inline">\(f_{UV}\)</span> necesitamos escribir <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> en función de <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span>:
<span class="math display">\[
\begin{pmatrix}X\\ Y\end{pmatrix}=\begin{pmatrix}1 &amp; -1\\ 1&amp; 1\end{pmatrix}^{-1}\cdot\begin{pmatrix}U\\ V\end{pmatrix}=\begin{pmatrix}\frac{1}{2} &amp; \frac{1}{2}\\ -\frac{1}{2}&amp; \frac{1}{2}\end{pmatrix}\cdot\begin{pmatrix}U\\ V\end{pmatrix},\ \Rightarrow \left.\begin{array}{rl}X &amp; = \frac{1}{2}(U+V),\\ Y &amp; =\frac{1}{2}(-U+V).\end{array}\right\}
\]</span></p>
</div>
<h3 id="ejemplo-76"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/EjTranLineal2.png" width="750px" /></p>
</div>
<h3 id="ejemplo-77"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>La <strong>función de densidad</strong> <span class="math inline">\(f_{UV}\)</span> será, por tanto,
<span class="math display">\[
\begin{array}{rl}
f_{UV}(u,v) &amp; =\frac{1}{\left|\mathrm{det}\begin{pmatrix}1 &amp; -1\\ 1&amp; 1\end{pmatrix}\right|}\cdot f_{XY}\left(\frac{1}{2}(u+v),\frac{1}{2}(-u+v)\right) \\ &amp; =\frac{1}{2}\cdot \frac{1}{2}\cdot \left(1+\frac{1}{2}(-u+v)+\frac{1}{2}(u+v)\right)=\frac{1}{4}(1+v),
\end{array}
\]</span>
para <span class="math inline">\((u,v)\)</span> perteneciente al cuadrado <span class="math inline">\(C\)</span> de vértices <span class="math inline">\((1,1)\)</span>, <span class="math inline">\((-1,1)\)</span>, <span class="math inline">\((-1,-1)\)</span> y <span class="math inline">\((1,-1)\)</span>, o si se quiere para <span class="math inline">\(-1\leq u\leq 1\)</span>, <span class="math inline">\(-1\leq v\leq 1\)</span>, y <span class="math inline">\(f_{UV}(u,v)=0\)</span>, en caso contrario.</p>
<p>Observamos que es más cómodo trabajar con las variables <span class="math inline">\((u,v)\)</span> en vez de trabajar con las variables <span class="math inline">\((x,y)\)</span> por dos razones:</p>
<ul>
<li>La región donde la <strong>función de densidad</strong> no es nula es más simple, ya que trabajar con un cuadrado simplifica mucho más los cálculos que trabajar con un rombo a la hora de hallar la <strong>función de distribución</strong>, <strong>densidades marginales</strong>, <strong>densidades condicionadas</strong>, <strong>valores esperados</strong>, etc.</li>
<li>La expresión de la <strong>función de densidad</strong> también es más simple, ya que sólo depende de la segunda variable <span class="math inline">\(v\)</span>; sin embargo, la <strong>función de densidad</strong> inicial <span class="math inline">\(f_{XY}\)</span> dependía de las dos variables <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>.</li>
</ul>
</div>
<h3 id="ejemplo-de-la-normal-bidimensional"><span class="header-section-number">2.2.6</span> Ejemplo de la normal bidimensional</h3>
<div class="example">
<p>Consideremos el caso en que la variable aleatoria <span class="math inline">\((X,Y)\)</span> tenga distribución <strong>normal bidimensional</strong>.</p>
<p>Recordemos que su <strong>función de densidad conjunta</strong> era:
<span class="math display">\[
f_{XY}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{(x^2-2\rho xy+y^2)}{2(1-\rho^2)}},\ -\infty &lt;x,y&lt;\infty.
\]</span>
Recordemos que las <strong>distribuciones marginales</strong> eran distribuciones <span class="math inline">\(N(0,1)\)</span>.</p>
<p>La idea es hallar la <strong>función de densidad conjunta</strong> de una distribución normal bidimensional para la que sus <strong>distribuciones marginales</strong> sean dos normales <span class="math inline">\(N(\mu_1,\sigma_1)\)</span> y <span class="math inline">\(N(\mu_2,\sigma_2)\)</span>.</p>
<p>Recordemos que si <span class="math inline">\(Z=N(0,1)\)</span>, entonces <span class="math inline">\(\sigma_1\cdot Z+\mu_1 =N(\mu_1,\sigma_1)\)</span>. Este hecho, motiva que consideremos el cambio lineal siguiente a las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>:
<span class="math display">\[
\begin{pmatrix}U\\ V\end{pmatrix}=\begin{pmatrix}\sigma_1 &amp; 0\\ 0&amp; \sigma_2\end{pmatrix}\cdot\begin{pmatrix}X\\ Y\end{pmatrix}+\begin{pmatrix}\mu_1\\\mu_2\end{pmatrix},\ \Rightarrow \left.\begin{array}{rl}U &amp; = \sigma_1\cdot X+\mu_1,\\ V &amp; =\sigma_2\cdot Y+\mu_2.\end{array}\right\}
\]</span></p>
</div>
<h3 id="ejemplo-de-la-normal-bidimensional-1"><span class="header-section-number">2.2.6</span> Ejemplo de la normal bidimensional</h3>
<div class="example">
<p>La función de densidad conjunta <span class="math inline">\(f_{UV}\)</span> será:
<span class="math display">\[
\begin{array}{rl}
f_{UV}(u,v) &amp; = \frac{1}{\left|\begin{pmatrix}\sigma_1 &amp; 0\\ 0&amp; \sigma_2\end{pmatrix}\right|} f_{XY}\left(\frac{u-\mu_1}{\sigma_1},\frac{v-\mu_2}{\sigma_2}\right)
=\frac{1}{\sigma_1\cdot \sigma_2}f_{XY}\left(\frac{u-\mu_1}{\sigma_1},\frac{v-\mu_2}{\sigma_2}\right)\\ &amp; =
\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\mathrm{e}^{-\frac{\left(\left(\frac{u-\mu_1}{\sigma_1}\right)^2-2\rho \left(\frac{u-\mu_1}{\sigma_1}\right)\left(\frac{v-\mu_2}{\sigma_2}\right)+\left(\frac{v-\mu_2}{\sigma_2}\right)^2\right)}{2(1-\rho^2)}},
\end{array}
\]</span>
para <span class="math inline">\((u,v)\in\mathbb{R}^2\)</span>.</p>
<p>Si llamamos <span class="math inline">\(\mathbf{\Sigma}\)</span> a la matriz <span class="math inline">\(\mathbf{\Sigma}=\begin{pmatrix}\sigma_1^2 &amp; \rho\sigma_1\sigma_2\\ \rho\sigma_1\sigma_2 &amp; \sigma_2^2\end{pmatrix}\)</span>, llamada <strong>matriz de covarianzas</strong> de la distribución normal <span class="math inline">\((U,V)\)</span> la <strong>función de densidad</strong> anterior puede escribirse como:
<span class="math display">\[
f_{UV}(u,v)=\frac{1}{2\pi \sqrt{\left|\mathrm{\Sigma}\right|}}\mathrm{e}^{-\frac{1}{2}(\mathbf{u}-\mathbf{\mu})^\top \mathbf{\Sigma}^{-1}(\mathbf{u}-\mathbf{\mu})},\ \mbox{donde $\mathbf{u}=\begin{pmatrix}u \\ v\end{pmatrix}$ y $\mathbf{\mu}=\begin{pmatrix}\mu_1\\\mu_2\end{pmatrix}$.}
\]</span>
La variable aleatoria bidimensional <span class="math inline">\((X,Y)\)</span> sería la <strong>variable aleatoria tipificada</strong> con respecto de la variable <span class="math inline">\((U,V)\)</span>.</p>
</div>
<h3 id="transformaciones-generales-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Transformaciones generales de variables aleatorias</h3>
<p>Consideremos una variable aleatoria bidimensional continua <span class="math inline">\((X,Y)\)</span> con <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span>.</p>
<p>Definimos la variable aleatoria bidimensional continua <span class="math inline">\((U,V)\)</span> a partir de una transformación general de la variable <span class="math inline">\((X,Y)\)</span>. O sea, existen dos funciones de dos variables <span class="math inline">\(g_1\)</span> y <span class="math inline">\(g_2\)</span> tal que:
<span class="math display">\[
U  = g_1 (X,Y),\quad 
V  = g_2 (X,Y).
\]</span>
Vamos a suponer que las funciones <span class="math inline">\(g_1\)</span> y <span class="math inline">\(g_2\)</span> son invertibles, o sea, dados <span class="math inline">\((u,v)\)</span>, podemos encontrar <span class="math inline">\((x,y)\)</span> tal que <span class="math inline">\(x=h_1(u,v)\)</span> e <span class="math inline">\(y=h_2(u,v)\)</span>. Las funciones <span class="math inline">\(h_1\)</span> y <span class="math inline">\(h_2\)</span> serían las inversas de las funciones <span class="math inline">\(g_1\)</span> y <span class="math inline">\(g_2\)</span>, respectivamente.</p>
<h3 id="transformaciones-generales-de-variables-aleatorias-1"><span class="header-section-number">2.2.6</span> Transformaciones generales de variables aleatorias</h3>
<p>La <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{UV}\)</span> se puede expresar de la forma siguiente en función de la <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{XY}\)</span>:
<span class="math display">\[
\begin{array}{rl}
f_{UV}(u,v) &amp; =\left|\mathrm{det}\begin{pmatrix}\frac{\partial h_1}{\partial u} &amp; \frac{\partial h_1}{\partial v}\\ \frac{\partial h_2}{\partial u} &amp; \frac{\partial h_2}{\partial v}\end{pmatrix}\right|f_{XY}(h_1(u,v),h_2(u,v))\\ &amp; =\frac{1}{\left|\mathrm{det}\begin{pmatrix}\frac{\partial g_1}{\partial x} &amp; \frac{\partial g_1}{\partial y}\\ \frac{\partial g_2}{\partial x} &amp; \frac{\partial g_2}{\partial y}\end{pmatrix}\right|_{x=h_1(u,v),y=h_2(u,v)}}f_{XY}(h_1(u,v),h_2(u,v)).
\end{array}
\]</span></p>
<h3 id="transformaciones-generales-de-variables-aleatorias-2"><span class="header-section-number">2.2.6</span> Transformaciones generales de variables aleatorias</h3>
<p>A la matriz <span class="math inline">\(\begin{pmatrix}\frac{\partial g_1}{\partial x} &amp; \frac{\partial g_1}{\partial y}\\ \frac{\partial g_2}{\partial x} &amp; \frac{\partial g_2}{\partial y}\end{pmatrix}\)</span> se le llama <strong>matriz jacobiana del cambio</strong> y a la matriz <span class="math inline">\(\begin{pmatrix}\frac{\partial h_1}{\partial u} &amp; \frac{\partial h_1}{\partial v}\\ \frac{\partial h_2}{\partial u} &amp; \frac{\partial h_2}{\partial v}\end{pmatrix}\)</span>, <strong>matriz jacobiana del cambio inverso</strong>.</p>
<h3 id="ejemplo-78"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo. Cambio a polares</strong></p>
<p>Sea <span class="math inline">\((X,Y)\)</span> una variable aleatoria bidimensional cuya <strong>función de densidad conjunta</strong> es:
<span class="math display">\[
f_{XY}(x,y)=
\begin{cases}
\frac{2}{\pi}\left(x^2 + y^2\right), &amp; \mbox{si }(x,y)\in D_1, \\
0, &amp; \mbox{en caso contrario,}
\end{cases}
\]</span>
donde <span class="math inline">\(D_1\)</span> es el disco de radio <span class="math inline">\(1\)</span>:
<span class="math display">\[
D_1 = \{(x,y)\in\mathbb{R}^2,\ | \ x^2+y^2\leq 1\}.
\]</span>
El cambio a polares consiste en considerar las coordenadas polares <span class="math inline">\((r,\alpha)\)</span> de un punto cualquiera <span class="math inline">\((x,y)\)</span> del plano, ver figura adjunta. El cambio que pasa de <span class="math inline">\((r,\alpha)\)</span> a <span class="math inline">\((x,y)\)</span> (fijaos que sería el cambio inverso, según nuestra notación o <span class="math inline">\(h_1\)</span>y <span class="math inline">\(h_2\)</span>, respectivamente) sería:
<span class="math display">\[
x=h_1(r,\alpha)=r\cdot \cos\alpha,\quad y=h_2(r,\alpha)=r\cdot \sin\alpha.
\]</span></p>
</div>
<h3 id="ejemplo-79"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Polares.png" width="750px" /></p>
</div>
<h3 id="ejemplo-80"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Fijémonos que, con el cambio a polares, el disco unidad <span class="math inline">\(D_1\)</span> se transforma en el rectángulo <span class="math inline">\([0,1]\times [0,2\pi]\)</span>.</p>
<p>Hallemos el <strong>jacobiano del cambio inverso</strong>:
<span class="math display">\[
\mathrm{det}\begin{pmatrix}\frac{\partial h_1}{\partial u} &amp; \frac{\partial h_1}{\partial v}\\ \frac{\partial h_2}{\partial u} &amp; \frac{\partial h_2}{\partial v}\end{pmatrix} =\mathrm{det}\begin{pmatrix}\cos\alpha &amp; -r\sin\alpha\\ \sin\alpha &amp; r\cdot\cos\alpha\end{pmatrix} = r.
\]</span>
La <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{r\alpha}\)</span> en las nuevas variables (polares) será:
<span class="math display">\[
f_{r\alpha}(r,\alpha)=r\cdot \frac{2}{\pi}\left((r\cos\alpha)^2+(r\sin\alpha)^2\right)=\frac{2}{\pi}\cdot r^3,
\]</span>
si <span class="math inline">\((r,\alpha)\in [0,1]\times [0,2\pi]\)</span>.</p>
</div>
<h3 id="ejemplo-81"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Podemos comentar que, gracias al cambio a polares, en este caso, es mucho más sencillo y cómodo trabajar con las variables <span class="math inline">\((r,\alpha)\)</span> en vez de trabajar con las variables <span class="math inline">\((x,y)\)</span> por dos razones:</p>
<ul>
<li>La región donde la <strong>función de densidad</strong> no es nula es más simple, ya que trabajar con un rectángulo simplifica mucho más los cálculos que trabajar con un disco a la hora de hallar la <strong>función de distribución</strong>, <strong>densidades marginales</strong>, <strong>densidades condicionadas</strong>, <strong>valores esperados</strong>, etc.</li>
</ul>
<p>Por ejemplo, comprobar que el área de la <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{r\alpha}\)</span> da <span class="math inline">\(1\)</span> es trivial:
<span class="math display">\[
\int_{r=0}^{r=1}\int_{\alpha =0}^{\alpha =2\pi}\frac{2}{\pi} r^3\, d\alpha\, dr = \frac{2}{\pi}\cdot 2\pi \left[\frac{r^4}{4}\right]_{r=0}^{r=1}=4\cdot \frac{1}{4}=1.
\]</span>
- La expresión de la <strong>función de densidad</strong> también es más simple, ya que sólo depende de la primera variable <span class="math inline">\(r\)</span>; sin embargo, la <strong>función de densidad</strong> inicial <span class="math inline">\(f_{XY}\)</span> dependía de las dos variables <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>.</p>
</div>

<h1 id="vectores-aleatorios"><span class="header-section-number">2.2.6</span> Vectores aleatorios</h1>
<h2 id="varias-variables-aleatorias"><span class="header-section-number">2.2.6</span> Varias variables aleatorias</h2>
<p>En el capítulo anterior trabajamos con <strong>variables aleatorias <span class="math inline">\(n\)</span>-dimensionales</strong></p>
<p>En este capítulo vamos a generalizar los conceptos introducidos para <strong>variables aleatorias bidimensionales</strong>, con <span class="math inline">\(n\geq 3\)</span>.</p>
<p>El ejemplo que comentamos en el capítulo de variables aleatorias de medir la temperatura media un día determinado del año durante 10 años sería un ejemplo de variable aleatoria 10-dimensional.</p>
<h3 id="varias-variables-aleatorias.-definición"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Definición</h3>
<p>La generalización de la noción de <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional</strong> a partir de la noción de <strong>variable aleatoria bidimensional</strong> es bastante obvia:</p>
<p><l class="definition">Definición de variable aleatoria <span class="math inline">\(n\)</span>-dimensional:</l>
Dado un experimento aleatorio con <strong>espacio muestral</strong> <span class="math inline">\(\Omega\)</span>, definimos <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional</strong> <span class="math inline">\(\mathbf{X}=(X_1,X_2,\ldots,X_n)\)</span> a toda aplicación
<span class="math display">\[
\begin{array}{rl}
\mathbf{X}=(X_1,X_2,\ldots,X_n): \Omega &amp; \longrightarrow \mathbb{R}^n\\
w &amp; \longrightarrow \mathbf{X}(w)=(X_1(w),X_2(w),\ldots,X_n(w)).
\end{array}
\]</span></p>
<h3 id="varias-variables-aleatorias.-ejemplos"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Ejemplos</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Tenemos tres puertos de entrada de paquetes de internet.</p>
<p>Supongamos que cada milisegundo llega un paquete y el switch lo asigna a cada uno de los puertos con probabilidad <span class="math inline">\(\frac{1}{3}\)</span>.</p>
<p>Estudiamos cómo se distribuyen los paquetes en 4 milisegundos.</p>
<p>Sea <span class="math inline">\(\mathbf{X}=(X_1,X_2,X_3)\)</span> la variable aleatoria 3-dimensional, donde <span class="math inline">\(X_i\)</span> nos da el número de paquetes que ha recibido el puerto <span class="math inline">\(i\)</span>-ésimo durante estos 4 milisegundos.</p>
<p>Por ejemplo, el suceso <span class="math inline">\(\{X_1\leq 1, X_2\geq 3, X_3\leq 1\}\)</span> sería <span class="math inline">\(\{(0,3,0),(0,3,1),(0,4,0),(0,4,1),(1,3,0),(1,3,1),(1,4,0),(1,4,1)\}\)</span>.</p>
</div>
<h3 id="varias-variables-aleatorias.-introducción"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Introducción</h3>
<p>Los sucesos que se derivan de una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional</strong> estan especificados por regiones del espacio <span class="math inline">\(n\)</span>-dimensional.</p>
<p>Veamos algunos ejemplos:</p>
<p>Suceso: <span class="math inline">\(\{X_1+X_2+X_3\leq 1\}\)</span>. En el gráfico siguiente, el plano <span class="math inline">\(x_1+x_2+x_3=1\)</span> separa el espacio en dos partes. Sería la parte que corresponde al punto <span class="math inline">\((0,0,0)\)</span>.</p>
<p>O sea, si pensamos el plano anterior como un “espejo” sería la parte de atrás del mismo.</p>
<h3 id="ejemplo-82"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/EjPlano3D.png" width="650px" /></p>
</div>
<h3 id="varias-variables-aleatorias.-introducción-1"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Introducción</h3>
<p>Suceso: <span class="math inline">\(\{X_1^2+X_2^2+X_3^2\leq 1\}\)</span>. Sería el interior de la esfera del gráfico siguiente:</p>
<h3 id="varias-variables-aleatorias.-introducción-2"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Introducción</h3>
<div class="center">
<p><img src="Images/EjEsfera3D.png" width="750px" /></p>
</div>
<h3 id="varias-variables-aleatorias.-introducción-3"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Introducción</h3>
<p>Suceso: <span class="math inline">\(\{0\leq X_1\leq 1,\ 0\leq X_2\leq 1,\ 0\leq X_3\leq 1\}\)</span>. Sería el interior del cubo del gráfico siguiente:</p>
<h3 id="varias-variables-aleatorias.-introducción-4"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Introducción</h3>
<div class="center">
<p><img src="Images/Ej3DCubo.png" width="950px" /></p>
</div>
<h3 id="varias-variables-aleatorias.-introducción-5"><span class="header-section-number">2.2.6</span> Varias variables aleatorias. Introducción</h3>
<p>La probabilidad de que la <strong>variable <span class="math inline">\(n\)</span>-dimensional</strong> pertenezca a una cierta <strong>región del <span class="math inline">\(n\)</span>-espacio <span class="math inline">\(B\subset \mathbb{R}^n\)</span></strong> se define de la forma siguiente:
<span class="math display">\[
P((X_1,X_2,\ldots,X_n)\in B)=P\{w\in \Omega,\ |\ (X_1(w),X_2(w),\ldots,X_n)\in B\},
\]</span>
o sea, la probabilidad anterior es la probabilidad del suceso formado por los elementos de <span class="math inline">\(w\in\Omega\)</span> que cumplen que su <strong>imagen</strong> por la <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\((X_1,X_2,\ldots,X_n)\)</span></strong> esté en <span class="math inline">\(B\)</span>.</p>
<p>Por ejemplo, si consideramos <span class="math inline">\(B=\{X_1+X_2+\cdots +X_n\leq 1\}\)</span>, <span class="math inline">\(P((X_1,X_2,\ldots,X_n)\in B)\)</span> sería la probabilidad del suceso formado por los elementos <span class="math inline">\(w\)</span> de <span class="math inline">\(\Omega\)</span> tal que la suma de las imágenes por <span class="math inline">\(X_i\)</span> desde <span class="math inline">\(i=1\)</span> hasta <span class="math inline">\(n\)</span> sea menor o igual que 1: <span class="math inline">\(X_1(w)+\cdots +X_n\leq 1\)</span>.</p>
<h2 id="función-de-distribución-conjunta-1"><span class="header-section-number">2.2.6</span> Función de distribución conjunta</h2>
<h3 id="función-de-distribución-conjunta.-introducción-3"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Introducción</h3>
<p>Dada una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional</strong> <span class="math inline">\((X_1,X_2,\ldots,X_n)\)</span>, queremos estudiar cómo se distribuye la probabilidad de sucesos cualesquiera de la forma <span class="math inline">\(\{(X_1,X_2,\ldots,X_n)\in B\}\)</span>, donde <span class="math inline">\(B\)</span> es una región del espació <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>Para ello, definimos la <strong>función de distribución conjunta</strong>:</p>
<p><l class="definition">Definición de función de distribución conjunta:</l>
Dada una variable <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\((X_1,X_2,\ldots,X_n)\)</span>, definimos su <strong>función de distribución conjunta</strong> <span class="math inline">\(F_{X_1\ldots X_n}\)</span> a la función definida sobre <span class="math inline">\(\mathbb{R}^n\)</span> de la manera siguiente:
<span class="math display">\[
\begin{array}{rl}
F_{X_1\ldots X_n}: \mathbb{R}^n &amp; \longrightarrow \mathbb{R}\\
(x_1,\ldots,x_n) &amp; \longrightarrow F_{X_1\ldots X_n}(x_1,\ldots,x_n)=P(X_1\leq x_1,\ldots,X_n\leq x_n).
\end{array}
\]</span></p>
<h3 id="función-de-distribución-conjunta.-introducción-4"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Introducción</h3>
<p>O sea, dado un valor <span class="math inline">\((x_1,\ldots,x_n)\in \mathbb{R}^n\)</span>, consideramos la región del espacio <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\((-\infty,x_1]\times\cdots\times (-\infty,x_n]\)</span>.</p>
<p>Entonces la <strong>función de distribución conjunta</strong> en el valor <span class="math inline">\((x_1,\ldots,x_n)\)</span> es la probabilidad del suceso formado por aquellos elementos tal que la imagen por la <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional</strong> <span class="math inline">\((X_1,X_2,\ldots,X_n)\)</span> caen dentro de la región anterior:</p>
<p><span class="math display">\[
\begin{array}{rl}
F_{X_1\ldots X_n}(x_1,\ldots,x_n) &amp; =P\{w\in\Omega,\ |\ (X_1(w),\ldots,X_n(w)) \\ &amp; \qquad\qquad\in (-\infty,x_1]\times\cdots\times (-\infty,x_n]\} \\ &amp; = P\{w\in\Omega,\ |\ X_1(w)\leq x_1,\ldots, X_n(w)\leq x_n\}.
\end{array}
\]</span></p>
<h3 id="función-de-distribución-conjunta.-introducción-5"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Introducción</h3>
<p>El gráfico siguiente muestra el conjunto <span class="math inline">\((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3]\)</span> en <span class="math inline">\(\mathbb{R}^3\)</span> para un valor <span class="math inline">\((x,y,z)\)</span>:</p>
<div class="center">
<p><img src="Images/Fx1x2x3.png" width="550px" /></p>
</div>
<h3 id="función-de-distribución-conjunta.-propiedades-5"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<p>Sea <span class="math inline">\((X_1,X_2,\ldots,X_n)\)</span> una variable <span class="math inline">\(n\)</span>-dimensional. Sean <span class="math inline">\(F_{X_1\ldots X_n}\)</span> su <strong>función de distribución conjunta</strong>. Dicha función satisface las propiedades siguientes:</p>
<ul>
<li><p>La función de distribución conjunta es no decreciente en cada una de las variables:
<span class="math display">\[
\mbox{Si }x_i\leq x_i&#39;, \mbox{ para todo $i$, }\mbox{ entonces, }F_{X_1\ldots X_n}(x_1,\ldots,x_n)\leq F_{X_1\ldots X_n}(x_1&#39;,\ldots,x_n&#39;).
\]</span></p></li>
<li><p><span class="math inline">\(F_{X_1\ldots X_n}(x_1,\ldots,x_{i-1},\stackrel{(i)}{-\infty},x_{i+1},\ldots,x_n)=0,\)</span> para todo <span class="math inline">\(i\)</span> y <span class="math inline">\(F_{X_1\ldots X_n}(\infty,\ldots,\infty)=1\)</span>, para todo <span class="math inline">\(x_1,\ldots,x_n\in\mathbb{R}\)</span>.</p></li>
</ul>
<h3 id="función-de-distribución-conjunta.-propiedades-6"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<ul>
<li>Las variables aleatorias <span class="math inline">\(X_1,\ldots, X_n\)</span> se llaman <strong>variables aleatorias marginales</strong> y sus funciones de distribución <span class="math inline">\(F_{X_1},\ldots, F_{X_n}\)</span> pueden hallarse de la forma siguiente como función de la <strong>función de distribución conjunta</strong> <span class="math inline">\(F_{X_1\ldots X_n}\)</span>:
<span class="math display">\[
F_{X_i}(x_i)=F_{X_1\ldots X_n}(\infty,\ldots,\infty,\stackrel{(i)}{x_i},\infty,\ldots,\infty),
\]</span>
para todo <span class="math inline">\(x_1,\ldots,x_n\in\mathbb{R}\)</span> y para todo <span class="math inline">\(i=1,\ldots,n\)</span>.</li>
</ul>
<h3 id="función-de-distribución-conjunta.-propiedades-7"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Propiedades</h3>
<ul>
<li>La función de distribución conjunta es continua por la derecha en todas las variables <span class="math inline">\(x_i\)</span>:
<span class="math display">\[
\begin{array}{rl}
 &amp; \lim\limits_{x_i\to a^+}F_{X_1\ldots X_n}(x_1,\ldots,x_{i-1},\stackrel{(i)}{x_i},x_{i+1},\ldots,x_n) \\ &amp;\qquad =\lim\limits_{x_i\to a, x_i&gt; a}F_{X_1\ldots X_n}(x_1,\ldots,x_{i-1},\stackrel{(i)}{x_i},x_{i+1},\ldots,x_n)\\ &amp;\qquad =F_{X_1\ldots X_n}(x_1,\ldots,x_{i-1},\stackrel{(i)}{a},x_{i+1},\ldots,x_n),
\end{array}
\]</span>
para todo <span class="math inline">\(a\in\mathbb{R}\)</span> y para todo <span class="math inline">\(i=1,\ldots,n\)</span>.</li>
</ul>
<h3 id="función-de-distribución-conjunta.-ejemplo-6"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos una variable aleatoria <span class="math inline">\(3\)</span>-dimensional <span class="math inline">\((X_1,X_2,X_3)\)</span> con <strong>función de distribución conjunta</strong>:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
0, &amp; \mbox{si }x_1&lt;0,\mbox{ o }x_2&lt;0,\mbox{ o }x_3 &lt;0\\
x_1^2\cdot x_2^2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
 x_2^2\cdot x_3^2, &amp; \mbox{si }x_1&gt; 1,\ 0\leq x_2\leq  1,\ 0\leq x_3\leq  1, \\
 x_1^2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 x_3^2, &amp; \mbox{si }x_1&gt; 1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 x_1^2\cdot x_2^2, &amp; \mbox{si }0\leq x_1\leq  1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
 x_1^2, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2 &gt;  1,\ x_3&gt; 1,\\
 x_2^2, &amp; \mbox{si }x_1&gt;1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
1, &amp; \mbox{si }x_1\geq 1,\ x_2\geq 1,\ x_3\geq 1.
\end{cases}
\]</span></p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-7"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>En las figuras siguientes, hemos representado por zonas cómo está definida <span class="math inline">\(F_{X_1X_2X_3}\)</span>.</p>
<p>La primera figura muestra las zonas en la “planta baja” o para <span class="math inline">\(0\leq x_3\leq 1\)</span>.
En color marrón, está representada la región <span class="math inline">\(0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1\)</span>, en color amarillo, la región <span class="math inline">\(x_1&gt; 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1\)</span>, en color verde, la región <span class="math inline">\(x_1&gt;1,\ x_2&gt;1,\ 0\leq x_3\leq 1\)</span> y en color violeta, la región <span class="math inline">\(0\leq x_1\leq 1,\ x_2&gt;1,\ 0\leq x_3\leq 1\)</span>.</p>
<p>La segunda figura muestra las zonas del “primer piso” o para <span class="math inline">\(x_3&gt;1\)</span>. Los colores tienen un significado similar a los de la primera figura: en color marrón, está representada la región <span class="math inline">\(0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\  x_3&gt; 1\)</span>, en color amarillo u ocre, la región <span class="math inline">\(x_1&gt; 1,\ 0\leq x_2\leq 1,\ x_3&gt; 1\)</span>, en color verde, la región <span class="math inline">\(x_1&gt;1,\ x_2&gt;1,\ x_3&gt; 1\)</span> y en color violeta, la región <span class="math inline">\(0\leq x_1\leq 1,\ x_2&gt;1,\ x_3&gt; 1\)</span>.</p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-8"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="center">
<p><img src="Images/Ej3DFxyz.png" width="650px" /></p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-9"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="center">
<p><img src="Images/Ej3DFxy2pis.png" width="650px" /></p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-10"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p>Comprobemos algunas de las propiedades que hemos enunciado anteriormente:</p>
<ul>
<li><p>Claramente <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,-\infty)=F_{X_1X_2X_3}(x_1,-\infty,x_3)=F_{X_1X_2X_3}(-\infty,x_2,x_3)=0\)</span> ya que <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\)</span> si <span class="math inline">\(x_1&lt;0\)</span> o <span class="math inline">\(x_2&lt;0\)</span> o <span class="math inline">\(x_3&lt;0\)</span>. Por tanto, si hacemos tender <span class="math inline">\(x_1\)</span> o <span class="math inline">\(x_2\)</span> o <span class="math inline">\(x_3\)</span> hacia <span class="math inline">\(-\infty\)</span>, obtendremos que <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,-\infty)=F_{X_1X_2X_3}(x_1,-\infty,x_3)=F_{X_1X_2X_3}(-\infty,x_2,x_3)=0\)</span>.</p></li>
<li><p>De la misma manera <span class="math inline">\(F_{X_1X_2X_3}(\infty,\infty,\infty)=1\)</span> ya que <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,x_3)=1\)</span> para <span class="math inline">\(x_1&gt;1\)</span>, <span class="math inline">\(x_2&gt;1\)</span> y <span class="math inline">\(x_3&gt;1\)</span>. Por tanto, si hacemos tender <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> y <span class="math inline">\(x_3\)</span> hacia <span class="math inline">\(\infty\)</span>, obtendremos <span class="math inline">\(F_{X_1X_2X_3}(\infty,\infty,\infty)=1\)</span>.</p></li>
<li><p>Hallemos las marginales:
<span class="math display">\[
F_{X_1}(x_1)=F_{X_1X_2X_3}(x_1,\infty,\infty)=\begin{cases}
0, &amp; \mbox{ si }x_1 &lt; 0,\\
x_1, &amp; \mbox{ si } 0\leq x_1\leq 1,\\
1, &amp; \mbox{ si } x_1&gt;1.
\end{cases}
\]</span>
Para ver la expresión anterior basta trazar el plano <span class="math inline">\(X_1=x_1\)</span> en el gráfico anterior y ver hacia dónde tiende a medida que las variables <span class="math inline">\(x_2\)</span> y <span class="math inline">\(x_3\)</span> se van hacia <span class="math inline">\(\infty\)</span>.</p></li>
</ul>
<p>¿Habéis averiguado cuál es la distribución de <span class="math inline">\(X_1\)</span>?</p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-11"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p>¡Efectivamente!, <span class="math inline">\(X_1\)</span> es la uniforme en el intervalo <span class="math inline">\((0,1)\)</span>.</p>
<p>Dejamos como ejercicio hallar la distribución marginal para las variables <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_3\)</span>.</p>
<ul>
<li>Comprobemos que <span class="math inline">\(F_{X_1X_2X_3}\)</span> es continua por la derecha para las variables <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> y <span class="math inline">\(x_3\)</span> en el punto <span class="math inline">\((1,1,1)\)</span> que sería un punto problemático:
<span class="math display">\[
\begin{array}{rl}
 \lim_{x_1\to 1,x_1&gt; 1} F_{X_1X_2X_3}(x_1,1,1) &amp; =\lim_{x_1\to 1,x_1&gt; 1} 1  = F_{X_1X_2X_3}(1,1,1),\\
 \lim_{x_2\to 1,x_2&gt; 1} F_{X_1X_2X_3}(1,x_2,1) &amp; =\lim_{x_2\to 1,x_2&gt; 1} 1 = F_{X_1X_2X_3}(1,1,1),\\  
\lim_{x_3\to 1,x_3&gt; 1} F_{X_1X_2X_3}(1,1,x_3) &amp; =\lim_{x_3\to 1,x_3&gt; 1} 1  = F_{X_1X_2X_3}(1,1,1).
\end{array}
\]</span></li>
</ul>
</div>
<h3 id="ejemplo-con-r-9"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Realizar un gráfico 3D de la <strong>función de distribución conjunta</strong> no es posible ya que deberíamos pasar a <span class="math inline">\(\mathbb{R}^4\)</span>.</p>
<p>Lo que sí es posible es dibujar las curvas de nivel de dicha función para un valor de <span class="math inline">\(x_3\)</span> fijado.</p>
<p>El los gráficos siguientes dibujamos las curvas de nivel para <span class="math inline">\(x_3=0,0.5,1\)</span> i <span class="math inline">\(x_3=1.5\)</span>.</p>
<p>Primero definimos la <strong>función</strong> y luego la dibujamos para <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> entre <span class="math inline">\(-1\)</span> y <span class="math inline">\(3\)</span>:</p>
</div>
<h3 id="ejemplo-con-r-10"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1">f.dist.con =<span class="st"> </span><span class="cf">function</span>(x1,x2,x3){<span class="kw">ifelse</span>(x1<span class="op">&lt;</span><span class="dv">0</span> <span class="op">|</span><span class="st"> </span>x2<span class="op">&lt;</span><span class="dv">0</span> <span class="op">|</span><span class="st"> </span>x3 <span class="op">&lt;</span><span class="dv">0</span>,<span class="dv">0</span>,</a>
<a class="sourceLine" id="cb96-2" data-line-number="2">          <span class="kw">ifelse</span>(x1<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x1<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x3<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x3<span class="op">&lt;=</span><span class="dv">1</span>,x1<span class="op">^</span><span class="dv">2</span><span class="op">*</span>x2<span class="op">^</span><span class="dv">2</span><span class="op">*</span>x3<span class="op">^</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb96-3" data-line-number="3">          <span class="kw">ifelse</span>(x1<span class="op">&gt;</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x3<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x3<span class="op">&lt;=</span><span class="dv">1</span>,x2<span class="op">^</span><span class="dv">2</span><span class="op">*</span>x3<span class="op">^</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb96-4" data-line-number="4">          <span class="kw">ifelse</span>(x1<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x1<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&gt;</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x3<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x3<span class="op">&lt;=</span><span class="dv">1</span>,x1<span class="op">^</span><span class="dv">2</span><span class="op">*</span>x3<span class="op">^</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb96-5" data-line-number="5">          <span class="kw">ifelse</span>(x1<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x1<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x3<span class="op">&gt;</span><span class="dv">1</span>,x1<span class="op">^</span><span class="dv">2</span><span class="op">*</span>x2<span class="op">^</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb96-6" data-line-number="6">          <span class="kw">ifelse</span>(x1<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x1<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x2 <span class="op">&gt;</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x3 <span class="op">&gt;</span><span class="dv">1</span>,x1<span class="op">^</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb96-7" data-line-number="7">          <span class="kw">ifelse</span>(x1<span class="op">&gt;</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x2 <span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x3 <span class="op">&gt;</span><span class="dv">1</span>,x2<span class="op">^</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb96-8" data-line-number="8">          <span class="kw">ifelse</span>(x1<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x1<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x2<span class="op">&lt;=</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>x3 <span class="op">&gt;</span><span class="dv">1</span>,x3<span class="op">^</span><span class="dv">2</span>,<span class="dv">1</span>))))))))}</a>
<a class="sourceLine" id="cb96-9" data-line-number="9">x1=<span class="kw">seq</span>(<span class="dt">from=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">to=</span><span class="dv">3</span>,<span class="dt">by=</span><span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb96-10" data-line-number="10">x2=<span class="kw">seq</span>(<span class="dt">from=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">to=</span><span class="dv">3</span>,<span class="dt">by=</span><span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb96-11" data-line-number="11">curva.nivel<span class="fl">.0</span>=<span class="kw">outer</span>(x1,x2,f.dist.con,<span class="dt">x3=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb96-12" data-line-number="12">curva.nivel.<span class="fl">0.5</span>=<span class="kw">outer</span>(x1,x2,f.dist.con,<span class="dt">x3=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb96-13" data-line-number="13">curva.nivel<span class="fl">.1</span>=<span class="kw">outer</span>(x1,x2,f.dist.con,<span class="dt">x3=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb96-14" data-line-number="14">curva.nivel.<span class="fl">1.5</span>=<span class="kw">outer</span>(x1,x2,f.dist.con,<span class="dt">x3=</span><span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb96-15" data-line-number="15"><span class="kw">image</span>(x1,x2,curva.nivel<span class="fl">.0</span>)</a>
<a class="sourceLine" id="cb96-16" data-line-number="16"><span class="kw">image</span>(x1,x2,curva.nivel.<span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb96-17" data-line-number="17"><span class="kw">image</span>(x1,x2,curva.nivel<span class="fl">.1</span>)</a>
<a class="sourceLine" id="cb96-18" data-line-number="18"><span class="kw">image</span>(x1,x2,curva.nivel.<span class="fl">1.5</span>)</a></code></pre></div>
</div>
<h3 id="ejemplo-con-r.-curva-de-nivel-para-x_30"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code>. Curva de nivel para <span class="math inline">\(x_3=0\)</span></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-66-1.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="ejemplo-con-r.-curva-de-nivel-para-x_30.5"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code>. Curva de nivel para <span class="math inline">\(x_3=0.5\)</span></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-67-1.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="ejemplo-con-r.-curva-de-nivel-para-x_31"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code>. Curva de nivel para <span class="math inline">\(x_3=1\)</span></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-68-1.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="ejemplo-con-r.-curva-de-nivel-para-x_31.5"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code>. Curva de nivel para <span class="math inline">\(x_3=1.5\)</span></h3>
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-69-1.png" width="672" style="display: block; margin: auto;" /></p>
<h3 id="función-de-distribución-conjunta.-ejemplo-12"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo del lanzamiento de un dado tres veces</strong></p>
<p>Consideremos el experimento aleatorio que consiste en lanzar un dado tres veces.</p>
<p>El espacio <span class="math inline">\(\Omega\)</span> de resultados será:
<span class="math display">\[
\Omega =\{(i,j,k),\ | i,j,k=1,2,3,4,5,6\}.
\]</span>
En total tendremos <span class="math inline">\(6\cdot 6\cdot 6=6^3=216\)</span> resultados posibles.</p>
<p>Consideremos la variable 3-dimensional <span class="math inline">\(\mathbf{X}=(X_1,X_2,X_3)\)</span>, donde <span class="math inline">\(X_1\)</span> nos da el número de 1’s obtenidos, <span class="math inline">\(X_2\)</span>, el número de 2’s y <span class="math inline">\(X_3\)</span>, el número de 3’s.</p>
<p>El conjunto <span class="math inline">\(\mathbf{X}(\Omega)\)</span> tiene en total 64 elementos ya que cada componente <span class="math inline">\(X_i\)</span> puede tener en total 4 resultados: 0, 1, 2 o 3. Por tanto el conjunto total de resultados será: <span class="math inline">\(4\cdot 4\cdot 4=4^3=64\)</span>.</p>
</div>
<h3 id="función-de-distribución-conjunta.-ejemplo-13"><span class="header-section-number">2.2.6</span> Función de distribución conjunta. Ejemplo</h3>
<div class="example">
<p>El valor de función de distribución conjunta en el resultado <span class="math inline">\((0,0,0)\)</span> será:
<span class="math display">\[
F_{X_1X_2X_3}(0,0,0)=p(X_1\leq 0,\ X_2\leq 0,\ X_3\leq 0)=\frac{3^3}{6^3}=\left(\frac{1}{2}\right)^3 =0.125,
\]</span>
ya que si <span class="math inline">\(X_1\leq 0\)</span>, <span class="math inline">\(X_2\leq 0\)</span> y <span class="math inline">\(X_3\leq 0\)</span>, significa que no ha salido ni ningún 1, ni ningún 2 ni ningún 3. Sólo pueden salir 4’s, 5’s o 6’s y existen <span class="math inline">\(3\cdot 3\cdot 3=3^3=27\)</span> posibilidades de que esto pase entre <span class="math inline">\(6^3=216\)</span> posibilidades posibles.</p>
</div>
<h2 id="variables-aleatorias-n-dimensionales-discretas"><span class="header-section-number">2.2.6</span> Variables aleatorias <span class="math inline">\(n\)</span>-dimensionales discretas</h2>
<h3 id="variables-aleatorias-n-dimensionales-discretas.-introducción"><span class="header-section-number">2.2.6</span> Variables aleatorias <span class="math inline">\(n\)</span>-dimensionales discretas. Introducción</h3>
<p><l class="definition">Definición de variable aleatoria <span class="math inline">\(n\)</span>-dimensional discreta:</l>
Sea <span class="math inline">\((X_1,\ldots,X_n)\)</span> una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional</strong>. Diremos que es discreta cuando su conjunto de valores en <span class="math inline">\(\mathbb{R}^n\)</span>, <span class="math inline">\((X_1,\ldots,X_n)(\Omega)\)</span> es un conjunto finito o numerable.</p>
<p>En la mayoría de los casos, dicho conjunto será un subconjunto de los enteros naturales.</p>
<h3 id="ejemplo-83"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>La variable aleatoria 3-dimensional anterior que nos daba el número de 1’s obtenidos, el número de 2’s y el número de 3’s es discreta ya que
<span class="math display">\[
\begin{array}{rl}
\mathbf{X}(\Omega)=\{&amp; (0,0,0),(1,0,0),(0,1,0),(0,0,1),(2,0,0),(0,2,0),(0,0,2),(3,0,0),(0,3,0),(0,0,3), \\
&amp; (0,1,1),(1,0,1),(1,1,0),(0,1,2),(0,2,1),(1,0,2),(2,0,1),(1,2,0),(2,1,0),(0,1,3), \\ &amp;
(0,3,1),(1,0,3),(3,0,1),(1,3,0),(3,1,0),(0,2,2),(2,0,2),(2,2,0),(0,2,3),(3,2,0),\\ &amp;
(2,0,3),(3,0,2),(2,3,0),(3,2,0),(0,3,3),(3,0,3),(3,3,0),(1,1,1),(1,1,2),(1,2,1),\\ &amp;
(2,1,1),(1,1,3),(1,3,1),(3,1,1),(1,2,2),(2,1,2),(2,2,1),(1,2,3),(2,1,3),(1,3,2),\\ &amp;
(3,1,2),(2,3,1),(3,2,1),(1,3,3),(3,1,3),(3,3,1),(2,2,2),(2,2,3),(2,3,2),(3,2,2)\\ &amp;
(2,3,3),(3,2,3),(3,3,2),(3,3,3)\}.
\end{array}
\]</span></p>
</div>
<h3 id="función-de-probabilidad-conjunta-2"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta</h3>
<p><l class="definition">Definición de función de probabilidad conjunta:</l>
Dada una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional discreta</strong> <span class="math inline">\((X_1\ldots,X_n)\)</span> con <span class="math inline">\((X_1\ldots,X_n)(\Omega)=\{(x_{i_1},x_{i_2},\ldots,x_{i_n}),\ i_1=1,2,\ldots,\ i_n=1,2,\ldots,\}\)</span>, definimos la función de probabilidad discreta <span class="math inline">\(P_{X_1\ldots X_n}\)</span> para un valor <span class="math inline">\((x_{i_1},x_{i_2},\ldots,x_{i_n})\in\mathbb{R}^n\)</span> de la siguiente forma:
<span class="math display">\[
\begin{array}{rl}
P_{X_1\ldots X_n}: \mathbb{R}^n &amp; \longrightarrow \mathbb{R}\\
(x_{i_1},x_{i_2},\ldots,x_{i_n}) &amp; \longrightarrow P_{X_1\ldots X_n}(x_{i_1},x_{i_2},\ldots,x_{i_n})=P(X= x_{i_1},\ldots X_n= x_{i_n}).
\end{array}
\]</span></p>
<h3 id="función-de-probabilidad-conjunta-3"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta</h3>
<p><l class="observ">Observación:</l>
Si <span class="math inline">\((x_{i_1},x_{i_2},\ldots,x_{i_n})\not\in (X_1\ldots,X_n)(\Omega)\)</span>, el valor de la <strong>función de probabilidad conjunta</strong> en <span class="math inline">\((x_{i_1},x_{i_2},\ldots,x_{i_n})\)</span> en nulo: <span class="math inline">\(P_{X_1\ldots X_n}(x_{i_1},x_{i_2},\ldots,x_{i_n})=0\)</span>, ya que, en este caso, el conjunto <span class="math inline">\(\{w\in\Omega,\ | (X_1(w),\ldots,X_n(w))=(x_{i_1},x_{i_2},\ldots,x_{i_n})\}=\emptyset\)</span> ya que recordemos <span class="math inline">\((x_{i_1},x_{i_2},\ldots,x_{i_n})\not\in (X_1\ldots,X_n)(\Omega)\)</span>.</p>
<h3 id="función-de-probabilidad-conjunta-4"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta</h3>
<p>Por tanto, de cara a calcular <span class="math inline">\(P_{X_1\ldots X_n}\)</span> basta calcular <span class="math inline">\(P_{X_1\ldots X_n}(x_{i_1},\ldots,x_{i_n})\)</span> para <span class="math inline">\((x_{i_1},\ldots,x_{i_n})\in (X_1\ldots,X_n)(\Omega)\)</span>.</p>
<p>Los valores de <span class="math inline">\(P_{X_1\ldots X_n}(x_{i_1},\ldots,x_{i_n})\)</span> estarían organizados en una tabla <span class="math inline">\(n\)</span>-dimensional.</p>
<h3 id="ejemplo-84"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la variable 3-dimensional que nos da el número de 1’s, 2’s y 3’s en el lanzamiento de un dado tres veces</strong></p>
<p>Para mostrar la <strong>función de probabilidad conjunta</strong> haremos una tabla bidimensional para cada valor de <span class="math inline">\(X_3\)</span>.</p>
<p>Como <span class="math inline">\(X_3(\Omega)=\{0,1,2,3\}\)</span>, en total mostraremos 4 tablas bidimensionales.</p>
</div>
<h3 id="ejemplo-85"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Tabla para <span class="math inline">\(X_3=0\)</span>:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1/X_2\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(\frac{1}{8}\)</span></td>
<td><span class="math inline">\(\frac{1}{8}\)</span></td>
<td><span class="math inline">\(\frac{1}{24}\)</span></td>
<td><span class="math inline">\(\frac{1}{216}\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(\frac{1}{8}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{72}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(\frac{1}{24}\)</span></td>
<td><span class="math inline">\(\frac{1}{72}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td><span class="math inline">\(\frac{1}{216}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="ejemplo-86"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Tabla para <span class="math inline">\(X_3=1\)</span>:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1/X_2\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(\frac{1}{8}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{72}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{36}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(\frac{1}{72}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="ejemplo-87"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Tabla para <span class="math inline">\(X_3=2\)</span>:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1/X_2\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(\frac{1}{24}\)</span></td>
<td><span class="math inline">\(\frac{1}{72}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(\frac{1}{72}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="ejemplo-88"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Tabla para <span class="math inline">\(X_3=3\)</span>:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1/X_2\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(\frac{1}{216}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="función-de-probabilidad-conjunta.-ejemplo-con-r-8"><span class="header-section-number">2.2.6</span> Función de probabilidad conjunta. Ejemplo con <code>R</code></h3>
<div class="example">
<p>La función <code>fun.prod.con</code> nos da la <strong>función de probabilidad conjunta</strong> de la variable aleatoria <span class="math inline">\(\mathbf{X}\)</span> cuando lanzamos un dado tres veces:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1">fun.prob.con=<span class="cf">function</span>(x1,x2,x3){</a>
<a class="sourceLine" id="cb97-2" data-line-number="2">  n=<span class="dv">6</span></a>
<a class="sourceLine" id="cb97-3" data-line-number="3">  cuenta<span class="fl">.1</span> =<span class="cf">function</span>(x){<span class="kw">length</span>(x[x<span class="op">==</span><span class="dv">1</span>])}</a>
<a class="sourceLine" id="cb97-4" data-line-number="4">  cuenta<span class="fl">.2</span> =<span class="cf">function</span>(x){<span class="kw">length</span>(x[x<span class="op">==</span><span class="dv">2</span>])}</a>
<a class="sourceLine" id="cb97-5" data-line-number="5">  cuenta<span class="fl">.3</span> =<span class="cf">function</span>(x){<span class="kw">length</span>(x[x<span class="op">==</span><span class="dv">3</span>])}</a>
<a class="sourceLine" id="cb97-6" data-line-number="6">  Dxyz=<span class="kw">data.frame</span>(<span class="dt">d1=</span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dt">each=</span>n),<span class="dt">d2=</span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dt">times=</span>n),<span class="dt">d3=</span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dt">each=</span>n<span class="op">*</span>n))</a>
<a class="sourceLine" id="cb97-7" data-line-number="7">  X1=<span class="kw">apply</span>(Dxyz,<span class="dv">1</span>,cuenta<span class="fl">.1</span>)</a>
<a class="sourceLine" id="cb97-8" data-line-number="8">  X2=<span class="kw">apply</span>(Dxyz,<span class="dv">1</span>,cuenta<span class="fl">.2</span>)</a>
<a class="sourceLine" id="cb97-9" data-line-number="9">  X3=<span class="kw">apply</span>(Dxyz,<span class="dv">1</span>,cuenta<span class="fl">.3</span>)</a>
<a class="sourceLine" id="cb97-10" data-line-number="10">  frecuencia =<span class="st"> </span><span class="kw">table</span>(X1<span class="op">==</span>x1 <span class="op">&amp;</span><span class="st"> </span>X2<span class="op">==</span>x2 <span class="op">&amp;</span><span class="st"> </span>X3<span class="op">==</span>x3)</a>
<a class="sourceLine" id="cb97-11" data-line-number="11">  res=<span class="kw">ifelse</span>(<span class="kw">length</span>(frecuencia)<span class="op">==</span><span class="dv">2</span>,frecuencia[<span class="dv">2</span>],<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb97-12" data-line-number="12">  <span class="kw">return</span>(res<span class="op">/</span><span class="dv">6</span><span class="op">^</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb97-13" data-line-number="13">}</a></code></pre></div>
</div>
<h3 id="ejemplo-con-r-11"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Para construir la tabla de la <strong>función de probabilidad conjunta</strong> para la variable <span class="math inline">\(\mathbf{X}=(X_1,X_2,X_3)\)</span> con <span class="math inline">\(X_3=0\)</span> hacemos lo siguiente:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1">valores.variables=<span class="dv">0</span><span class="op">:</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb98-2" data-line-number="2">tabla<span class="fl">.0</span> =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb98-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(valores.variables)){<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(valores.variables)){</a>
<a class="sourceLine" id="cb98-4" data-line-number="4">  tabla<span class="fl">.0</span>=<span class="kw">c</span>(tabla<span class="fl">.0</span>,<span class="kw">fun.prob.con</span>(valores.variables[i],valores.variables[j],<span class="dv">0</span>));</a>
<a class="sourceLine" id="cb98-5" data-line-number="5">}}</a>
<a class="sourceLine" id="cb98-6" data-line-number="6">tabla<span class="fl">.0</span> =<span class="st"> </span><span class="kw">matrix</span>(tabla<span class="fl">.0</span>,<span class="kw">length</span>(valores.variables),<span class="kw">length</span>(valores.variables))</a>
<a class="sourceLine" id="cb98-7" data-line-number="7"><span class="kw">rownames</span>(tabla<span class="fl">.0</span>)=valores.variables</a>
<a class="sourceLine" id="cb98-8" data-line-number="8"><span class="kw">colnames</span>(tabla<span class="fl">.0</span>)=valores.variables</a>
<a class="sourceLine" id="cb98-9" data-line-number="9">knitr<span class="op">::</span><span class="kw">kable</span>(tabla<span class="fl">.0</span>)</a></code></pre></div>
<p>Con los demás valores de <span class="math inline">\(X_3\)</span>, lo haríamos de forma similar.</p>
</div>
<h3 id="ejemplo-con-r-12"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Tabla con <span class="math inline">\(X_3=0\)</span>:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">0</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td align="right">0.1250000</td>
<td align="right">0.1250000</td>
<td align="right">0.0416667</td>
<td align="right">0.0046296</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right">0.1250000</td>
<td align="right">0.0833333</td>
<td align="right">0.0138889</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td>2</td>
<td align="right">0.0416667</td>
<td align="right">0.0138889</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0046296</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-con-r-13"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Tabla con <span class="math inline">\(X_3=1\)</span>:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">0</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td align="right">0.1250000</td>
<td align="right">0.0833333</td>
<td align="right">0.0138889</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right">0.0833333</td>
<td align="right">0.0277778</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>2</td>
<td align="right">0.0138889</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-con-r-14"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Tabla con <span class="math inline">\(X_3=2\)</span>:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">0</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td align="right">0.0416667</td>
<td align="right">0.0138889</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right">0.0138889</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>2</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-con-r-15"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Tabla con <span class="math inline">\(X_3=3\)</span>:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">0</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td align="right">0.0046296</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>2</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta-3"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta</h3>
<p>Sea <span class="math inline">\((X_1\ldots,X_n)\)</span> una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional discreta</strong> con conjunto de valores <span class="math inline">\((X_1\ldots,X_n)(\Omega)=\{(x_{i_1},\ldots,x_{i_n})\, i_1=1,\ldots,\ i_n=1,\ldots\}\)</span>. Entonces su <strong>función de probabilidad conjunta</strong> verifica las propiedades siguientes:</p>
<p>La suma de todos los valores de la <strong>función de probabilidad conjunta</strong> sobre el conjunto de valores siempre vale 1: <span class="math display">\[\sum_{i_1}\cdots\sum_{i_n} P_{X_1\ldots X_n}(x_{i_1},\ldots,x_{i_n})=1.\]</span></p>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta-4"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta</h3>
<p>Sea <span class="math inline">\(B\)</span> una región del espacio <span class="math inline">\(\mathbb{R}^n\)</span>. El valor de la probabilidad <span class="math inline">\(P((X_1\ldots,X_n)\in B)\)</span> se puede calcular de la forma siguiente:
<span class="math display">\[
P((X_1\ldots,X_n)\in B) =\sum_{(x_{i_1},\ldots,x_{i_n})\in B} P_{X_1\ldots X_n}(x_{i_1},\ldots,x_{i_n}).
\]</span>
O sea, la probabilidad de que la variable <span class="math inline">\(n\)</span>-dimensional coja valores en <span class="math inline">\(B\)</span> es igual a la suma de todos aquellos valores de la función de probabilidad conjunta que están en <span class="math inline">\(B\)</span>.</p>
<h3 id="propiedades-de-la-función-de-probabilidad-conjunta-5"><span class="header-section-number">2.2.6</span> Propiedades de la función de probabilidad conjunta</h3>
<p>En particular, tenemos la relación siguiente que relaciona la <strong>función de distribución conjunta</strong> con la <strong>función de probabilidad conjunta</strong>:
<span class="math display">\[
F_{X_1\ldots X_n}(x_1,\ldots,x_n)=\sum_{x_{i_1}\leq x_1,\ldots, x_{i_n}\leq x_n} P_{X_1\ldots X_n}(x_{i_1},\ldots,x_{i_n}).
\]</span>
Dicha expresión se deduce de la expresión anterior considerando <span class="math inline">\(B=(-\infty,x_1]\times\cdots\times (-\infty,x_n]\)</span>.</p>
<h3 id="ejemplo-89"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo anterior del lanzamiento de un dado tres veces</strong></p>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Comprobad usando la tabla de la función de probabilidad conjunta que la suma de todos sus valores suma 1.</p>
</div>
<p>Apliquemos la fórmula que relaciona la función de distribución conjunta con la función de probabilidad conjunta para <span class="math inline">\((x_1,x_2,x_3)=(1,2,2)\)</span>.</p>
</div>
<h3 id="ejemplo-90"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Observamos que los únicos valores <span class="math inline">\((x_{i_1},x_{i_2},x_{i_3})\in (X_1 X_2,X_3)(\Omega)\)</span> que verifican <span class="math inline">\(x_{i_1}\leq 1\)</span>, <span class="math inline">\(x_{i_2}\leq 2\)</span> y <span class="math inline">\(x_{i_3}\leq 2\)</span> son <span class="math inline">\((0,0,0)\)</span>, <span class="math inline">\((0,0,1)\)</span>, <span class="math inline">\((0,0,2)\)</span>, <span class="math inline">\((0,1,0)\)</span>, <span class="math inline">\((0,1,1)\)</span>, <span class="math inline">\((0,1,2)\)</span>, <span class="math inline">\((0,2,0)\)</span>, <span class="math inline">\((0,2,1)\)</span>, <span class="math inline">\((0,2,2)\)</span>, <span class="math inline">\((1,0,0)\)</span>, <span class="math inline">\((1,0,1)\)</span>, <span class="math inline">\((1,0,2)\)</span>, <span class="math inline">\((1,1,0)\)</span>, <span class="math inline">\((1,1,1)\)</span>, <span class="math inline">\((1,1,2)\)</span>, <span class="math inline">\((1,2,0)\)</span>, <span class="math inline">\((1,2,1)\)</span> y <span class="math inline">\((1,2,2)\)</span>:
<span class="math display">\[
\begin{array}{rl}
F_{X_1X_2X_3}(1,2,2) &amp; =P_{X_1X_2X_3}(0,0,0)+P_{X_1X_2X_3}(0,0,1)+P_{X_1X_2X_3}(0,0,2)+P_{X_1X_2X_3}(0,1,0)\\ &amp; +P_{X_1X_2X_3}(0,1,1)+P_{X_1X_2X_3}(0,1,2)+P_{X_1X_2X_3}(0,2,0)+P_{X_1X_2X_3}(0,2,1)\\ &amp; +P_{X_1X_2X_3}(0,2,2) +
P_{X_1X_2X_3}(1,0,0)+P_{X_1X_2X_3}(1,0,1)+P_{X_1X_2X_3}(1,0,2)\\ &amp; +P_{X_1X_2X_3}(1,1,0)+P_{X_1X_2X_3}(1,1,1)+
P_{X_1X_2X_3}(1,1,2)+P_{X_1X_2X_3}(1,2,0)\\ &amp; + P_{X_1X_2X_3}(1,2,1)+P_{X_1X_2X_3}(1,2,2)\\ &amp;=
\frac{1}{8}+\frac{1}{8}+\frac{1}{24}+\frac{1}{8}+\frac{1}{12}+\frac{1}{72}+\frac{1}{24}+\frac{1}{72}+0+\frac{1}{8}+\frac{1}{12}+\frac{1}{72}+\frac{1}{12}+\frac{1}{36}\\ &amp; +0+\frac{1}{72}+0+0=\frac{11}{12}=0.9167.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-con-r-16"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>La función de distribución conjunta sería en <code>R</code>:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1">fun.dis.con =<span class="st"> </span><span class="cf">function</span>(x1,x2,x3){</a>
<a class="sourceLine" id="cb99-2" data-line-number="2">  suma=<span class="dv">0</span></a>
<a class="sourceLine" id="cb99-3" data-line-number="3">  i1=<span class="dv">0</span>; i2=<span class="dv">0</span>; i3=<span class="dv">0</span>;</a>
<a class="sourceLine" id="cb99-4" data-line-number="4">  <span class="cf">while</span>(i1 <span class="op">&lt;=</span>x1 <span class="op">&amp;</span><span class="st"> </span>i1<span class="op">&lt;=</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb99-5" data-line-number="5">    <span class="cf">while</span>(i2 <span class="op">&lt;=</span><span class="st"> </span>x2 <span class="op">&amp;</span><span class="st"> </span>i2<span class="op">&lt;=</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb99-6" data-line-number="6">      <span class="cf">while</span>(i3<span class="op">&lt;=</span><span class="st"> </span>x3 <span class="op">&amp;</span><span class="st"> </span>i3 <span class="op">&lt;=</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb99-7" data-line-number="7">        suma=suma<span class="op">+</span><span class="kw">fun.prob.con</span>(i1,i2,i3); i3=i3<span class="op">+</span><span class="dv">1</span>;</a>
<a class="sourceLine" id="cb99-8" data-line-number="8">      }</a>
<a class="sourceLine" id="cb99-9" data-line-number="9">      i3=<span class="dv">0</span>; i2=i2<span class="op">+</span><span class="dv">1</span>;</a>
<a class="sourceLine" id="cb99-10" data-line-number="10">    }</a>
<a class="sourceLine" id="cb99-11" data-line-number="11">    i2=<span class="dv">0</span>; i3=<span class="dv">0</span>; i1=i1<span class="op">+</span><span class="dv">1</span>;</a>
<a class="sourceLine" id="cb99-12" data-line-number="12">  }</a>
<a class="sourceLine" id="cb99-13" data-line-number="13">  <span class="kw">return</span>(suma)</a>
<a class="sourceLine" id="cb99-14" data-line-number="14">}</a></code></pre></div>
</div>
<h3 id="ejemplo-con-r-17"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>Comprobemos que la <strong>función de distribución conjunta</strong> en el valor <span class="math inline">\((1,2,2)\)</span> nos da el mismo resultado que vimos anteriormente:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="kw">fun.dis.con</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 0.9166667</code></pre>
</div>
<h3 id="variables-aleatorias-marginales-3"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales</h3>
<p>Consideremos una variable aleatoria <strong><span class="math inline">\(n\)</span>-dimensional discreta <span class="math inline">\((X_1\ldots,X_n)\)</span></strong> con <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{X_1\ldots,X_n}(x_{i_1},\ldots,x_{i_n})\)</span>, con <span class="math inline">\((x_{i_1},\ldots,x_{i_n})\in (X_1\ldots,X_n)(\Omega)\)</span>, <span class="math inline">\(i_1=1,2,\ldots\)</span>, <span class="math inline">\(i_n=1,2,\ldots\)</span>.</p>
<p>La tabla de la <strong>función de probabilidad conjunta</strong> contiene suficiente información para obtener las <strong>funciones de probabilidad conjunta</strong> de cualquier <strong>variable aleatoria <span class="math inline">\(k\)</span>-dimensional</strong> <span class="math inline">\((X_{s_1},\ldots,X_{s_k})\)</span> donde <span class="math inline">\(\{s_1,\ldots,s_k\}\)</span> es un subconjunto de las componentes <span class="math inline">\(\{1,\ldots,n\}\)</span> de la variable aleatoria <span class="math inline">\(n\)</span>-dimensional. .</p>
<h3 id="variables-aleatorias-marginales-4"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales</h3>
<p><l class="prop">Proposición. Expresión de las funciones de probabilidad marginales. </l>
Sea <span class="math inline">\(\mathbf{X}=(X_1\ldots,X_n)\)</span> una variable aleatoria <strong><span class="math inline">\(n\)</span>-dimensional discreta</strong> con <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{X_1\ldots X_n}(x_{i_1},\ldots,x_{i_n})\)</span>, con <span class="math inline">\((x_{i_1},\ldots,x_{i_n})\in (X_1\ldots,X_n)(\Omega)\)</span>, <span class="math inline">\(i_1=1,2,\ldots\)</span>, <span class="math inline">\(i_n=1,2,\ldots\)</span>.</p>
<p>Sea <span class="math inline">\(\{s_1,\ldots,s_k\}\)</span> un subconjunto del conjunto de las componentes <span class="math inline">\(\{1,\ldots,n\}\)</span> donde suponemos que <span class="math inline">\(s_1 &lt; s_2&lt;\cdots &lt; s_k\)</span>. Entonces la <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{X_{s_1}\ldots X_{s_k}}\)</span> de la variable aleatoria <span class="math inline">\(k\)</span>-dimensional <span class="math inline">\((X_{s_1},\ldots, X_{s_k})\)</span> se calcula usando la expresión siguiente:
<span class="math display">\[
\begin{array}{rl}
P_{X_{s_1}\ldots X_{s_k}}(x_{s_1},\ldots,x_{s_k})  &amp; = \sum_{x_t} P_{X_1\ldots X_n}(\mathbf{x}_s,\mathbf{x}_t),
\end{array}
\]</span></p>
<h3 id="variables-aleatorias-marginales-5"><span class="header-section-number">2.2.6</span> Variables aleatorias marginales</h3>
<p>donde <span class="math inline">\((\mathbf{x}_s,\mathbf{x}_t)\)</span> es un valor de <span class="math inline">\(\mathbf{X}(\Omega)\)</span> tal que tiene como componente <span class="math inline">\(s_i\)</span> el valor <span class="math inline">\(x_{s_i}\)</span> para <span class="math inline">\(i=1,\ldots, k\)</span> y con <span class="math inline">\(\mathbf{x}_t\)</span> queremos decir todas las demás componentes que no son las <span class="math inline">\(s_i\)</span>. La suma tiene todos los sumandos <span class="math inline">\(\mathbf{x}_t\)</span> para los que se cumpla que <span class="math inline">\((\mathbf{x}_s,\mathbf{x}_t)\in \mathbf{X}(\Omega)\)</span>.</p>
<p>En el caso particular de <span class="math inline">\(n=3\)</span>, podemos calcular las <strong>funciones de probabilidad conjunta</strong> de las variables unidimensionales <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span> y de las variables bidimensionales <span class="math inline">\((X_1,X_2)\)</span>, <span class="math inline">\((X_1,X_3)\)</span> y <span class="math inline">\((X_2,X_3)\)</span>.</p>
<h3 id="ejemplo-91"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la variable 3-dimensional que nos da el número de 1’s, 2’s y 3’s en el lanzamiento de un dado tres veces</strong></p>
<p>Hallemos, en primer lugar, la <strong>función de probabilidad marginal</strong> de las variable <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span>. Empecemos con <span class="math inline">\(X_1\)</span>:
<span class="math display">\[
\begin{array}{rl}
P_{X_1}(0) &amp; = P_{X_1X_2X_3}(0,0,0)+P_{X_1X_2X_3}(0,0,1)+P_{X_1X_2X_3}(0,0,2)+P_{X_1X_2X_3}(0,0,3)+P_{X_1X_2X_3}(0,1,0) \\ &amp; +P_{X_1X_2X_3}(0,1,1)+P_{X_1X_2X_3}(0,1,2)+P_{X_1X_2X_3}(0,1,3)+P_{X_1X_2X_3}(0,2,0)+P_{X_1X_2X_3}(0,2,1)\\ &amp;
+P_{X_1X_2X_3}(0,2,2)+P_{X_1X_2X_3}(0,2,3)+P_{X_1X_2X_3}(0,3,0)+P_{X_1X_2X_3}(0,3,1)+P_{X_1X_2X_3}(0,3,2)\\ &amp;
P_{X_1X_2X_3}(0,3,3)=0.5787,\\
P_{X_1}(1) &amp; = P_{X_1X_2X_3}(1,0,0)+P_{X_1X_2X_3}(1,0,1)+P_{X_1X_2X_3}(1,0,2)+P_{X_1X_2X_3}(1,0,3)+P_{X_1X_2X_3}(1,1,0) \\ &amp; +P_{X_1X_2X_3}(1,1,1)+P_{X_1X_2X_3}(1,1,2)+P_{X_1X_2X_3}(1,1,3)+P_{X_1X_2X_3}(1,2,0)+P_{X_1X_2X_3}(1,2,1)\\ &amp;
+P_{X_1X_2X_3}(1,2,2)+P_{X_1X_2X_3}(1,2,3)+P_{X_1X_2X_3}(1,3,0)+P_{X_1X_2X_3}(1,3,1)+P_{X_1X_2X_3}(1,3,2)\\ &amp;
P_{X_1X_2X_3}(1,3,3)=0.3472,\\
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-92"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><span class="math display">\[
\begin{array}{rl}
P_{X_1}(2) &amp; = P_{X_1X_2X_3}(2,0,0)+P_{X_1X_2X_3}(2,0,1)+P_{X_1X_2X_3}(2,0,2)+P_{X_1X_2X_3}(2,0,3)+P_{X_1X_2X_3}(2,1,0) \\ &amp; +P_{X_1X_2X_3}(2,1,1)+P_{X_1X_2X_3}(2,1,2)+P_{X_1X_2X_3}(2,1,3)+P_{X_1X_2X_3}(2,2,0)+P_{X_1X_2X_3}(2,2,1)\\ &amp;
+P_{X_1X_2X_3}(2,2,2)+P_{X_1X_2X_3}(2,2,3)+P_{X_1X_2X_3}(2,3,0)+P_{X_1X_2X_3}(2,3,1)+P_{X_1X_2X_3}(2,3,2)\\ &amp;
P_{X_1X_2X_3}(2,3,3)=0.0694,\\
P_{X_1}(3) &amp; = P_{X_1X_2X_3}(3,0,0)+P_{X_1X_2X_3}(3,0,1)+P_{X_1X_2X_3}(3,0,2)+P_{X_1X_2X_3}(3,0,3)+P_{X_1X_2X_3}(3,1,0) \\ &amp; +P_{X_1X_2X_3}(3,1,1)+P_{X_1X_2X_3}(3,1,2)+P_{X_1X_2X_3}(3,1,3)+P_{X_1X_2X_3}(3,2,0)+P_{X_1X_2X_3}(3,2,1)\\ &amp;
+P_{X_1X_2X_3}(3,2,2)+P_{X_1X_2X_3}(3,2,3)+P_{X_1X_2X_3}(3,3,0)+P_{X_1X_2X_3}(3,3,1)+P_{X_1X_2X_3}(3,3,2)\\ &amp;
P_{X_1X_2X_3}(3,3,3)=0.0046
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-93"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>La distribución marginal de la variable <span class="math inline">\(X_1\)</span> es la siguiente:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_{X_1}\)</span></td>
<td><span class="math inline">\(0.5787\)</span></td>
<td><span class="math inline">\(0.3472\)</span></td>
<td><span class="math inline">\(0.0694\)</span></td>
<td><span class="math inline">\(0.0046\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Las distribuciones marginales de las variables <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span> coinciden con la distribución marginal de la variable <span class="math inline">\(X_1\)</span>. Lo dejamos como ejercicio.</p>
</div>
<h3 id="ejemplo-94"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>A continuación, calculemos la <strong>función de probabilidad marginal conjunta</strong> de la variable <span class="math inline">\((X_1,X_2)\)</span>:
<span class="math display">\[
\begin{array}{rl}
P_{X_1X_2}(0,0) &amp; = P_{X_1X_2X_3}(0,0,0)+P_{X_1X_2X_3}(0,0,1)+P_{X_1X_2X_3}(0,0,2)+P_{X_1X_2X_3}(0,0,3)=0.2963, \\
P_{X_1X_2}(0,1) &amp; = P_{X_1X_2X_3}(0,1,0)+P_{X_1X_2X_3}(0,1,1)+P_{X_1X_2X_3}(0,1,2)+P_{X_1X_2X_3}(0,1,3)=0.2222, \\
P_{X_1X_2}(0,2) &amp; = P_{X_1X_2X_3}(0,2,0)+P_{X_1X_2X_3}(0,2,1)+P_{X_1X_2X_3}(0,2,2)+P_{X_1X_2X_3}(0,2,3)=0.0556, \\
P_{X_1X_2}(0,3) &amp; = P_{X_1X_2X_3}(0,3,0)+P_{X_1X_2X_3}(0,3,1)+P_{X_1X_2X_3}(0,3,2)+P_{X_1X_2X_3}(0,3,3)=0.0046, \\
P_{X_1X_2}(1,0) &amp; = P_{X_1X_2X_3}(1,0,0)+P_{X_1X_2X_3}(1,0,1)+P_{X_1X_2X_3}(1,0,2)+P_{X_1X_2X_3}(1,0,3)=0.2222, \\
P_{X_1X_2}(1,1) &amp; = P_{X_1X_2X_3}(1,1,0)+P_{X_1X_2X_3}(1,1,1)+P_{X_1X_2X_3}(1,1,2)+P_{X_1X_2X_3}(1,1,3)=0.1111, \\
P_{X_1X_2}(1,2) &amp; = P_{X_1X_2X_3}(1,2,0)+P_{X_1X_2X_3}(1,2,1)+P_{X_1X_2X_3}(1,2,2)+P_{X_1X_2X_3}(1,2,3)=0.0139, \\
P_{X_1X_2}(1,3) &amp; = P_{X_1X_2X_3}(1,3,0)+P_{X_1X_2X_3}(1,3,1)+P_{X_1X_2X_3}(1,3,2)+P_{X_1X_2X_3}(1,3,3)=0, \\
P_{X_1X_2}(2,0) &amp; = P_{X_1X_2X_3}(2,0,0)+P_{X_1X_2X_3}(2,0,1)+P_{X_1X_2X_3}(2,0,2)+P_{X_1X_2X_3}(2,0,3)=0.0556, \\
P_{X_1X_2}(2,1) &amp; = P_{X_1X_2X_3}(2,1,0)+P_{X_1X_2X_3}(2,1,1)+P_{X_1X_2X_3}(2,1,2)+P_{X_1X_2X_3}(2,1,3)=0.0139, \\
P_{X_1X_2}(2,2) &amp; = P_{X_1X_2X_3}(2,2,0)+P_{X_1X_2X_3}(2,2,1)+P_{X_1X_2X_3}(2,2,2)+P_{X_1X_2X_3}(2,2,3)=0, \\
P_{X_1X_2}(2,3) &amp; = P_{X_1X_2X_3}(2,3,0)+P_{X_1X_2X_3}(2,3,1)+P_{X_1X_2X_3}(2,3,2)+P_{X_1X_2X_3}(2,3,3)=0, \\
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-95"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><span class="math display">\[
\begin{array}{rl}
P_{X_1X_2}(3,0) &amp; = P_{X_1X_2X_3}(3,0,0)+P_{X_1X_2X_3}(3,0,1)+P_{X_1X_2X_3}(3,0,2)+P_{X_1X_2X_3}(3,0,3)=0.0046, \\
P_{X_1X_2}(3,1) &amp; = P_{X_1X_2X_3}(3,1,0)+P_{X_1X_2X_3}(3,1,1)+P_{X_1X_2X_3}(3,1,2)+P_{X_1X_2X_3}(3,1,3)=0, \\
P_{X_1X_2}(3,2) &amp; = P_{X_1X_2X_3}(3,2,0)+P_{X_1X_2X_3}(3,2,1)+P_{X_1X_2X_3}(3,2,2)+P_{X_1X_2X_3}(3,2,3)=0, \\
P_{X_1X_2}(3,3) &amp; = P_{X_1X_2X_3}(3,3,0)+P_{X_1X_2X_3}(3,3,1)+P_{X_1X_2X_3}(3,3,2)+P_{X_1X_2X_3}(3,3,3)=0, \\
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-96"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>La <strong>función de probabilidad marginal conjunta</strong> de la variable <span class="math inline">\((X_1,X_2)\)</span> queda resumida en la tabla siguiente:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1/X_2\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0.2963\)</span></td>
<td><span class="math inline">\(0.2222\)</span></td>
<td><span class="math inline">\(0.0556\)</span></td>
<td><span class="math inline">\(0.0046\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(0.2222\)</span></td>
<td><span class="math inline">\(0.1111\)</span></td>
<td><span class="math inline">\(0.0139\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(2\)</span></td>
<td><span class="math inline">\(0.0556\)</span></td>
<td><span class="math inline">\(0.0139\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(3\)</span></td>
<td><span class="math inline">\(0.0046\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Las <strong>funciones de probabilidad marginales</strong> de las variables <span class="math inline">\((X_1,X_3)\)</span> y <span class="math inline">\((X_2,X_3)\)</span> dan el mismo resultado que la tabla anterior.</p>
</div>
<h3 id="ejemplo-con-r-18"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>La <strong>función de probabilidad marginal</strong> de la variable <span class="math inline">\(X_1\)</span> en <code>R</code> se halla de la forma siguiente:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1">fun.marginal.X1 =<span class="st"> </span><span class="cf">function</span>(x){</a>
<a class="sourceLine" id="cb102-2" data-line-number="2">  suma=<span class="dv">0</span>;</a>
<a class="sourceLine" id="cb102-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){suma=suma<span class="op">+</span><span class="kw">fun.prob.con</span>(x,i,j)}}</a>
<a class="sourceLine" id="cb102-4" data-line-number="4">  <span class="kw">return</span>(suma)</a>
<a class="sourceLine" id="cb102-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb102-6" data-line-number="6">tabla.fun.marginal.X1=<span class="kw">data.frame</span>(<span class="kw">fun.marginal.X1</span>(<span class="dv">0</span>),<span class="kw">fun.marginal.X1</span>(<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb102-7" data-line-number="7">                            <span class="kw">fun.marginal.X1</span>(<span class="dv">2</span>),<span class="kw">fun.marginal.X1</span>(<span class="dv">3</span>));</a>
<a class="sourceLine" id="cb102-8" data-line-number="8"><span class="kw">colnames</span>(tabla.fun.marginal.X1)=<span class="dv">0</span><span class="op">:</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb102-9" data-line-number="9">knitr<span class="op">::</span><span class="kw">kable</span>(tabla.fun.marginal.X1)</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">0</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.5787037</td>
<td align="right">0.3472222</td>
<td align="right">0.0694444</td>
<td align="right">0.0046296</td>
</tr>
</tbody>
</table>
</div>
<h3 id="ejemplo-con-r-19"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<p>La <strong>función de probabilidad marginal</strong> de la variable <span class="math inline">\((X_1,X_2)\)</span> en <code>R</code> se halla de la forma siguiente:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1">fun.marginal.X1.X2 =<span class="st"> </span><span class="cf">function</span>(x,y){</a>
<a class="sourceLine" id="cb103-2" data-line-number="2">  suma=<span class="dv">0</span>;</a>
<a class="sourceLine" id="cb103-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){suma=suma<span class="op">+</span><span class="kw">fun.prob.con</span>(x,y,i)}</a>
<a class="sourceLine" id="cb103-4" data-line-number="4">  <span class="kw">return</span>(suma)</a>
<a class="sourceLine" id="cb103-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb103-6" data-line-number="6">tabla.fun.marginal.X1.X2=<span class="kw">c</span>()</a>
<a class="sourceLine" id="cb103-7" data-line-number="7"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb103-8" data-line-number="8">  tabla.fun.marginal.X1.X2=<span class="kw">cbind</span>(tabla.fun.marginal.X1.X2,<span class="kw">c</span>(<span class="kw">fun.marginal.X1.X2</span>(i,<span class="dv">0</span>),</a>
<a class="sourceLine" id="cb103-9" data-line-number="9">                                                            <span class="kw">fun.marginal.X1.X2</span>(i,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb103-10" data-line-number="10">                                                            <span class="kw">fun.marginal.X1.X2</span>(i,<span class="dv">2</span>),</a>
<a class="sourceLine" id="cb103-11" data-line-number="11">                                                            <span class="kw">fun.marginal.X1.X2</span>(i,<span class="dv">3</span>)))}</a>
<a class="sourceLine" id="cb103-12" data-line-number="12">tabla.fun.marginal.X1.X2=<span class="kw">as.data.frame</span>(tabla.fun.marginal.X1.X2)</a>
<a class="sourceLine" id="cb103-13" data-line-number="13"><span class="kw">rownames</span>(tabla.fun.marginal.X1.X2)=<span class="dv">0</span><span class="op">:</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb103-14" data-line-number="14"><span class="kw">colnames</span>(tabla.fun.marginal.X1.X2)=<span class="dv">0</span><span class="op">:</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb103-15" data-line-number="15">knitr<span class="op">::</span><span class="kw">kable</span>(tabla.fun.marginal.X1.X2)</a></code></pre></div>
</div>
<h3 id="ejemplo-con-r-20"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">0</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td align="right">0.2962963</td>
<td align="right">0.2222222</td>
<td align="right">0.0555556</td>
<td align="right">0.0046296</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right">0.2222222</td>
<td align="right">0.1111111</td>
<td align="right">0.0138889</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td>2</td>
<td align="right">0.0555556</td>
<td align="right">0.0138889</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td>3</td>
<td align="right">0.0046296</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
</tr>
</tbody>
</table>
</div>
<h2 id="variables-aleatorias-n-dimensionales-continuas"><span class="header-section-number">2.2.6</span> Variables aleatorias <span class="math inline">\(n\)</span>-dimensionales continuas</h2>
<h3 id="introducción-11"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Recordemos la definición de <strong>variable continua bidimensional</strong>: <span class="math inline">\((X,Y)\)</span> es continua si existe una función <span class="math inline">\(f_{XY}:\mathbb{R}^2\longrightarrow \mathbb{R}\)</span>, llamada <strong>función de densidad conjunta</strong> no negativa <span class="math inline">\(f_{XY}(x,y)\geq 0\)</span>, para todo <span class="math inline">\((x,y)\in\mathbb{R}^2\)</span> tal que para cualquier región <span class="math inline">\(B\)</span> del plano, la probabilidad de que <span class="math inline">\((X,Y)\)</span> esté en <span class="math inline">\(B\)</span> se calcula de la forma siguiente:
<span class="math display">\[
P((X,Y)\in B)=\int\int_B f_{XY}(x,y)\,dx\, dy.
\]</span></p>
<h3 id="introducción-12"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>La generalización natural será, entonces:</p>
<p><l class="definition">Definición de variable aleatoria <span class="math inline">\(n\)</span>-dimensional continua. </l>
Sea <span class="math inline">\((X_1\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional. Diremos que <span class="math inline">\((X_1\ldots,X_n)\)</span> es continua si existe una función
<span class="math inline">\(f_{X_1\ldots X_n}:\mathbb{R}^n\longrightarrow \mathbb{R}\)</span> llamada <strong>función de densidad conjunta</strong> no negativa <span class="math inline">\(f_{X_1\ldots X_n}(x_1,\ldots,x_n)\geq 0\)</span> para todo <span class="math inline">\((x_1,\ldots,x_n)\in\mathbb{R}^n\)</span> tal que dado cualquier región <span class="math inline">\(B\)</span> del espacio <span class="math inline">\(n\)</span>-dimensional, la probabilidad de que <span class="math inline">\((X_1\ldots,X_n)\)</span> esté en <span class="math inline">\(B\)</span> se calcula de la forma siguiente:
<span class="math display">\[
P((X_1\ldots,X_n)\in B)=\int\cdots\int_B f_{X_1\ldots X_n}(x_1,\ldots,x_n)\,dx_1\cdots\,dx_n.
\]</span></p>
<h3 id="ejemplo-97"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos una variable aleatoria <span class="math inline">\(3\)</span>-dimensional <span class="math inline">\((X_1,X_2,X_3)\)</span> con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
8 x_1\cdot x_2\cdot x_3, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
0, &amp; \mbox{en caso contrario.}\\
\end{cases}
\]</span></p>
<p>En la figura siguiente hemos dibujado en rosa la región donde <span class="math inline">\(f_{X_1X_2X_3}\)</span> no es cero, o sea <span class="math inline">\([0,1]\times [0,1]\times [0,1]\)</span>.</p>
</div>
<h3 id="ejemplo-98"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Cubo3D2.png" width="900px" /></p>
</div>
<h3 id="propiedades-de-la-función-de-densidad-3"><span class="header-section-number">2.2.6</span> Propiedades de la función de densidad</h3>
<p>Sea <span class="math inline">\((X_1\ldots,X_n)\)</span> una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional continua</strong> con <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{X_1\ldots X_n}\)</span>. Entonces dicha función verifica las propiedades siguientes:</p>
<ul>
<li>La integral de dicha función sobre todo el espacio <span class="math inline">\(n\)</span>-dimensional vale 1:
<span class="math display">\[
\int\int_{\mathbb{R}^n} f_{X_1\ldots X_n}(x_1,\ldots,x_n)\,dx_1\cdots dx_n =1.
\]</span>
Para ver dicha propiedad, basta considerar <span class="math inline">\(B=\mathbb{R}^n\)</span>, tener en cuenta que el suceso <span class="math inline">\((X_1\ldots,X_n)\in \mathbb{R}^n\)</span> es el total <span class="math inline">\(\Omega\)</span> y aplicar la definición de <span class="math inline">\(f_{X_1\ldots X_n}\)</span>:
<span class="math display">\[
P((X_1\ldots,X_n)\in \mathbb{R}^n)=1= \int\cdots\int_{\mathbb{R}^n} f_{X_1\ldots X_n}(x_1,\ldots,x_n)\,dx_1\cdots dx_n.
\]</span></li>
</ul>
<h3 id="propiedades-de-la-función-de-densidad-4"><span class="header-section-number">2.2.6</span> Propiedades de la función de densidad</h3>
<ul>
<li>La relación que hay entre la <strong>función de distribución conjunta</strong> <span class="math inline">\(F_{X_1\ldots X_n}\)</span> y la <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{X_1\ldots X_n}\)</span> es la siguiente:
<span class="math display">\[
F_{X_1\ldots X_n}(x_1,\ldots,x_n)=\int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_n} f_{X_1\ldots X_n}(u_1,\ldots,u_n)\,du_1\cdots du_n.
\]</span>
Para ver dicha propiedad, basta considerar <span class="math inline">\(B=(-\infty,x_1]\times\cdots\times (-\infty,x_n]\)</span> y aplicar la definición de <strong>función de distribución conjunta</strong>:
<span class="math display">\[
\begin{array}{rl}
&amp; F_{X_1\ldots X_n}(x_1,\ldots,x_n)=P((X_1\ldots,X_n)\in (-\infty,x_1]\times\cdots (-\infty,x_n])\\ &amp;\qquad =\int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_n} f_{X_1\ldots X_n}(u_1,\ldots,u_n)\,du_1\cdots du_n.
\end{array}
\]</span></li>
</ul>
<h3 id="propiedades-de-la-función-de-densidad-5"><span class="header-section-number">2.2.6</span> Propiedades de la función de densidad</h3>
<ul>
<li>La relación que hay entre la <strong>función de densidad</strong> <span class="math inline">\(F_{X_1\ldots X_n}\)</span> y la <strong>función de distribución</strong> <span class="math inline">\(f_{X_1\ldots X_n}\)</span> es la siguiente:
<span class="math display">\[
f_{X_1\ldots X_n}(x_1,\ldots,x_n)=\frac{\partial^n F_{X_1\ldots X_n}(x_1,\ldots,x_n)}{\partial x_1\cdots\partial x_n}.
\]</span>
Dicha propiedad se deduce de la anterior, derivando primero respecto a <span class="math inline">\(x_1\)</span>, después respecto a <span class="math inline">\(x_2\)</span> y sucesivamente hasta llegar a <span class="math inline">\(x_n\)</span> para eliminar las <span class="math inline">\(n\)</span> integrales.</li>
</ul>
<h3 id="propiedades-de-la-función-de-densidad-6"><span class="header-section-number">2.2.6</span> Propiedades de la función de densidad</h3>
<ul>
<li>La <strong>función de densidad marginal</strong> de la variable <span class="math inline">\(k\)</span> dimensional <span class="math inline">\((X_{s_1},\ldots,X_{s_k})\)</span> con <span class="math inline">\(\{s_1,\ldots, s_k\}\)</span> un subconjunto de <span class="math inline">\(\{1,\ldots,n\}\)</span>, <span class="math inline">\(f_{X_{s_1}\ldots,X_{s_k}}\)</span> se calculan de la forma siguiente:
<span class="math display">\[
f_{X_{s_1}\ldots,X_{s_k}}(x_{s_1},\ldots,x_{s_k})=\int_{x_{t_1}=-\infty}^{x_{t_1}=\infty}\cdots \int_{x_{t_{n-k}}=-\infty}^{x_{t_{n-k}}=\infty} f_{X_1\ldots X_n}(x_1,\ldots,x_n)\, dx_{t_1}\cdots dx_{t_{n-k}},
\]</span>
con <span class="math inline">\(\{t_1,\ldots,t_{n-k}\}=\{1,\ldots,n\}\setminus \{s_1,\ldots,s_k\}.\)</span> O sea, las variables <span class="math inline">\(t\)</span>’s son las que no aparecen en la definición de la variable aleatoria <span class="math inline">\(k\)</span> dimensional <span class="math inline">\((X_{s_1},\ldots,X_{s_k})\)</span>.</li>
</ul>
<h3 id="ejemplo-99"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo anterior</strong></p>
<p>Comprobemos las propiedades usando la <strong>función de densidad</strong> del ejemplo anterior:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
8 x_1\cdot x_2\cdot x_3, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
0, &amp; \mbox{en caso contrario.}\\
\end{cases}
\]</span></p>
<p>La integral de <span class="math inline">\(f_{X_1X_2X_3}\)</span> sobre todo el espacio 3D vale 1:
<span class="math display">\[
\begin{array}{rl}
&amp; \int\int\int_{\mathbb{R}^3} f_{X_1X_2X_3}(x_1,x_2,x_3)\,dx\, dy=\int_0^1\int_0^1\int_0^1 8 x_1\cdot x_2\cdot x_3\, dx_1\,dx_2\,dx_3\\ &amp; \qquad=8\int_0^1 x_1\, dx_1\int_0^1 x_2\, dx_2\int_0^1 x_3\,dx_3=8\left[\frac{x_1^2}{2}\right]_0^1\cdot\left[\frac{x_2^2}{2}\right]_0^1\cdot \left[\frac{x_3^2}{2}\right]_0^1=8\cdot\left(\frac{1}{2}\right)^3 =1.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-100"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Vamos a calcular la función de distribución <span class="math inline">\(F_{X_1X_2X_3}\)</span>.</p>
<p>Recordemos que la expresión de la función de distribución en función de la función de densidad era:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\int_{-\infty}^{x_1}\int_{-\infty}^{x_2}\int_{-\infty}^{x_3}f_{X_1X_2X_3}(u_1,u_2,u_3)\,du_1\, du_2\, du_3.
\]</span>
Como la región del espacio 3D donde <span class="math inline">\(f_{X_1X_2X_3}(x_1,x_2,x_3)\)</span> es no nula es el cubo unidad <span class="math inline">\([0,1]\times [0,1]\times [0,1]\)</span>, fijado un punto del espacio <span class="math inline">\((x_1,x_2,x_3)\)</span> será fundamental calcular la intersección de dicho cubo unidad con la región <span class="math inline">\((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3]\)</span>.</p>
<p>Dicha intersección <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])\)</span> será la región donde tendremos que integrar la función de densidad para hallar la función de distribución en el punto <span class="math inline">\((x_1,x_2,x_3)\)</span>.</p>
<p>Para hallar la región anterior, vamos a dividir el espacio 3D en tres “pisos”:</p>
<ul>
<li>“Sótano” o zona donde <span class="math inline">\(x_3&lt;0\)</span>.</li>
<li>“Planta baja” o zona donde <span class="math inline">\(0\leq x_3\leq 1\)</span>.</li>
<li>“Primer piso” o zona donde <span class="math inline">\(x_3&gt;1\)</span>.</li>
</ul>
</div>
<h3 id="ejemplo-101"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<ul>
<li><p>Si <span class="math inline">\((x_1,x_2,x_3)\)</span> está en el “sótano” o <span class="math inline">\(x_3&lt;0\)</span>, claramente, <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=\emptyset\)</span>. Por tanto, <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\)</span>.</p></li>
<li><p>Si <span class="math inline">\((x_1,x_2,x_3)\)</span> está en la planta baja o <span class="math inline">\(0\leq x_3\leq 1\)</span>, vamos a distinguir cuatro casos dependiendo de los valores de <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>:</p>
<ul>
<li><span class="math inline">\(x_1 &lt;0\)</span> o <span class="math inline">\(x_2 &lt;0\)</span>. En este caso, <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=\emptyset\)</span>. Por tanto, <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\)</span>.</li>
<li><span class="math inline">\(0\leq x_1\leq 1\)</span> y <span class="math inline">\(0\leq x_2\leq 1\)</span>. En este caso: <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=[0,x_1]\times [0,x_2]\times [0,x_3]\)</span>, ver figura adjunta.</li>
</ul></li>
</ul>
<p>Por tanto,
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\int_{0}^{x_1}\int_{0}^{x_2}\int_{0}^{x_3} 8 x_1 x_2 x_3 dx_1\, dx_2\ dx_3 = 
8\left[\frac{x_1^2}{2}\right]_0^{x_1}\left[\frac{x_2^2}{2}\right]_0^{x_2}\left[\frac{x_3^2}{2}\right]_0^{x_3} = x_1^2 x_2^2 x_3^2.
\]</span></p>
</div>
<h3 id="ejemplo-102"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Fx1x2x3bajos.png" width="800px" /></p>
</div>
<h3 id="ejemplo-103"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Seguimos en la planta “baja”,</p>
<ul>
<li>Si <span class="math inline">\(x_1 &gt;1\)</span> y <span class="math inline">\(0\leq x_2\leq 1\)</span>, <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=[0,1]\times [0,x_2]\times [0,x_3]\)</span>, ver figura adjunta. Hemos dibujado sólo la parte “positiva” de la región <span class="math inline">\((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3]\)</span> ya que la parte “negativa” claramente no interseca con <span class="math inline">\([0,1]\times [0,1]\times [0,1]\)</span> para no complicar demasiado la figura.</li>
</ul>
<p>En este caso,
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\int_{0}^{1}\int_{0}^{x_2}\int_{0}^{x_3} 8 x_1 x_2 x_3 dx_1\, dx_2\ dx_3 = 
8\left[\frac{x_1^2}{2}\right]_0^{1}\left[\frac{x_2^2}{2}\right]_0^{x_2}\left[\frac{x_3^2}{2}\right]_0^{x_3} = x_2^2 x_3^2.
\]</span></p>
<ul>
<li>Si <span class="math inline">\(0\leq x_1\)</span> y <span class="math inline">\(x_2&gt;1\)</span>, sería un caso parecido al caso anterior pero “cambiando los papeles” de <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>.
Por tanto,
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2 x_3^2.
\]</span></li>
</ul>
</div>
<h3 id="ejemplo-104"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Fx1x2x3bajosx1.png" width="800px" /></p>
</div>
<h3 id="ejemplo-105"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Seguimos en la planta “baja”,</p>
<ul>
<li>Si <span class="math inline">\(x_1&gt;1\)</span> y <span class="math inline">\(x_2&gt;1\)</span>, <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=[0,1]\times [0,1]\times [0,x_3]\)</span>, ver figura adjunta. También hemos dibujado sólo la parte “positiva” de la región <span class="math inline">\((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3]\)</span> ya que la parte “negativa” claramente no interseca con <span class="math inline">\([0,1]\times [0,1]\times [0,1]\)</span> para no complicar demasiado la figura.</li>
</ul>
<p>En este caso,
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\int_{0}^{1}\int_{0}^{1}\int_{0}^{x_3} 8 x_1 x_2 x_3 dx_1\, dx_2\ dx_3 = 
8\left[\frac{x_1^2}{2}\right]_0^{1}\left[\frac{x_2^2}{2}\right]_0^{1}\left[\frac{x_3^2}{2}\right]_0^{x_3} = x_3^2.
\]</span></p>
</div>
<h3 id="ejemplo-106"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Fx1x2x3bajosx1x2.png" width="800px" /></p>
</div>
<h3 id="ejemplo-107"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Supongamos ahora que <span class="math inline">\((x_1,x_2,x_3)\)</span> está en el “primer piso” o <span class="math inline">\(x_3&gt;1\)</span>. Aquí también vamos a distinguir 4 casos:</p>
<ul>
<li><p><span class="math inline">\(x_1 &lt;0\)</span> o <span class="math inline">\(x_2 &lt;0\)</span>. En este caso, <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=\emptyset\)</span>. Por tanto, <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\)</span>.</p></li>
<li><p><span class="math inline">\(0\leq x_1\leq 1\)</span> y <span class="math inline">\(0\leq x_2\leq 1\)</span>. En este caso: <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=[0,x_1]\times [0,x_2]\times [0,1]\)</span>, ver figura adjunta.</p></li>
</ul>
<p>En este caso,
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\int_{0}^{x_1}\int_{0}^{x_2}\int_{0}^{1} 8 x_1 x_2 x_3 dx_1\, dx_2\ dx_3 = 
8\left[\frac{x_1^2}{2}\right]_0^{x_1}\left[\frac{x_2^2}{2}\right]_0^{x_2}\left[\frac{x_3^2}{2}\right]_0^{1} = x_1^2 x_2^2.
\]</span></p>
</div>
<h3 id="ejemplo-108"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Fx1x2x3piso.png" width="800px" /></p>
</div>
<h3 id="ejemplo-109"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Seguimos en el “primer piso”,</p>
<ul>
<li>Si <span class="math inline">\(x_1 &gt;1\)</span> y <span class="math inline">\(0\leq x_2\leq 1\)</span>, <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=[0,1]\times [0,x_2]\times [0,1]\)</span>, ver figura adjunta.</li>
</ul>
<p>En este caso,</p>
<p><span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\int_{0}^{1}\int_{0}^{x_2}\int_{0}^{1} 8 x_1 x_2 x_3 dx_1\, dx_2\ dx_3 = 
8\left[\frac{x_1^2}{2}\right]_0^{1}\left[\frac{x_2^2}{2}\right]_0^{x_2}\left[\frac{x_3^2}{2}\right]_0^{1} = x_2^2.
\]</span></p>
<ul>
<li>Si <span class="math inline">\(0\leq x_1 \leq 1\)</span> y $ x_2&gt; 1$. Este caso sería parecido al caso anterior pero “cambiando los papeles” de <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>.
Por tanto,
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2.
\]</span></li>
</ul>
</div>
<h3 id="ejemplo-110"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Fx1x2x3pisox1.png" width="800px" /></p>
</div>
<h3 id="ejemplo-111"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Seguimos en el “primer piso”,</p>
<ul>
<li>Si <span class="math inline">\(x_1&gt;1\)</span> y <span class="math inline">\(x_2&gt;1\)</span>, <span class="math inline">\(([0,1]\times [0,1]\times [0,1])\cap ((-\infty,x_1]\times (-\infty,x_2]\times (-\infty,x_3])=[0,1]\times [0,1]\times [0,1]\)</span>, ver figura adjunta.</li>
</ul>
<p>En este caso,
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\int_{0}^{1}\int_{0}^{1}\int_{0}^{1} 8 x_1 x_2 x_3 dx_1\, dx_2\ dx_3 = 
8\left[\frac{x_1^2}{2}\right]_0^{1}\left[\frac{x_2^2}{2}\right]_0^{1}\left[\frac{x_3^2}{2}\right]_0^{1} = 1.
\]</span></p>
</div>
<h3 id="ejemplo-112"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="center">
<p><img src="Images/Fx1x2x3pisox1x2.png" width="800px" /></p>
</div>
<h3 id="ejemplo-113"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>En resumen:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
0, &amp; \mbox{si }x_1&lt;0,\mbox{ o }x_2&lt;0,\mbox{ o }x_3 &lt;0\\
x_1^2\cdot x_2^2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
 x_2^2\cdot x_3^2, &amp; \mbox{si }x_1&gt; 1,\ 0\leq x_2\leq  1,\ 0\leq x_3\leq  1, \\
 x_1^2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 x_3^2, &amp; \mbox{si }x_1&gt; 1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 x_1^2\cdot x_2^2, &amp; \mbox{si }0\leq x_1\leq  1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
 x_1^2, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2 &gt;  1,\ x_3&gt; 1,\\
 x_2^2, &amp; \mbox{si }x_1&gt;1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
1, &amp; \mbox{si }x_1\geq 1,\ x_2\geq 1,\ x_3\geq 1.
\end{cases}
\]</span></p>
<p>Dicha función era la función que nos sirvió como ejemplo a la hora de introducir la variables aleatorias <span class="math inline">\(n\)</span>-dimensionales. Ahora sabemos que es continua y conocemos su función de densidad.</p>
</div>
</div>
<h3 id="ejemplo-114"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Comprobemos seguidamente que si derivamos tres veces la expresión de <span class="math inline">\(F_{X_1X_2X_3}\)</span>, primero respecto <span class="math inline">\(x_1\)</span>, luego respecto <span class="math inline">\(x_2\)</span> y finalmente respecto <span class="math inline">\(x_3\)</span>, obtendremos la <strong>función de densidad</strong> <span class="math inline">\(f_{X_1X_2X_3}\)</span>.</p>
<p>Si derivamos respecto <span class="math inline">\(x_1\)</span> obtenemos:
<span class="math display">\[
\frac{\partial F_{X_1X_2X_3}(x_1,x_2,x_3)}{\partial x_1}=\begin{cases}
0, &amp; \mbox{si }x_1&lt;0,\mbox{ o }x_2&lt;0,\mbox{ o }x_3 &lt;0\\
2 x_1\cdot x_2^2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
 0, &amp; \mbox{si }x_1&gt; 1,\ 0\leq x_2\leq  1,\ 0\leq x_3\leq  1, \\
 2 x_1 \cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 0, &amp; \mbox{si }x_1&gt; 1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 2 x_1\cdot x_2^2, &amp; \mbox{si }0\leq x_1\leq  1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
 2 x_1, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2 &gt;  1,\ x_3&gt; 1,\\
 0, &amp; \mbox{si }x_1&gt;1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
0, &amp; \mbox{si }x_1\geq 1,\ x_2\geq 1,\ x_3\geq 1.
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-115"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Si ahora derivamos respecto <span class="math inline">\(x_2\)</span> obtenemos:
<span class="math display">\[
\frac{\partial^2 F_{X_1X_2X_3}(x_1,x_2,x_3)}{\partial x_2\partial x_1}=\begin{cases}
0, &amp; \mbox{si }x_1&lt;0,\mbox{ o }x_2&lt;0,\mbox{ o }x_3 &lt;0\\
4 x_1\cdot x_2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
 0, &amp; \mbox{si }x_1&gt; 1,\ 0\leq x_2\leq  1,\ 0\leq x_3\leq  1, \\
 0, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 0, &amp; \mbox{si }x_1&gt; 1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 4 x_1\cdot x_2, &amp; \mbox{si }0\leq x_1\leq  1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
 0, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2 &gt;  1,\ x_3&gt; 1,\\
 0, &amp; \mbox{si }x_1&gt;1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
0, &amp; \mbox{si }x_1\geq 1,\ x_2\geq 1,\ x_3\geq 1.
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-116"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Por último, si derivamos respecto <span class="math inline">\(x_3\)</span>, obtenemos:
<span class="math display">\[
\frac{\partial^3 F_{X_1X_2X_3}(x_1,x_2,x_3)}{\partial x_3\partial x_2\partial x_1}=\begin{cases}
0, &amp; \mbox{si }x_1&lt;0,\mbox{ o }x_2&lt;0,\mbox{ o }x_3 &lt;0\\
8 x_1\cdot x_2\cdot x_3, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
 0, &amp; \mbox{si }x_1&gt; 1,\ 0\leq x_2\leq  1,\ 0\leq x_3\leq  1, \\
 0, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 0, &amp; \mbox{si }x_1&gt; 1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 0, &amp; \mbox{si }0\leq x_1\leq  1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
 0, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2 &gt;  1,\ x_3&gt; 1,\\
 0, &amp; \mbox{si }x_1&gt;1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
0, &amp; \mbox{si }x_1\geq 1,\ x_2\geq 1,\ x_3\geq 1,
\end{cases}
\]</span>
expresión que coincide con la función de densidad <span class="math inline">\(f_{X_1X_2X_3}(x_1,x_2,x_3)\)</span>.</p>
</div>
<h3 id="ejemplo-117"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Acabemos el ejemplo calculando las <strong>funciones de densidad marginales</strong> de las variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, <span class="math inline">\((X_1,X_2)\)</span>, <span class="math inline">\((X_1,X_3)\)</span>, <span class="math inline">\((X_2,X_3)\)</span>.</p>
<p>Debido a la simetría de la región donde <span class="math inline">\(f_{X_1X_2X_3}(x_1,x_2,x_3)\)</span> no se anula, es suficiente calcular la función de densidad marginal para las variables <span class="math inline">\(X_1\)</span> y <span class="math inline">\((X_1,X_2)\)</span>. Para ver las demás, basta cambiar los “papeles” de las variables correspondientes. Por ejemplo, la función de densidad de la variable <span class="math inline">\(X_2\)</span> será la misma que la de la variable <span class="math inline">\(X_1\)</span> cambiando <span class="math inline">\(x_1\)</span> por <span class="math inline">\(x_2\)</span>.</p>
</div>
<h3 id="ejemplo-118"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Para hallar la función de densidad de la variable <span class="math inline">\(X_1\)</span>, aplicamos la fórmula vista anteriormente:
<span class="math display">\[
f_{X_1}(x_1)=\int_{-\infty}^\infty\int_{-\infty}^\infty  f_{X_1X_2X_3}(x_1,x_2,x_3)\, dx_2\, dx_3.
\]</span>
Recordemos que la región donde no se anulaba la <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{X_1X_2X_3}\)</span> era el cubo <span class="math inline">\([0,1]\times [0,1]\times [0,1]\)</span>. Por tanto, fijado <span class="math inline">\(x_1\)</span>, el valor de <span class="math inline">\(f_{X_1}(x_1)\)</span> será no nulo si el plano “vertical” <span class="math inline">\(X_1=x_1\)</span> interseca dicho cubo. Y esto ocurre siempre que <span class="math inline">\(x_1\in (0,1)\)</span>. Por tanto,
<span class="math display">\[
f_{X_1}(x_1)=\begin{cases}
\int_{0}^1\int_0^1 8 x_1x_2 x_3  \, dx_2\, dx_3=8x_1\left[\frac{x_2^2}{2}\right]_0^1 \left[\frac{x_3^2}{2}\right]_0^1 =2 x_1, &amp; \mbox{ si }x_1\in (0,1),\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-119"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Para hallar la función de densidad conjunta de la variable <span class="math inline">\((X_1,X_2)\)</span>, aplicamos la expresión siguiente:
<span class="math display">\[
f_{X_1X_2}(x_1,x_2)=\int_{-\infty}^\infty  f_{X_1X_2X_3}(x_1,x_2,x_3)\, dx_3.
\]</span>
En este caso, fijado <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>, tenemos que ver cuando la recta “vertical” <span class="math inline">\(X_1=x_1\)</span>, <span class="math inline">\(X_2=x_2\)</span> intersecta el cubo <span class="math inline">\([0,1]\times [0,1]\times [0,1]\)</span> y esto ocurre siempre que <span class="math inline">\((x_1,x_2)\in [0,1]\times [0,1]\)</span>.
Por tanto,</p>
<p><span class="math display">\[
f_{X_1X_2}(x_1,x_2)=\begin{cases}
\int_{0}^1 8 x_1x_2 x_3  \, dx_3=8x_1x_2 \left[\frac{x_3^2}{2}\right]_0^1 =4 x_1 x_2, &amp; \mbox{ si }(x_1,x_2)\in [0,1]\times [0,1],\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
</div>
<h3 id="la-distribución-gaussiana-n-dimensional"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>Vamos a generalizar la distribución normal a <span class="math inline">\(n\)</span> dimensiones.</p>
<p><l class="definition">Definición de distribución gaussiana <span class="math inline">\(n\)</span>-dimensional. </l>
Diremos que la distribución de la variable aleatoria <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\((X_1\ldots,X_n)\)</span> es <strong>gaussiana <span class="math inline">\(n\)</span>-dimensional</strong> dependiendo del <strong>vector de medias</strong> <span class="math inline">\(\mathbf{\mu}\)</span> y de la <strong>matriz de covarianzas</strong> <span class="math inline">\(\Sigma\)</span> si su <strong>función de densidad conjunta</strong> es:
<span class="math display">\[
\begin{array}{rl}
&amp; f_{X_1\ldots X_n}(x_1,\ldots,x_n)=\frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{\mathbf{|\Sigma|}}}\mathrm{e}^{-\frac{1}{2}(\mathbf{x-\mu})^\top\mathbf{\Sigma}^{-1}(\mathbf{x-\mu})},\\ &amp; \qquad  -\infty &lt;x_1,\ldots,x_n&lt;\infty,
\end{array}
\]</span>
Se denota <span class="math inline">\(\mathbf{X}=(X_1,\ldots,X_n)\)</span> con <span class="math inline">\(\mathbf{X}={\cal N}(\mathbf{\mu},\mathbf{\Sigma})\)</span>.</p>
<h3 id="la-distribución-gaussiana-n-dimensional-1"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>donde <span class="math inline">\(\mathbf{x}=\begin{pmatrix}x_1\\\vdots\\ x_n\end{pmatrix}\)</span>, <span class="math inline">\(\mathbf{\mu}=\begin{pmatrix}\mu_1\\\vdots\\ \mu_n\end{pmatrix}\)</span> es el <strong>vector de medias</strong> de cada variable aleatoria <span class="math inline">\(X_1,\ldots, X_n\)</span> y <span class="math inline">\(\mathbf{\Sigma}\)</span> es la denominada <strong>matriz de covarianzas</strong> que nos sirve para estudiar la relación lineal entre las variables <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1,\ldots, n\)</span>.</p>
<p>De hecho la componente <span class="math inline">\((i,j)\)</span> de la <strong>matriz de covarianzas</strong> <span class="math inline">\(\mathbf{\Sigma}\)</span>, <span class="math inline">\(\sigma_{ij}\)</span> es la <strong>covarianza</strong> entre las variables <span class="math inline">\(X_i\)</span> y <span class="math inline">\(X_j\)</span>.</p>
<p>Por tanto, los elementos de la diagonal de la <strong>matriz de covarianzas</strong> <span class="math inline">\(\mathbf{\Sigma}\)</span>, <span class="math inline">\(\sigma_{ii}\)</span>, serán las varianzas de las variables <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>.</p>
<h3 id="la-distribución-gaussiana-n-dimensional-2"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>Propiedades de la <strong>función de densidad de la variable gaussiana <span class="math inline">\(n\)</span>-dimensional</strong>:</p>
<ul>
<li><p>Para cualquier punto <span class="math inline">\((x_1,\ldots,x_n)\in\mathbb{R}^n\)</span>, la <strong>función de densidad</strong> es no nula: <span class="math inline">\(f_{X_1\ldots X_n}(x_1,\ldots,x_n)&gt;0\)</span>.</p></li>
<li><p>La <strong>función de densidad</strong> tiene un único máximo absoluto en el punto <span class="math inline">\(\mathbf{\mu}\)</span> que vale <span class="math inline">\(f_{X_1\ldots X_n}(\mathbf{\mu})=\frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{\mathbf{|\Sigma|}}}\)</span>.</p></li>
</ul>
<h3 id="la-distribución-gaussiana-n-dimensional-3"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>Antes de estudiar cómo son las distribuciones de las <strong>marginales</strong> de una distribución <strong>normal <span class="math inline">\(n\)</span>-dimensional</strong>, enunciemos el resultado siguiente:</p>
<p><l class="prop">Proposición (transformación afín de una <strong>normal <span class="math inline">\(n\)</span>-dimensional</strong>)</l>
Sea <span class="math inline">\(\mathbf{X}={\cal N}(\mathbf{\mu},\mathbf{\Sigma})\)</span> una distribución <strong>normal <span class="math inline">\(n\)</span>-dimensional</strong>. Sea la variable <span class="math inline">\(k\)</span>-dimensional <span class="math inline">\(\mathbf{Y}\)</span> construida como <span class="math inline">\(\mathbf{Y}=\mathbf{c}+\mathbf{C}\mathbf{X}\)</span>, con <span class="math inline">\(\mathbf{C}\)</span> una matriz <span class="math inline">\(k\times n\)</span> y <span class="math inline">\(\mathbf{c}\)</span> un vector <span class="math inline">\(k\)</span>-dimensional. Entonces la variable <span class="math inline">\(Y\)</span> se distribuye como una variable <strong>normal <span class="math inline">\(k\)</span>-dimensional</strong> de media <span class="math inline">\(\mathbf{\mu}_{\mathbf{Y}}=\mathbf{c}+\mathbf{C}\mathbf{\mu}\)</span> y matriz de covarianzas <span class="math inline">\(\mathbf{\Sigma}_{\mathbf{Y}}=\mathbf{C}\mathbf{\Sigma}\mathbf{C}^\top\)</span>: <span class="math inline">\(\mathbf{Y}={\cal N}(\mathbf{c}+\mathbf{C}\mathbf{\mu},\mathbf{C}\mathbf{\Sigma}\mathbf{C}^\top)\)</span>.</p>
<h3 id="la-distribución-gaussiana-n-dimensional-4"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>Usando la proposición anterior, podemos afirmar:</p>
<p><l class="prop">Proposición (distribución marginal de una <strong>variable normal <span class="math inline">\(n\)</span>-dimensional</strong>)</l>
Sea <span class="math inline">\(\mathbf{X}={\cal N}(\mathbf{\mu},\mathbf{\Sigma})\)</span> una distribución <strong>normal <span class="math inline">\(n\)</span>-dimensional</strong>. Sea <span class="math inline">\((X_{s_1},\ldots,X_{s_k})\)</span> la variable <span class="math inline">\(k\)</span> dimensional con las componentes <span class="math inline">\(s_1,\ldots,s_k\)</span>, con <span class="math inline">\(s_i\in\{1,\ldots,n\}\)</span>, entonces la variable <span class="math inline">\((X_{s_1},\ldots,X_{s_k})\)</span> se distribuye según una <strong>normal <span class="math inline">\(k\)</span>-dimensional</strong> de media <span class="math inline">\((\mu_{s_1},\ldots,\mu_{s_k})\)</span> y matriz de covarianzas <span class="math inline">\(\mathbf{\Sigma&#39;}\)</span> formada por las <span class="math inline">\(s_1,\ldots,s_k\)</span> filas y columnas de la matriz de covarianzas de la variable <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{\Sigma}\)</span>.</p>
<h3 id="la-distribución-gaussiana-n-dimensional-5"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>Para ver la proposición anterior a partir de la proposición de la transformación afín, hagamos un ejemplo concreto:</p>
<div class="example">
<p>Consideremos una variable normal <span class="math inline">\(5\)</span>-dimensional de <strong>vector de medias</strong> general <span class="math inline">\(\mathbf{\mu}=(\mu_1,\mu_2,\mu_3,\mu_4,\mu_5)\)</span> y <strong>matriz de covarianzas</strong>
<span class="math display">\[
\mathbf{\Sigma}=\begin{pmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \sigma_{13} &amp; \sigma_{14} &amp; \sigma_{15} \\
\sigma_{21} &amp; \sigma_{22} &amp; \sigma_{23} &amp; \sigma_{24} &amp; \sigma_{25} \\
\sigma_{31} &amp; \sigma_{32} &amp; \sigma_{33} &amp; \sigma_{34} &amp; \sigma_{35} \\
\sigma_{41} &amp; \sigma_{42} &amp; \sigma_{43} &amp; \sigma_{44} &amp; \sigma_{45} \\
\sigma_{51} &amp; \sigma_{52} &amp; \sigma_{53} &amp; \sigma_{54} &amp; \sigma_{55} 
\end{pmatrix}
\]</span></p>
</div>
<h3 id="la-distribución-gaussiana-n-dimensional-6"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<div class="example">
<p>Queremos estudiar cuál es la distribución de la variable <span class="math inline">\(3\)</span>-dimensional <span class="math inline">\((X_2,X_4,X_5)\)</span>. Para ello consideramos el vector <span class="math inline">\(\mathbf{c}=0\)</span> y la matriz <span class="math inline">\(\mathbf{C}\)</span> siguiente:
<span class="math display">\[
\mathbf{C}=\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
\end{pmatrix}
\]</span>
O sea, <span class="math inline">\(\mathbf{C}\)</span> es una matriz <span class="math inline">\(3\times 5\)</span> que vale 1 en los lugares <span class="math inline">\((1,2)\)</span>, <span class="math inline">\((2,4)\)</span> y <span class="math inline">\((3,5)\)</span>. Fijémonos que las segundas componentes de los lugares anteriores son precisamente las componentes elegidas de la variable <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>La matriz <span class="math inline">\(\mathbf{Y}=\mathbf{C}\mathbf{X}=\begin{pmatrix}X_2\\X_4\\X_5\end{pmatrix}\)</span> vale precisamente la variable marginal que queremos estudiar.</p>
</div>
<h3 id="la-distribución-gaussiana-n-dimensional-7"><span class="header-section-number">2.2.6</span> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<div class="example">
<p>Aplicando la proposición de la transformación afín, podemos afirmar que la distribución de la variable <span class="math inline">\(\mathbf{Y}=\begin{pmatrix}X_2\\X_4\\X_5\end{pmatrix}\)</span> es una normal <span class="math inline">\(3\)</span> dimensional de <strong>vector de medias</strong> <span class="math inline">\(\mu_{\mathbf{Y}}=\mathbf{C}\mathbf{\mu}=\begin{pmatrix}\mu_2\\\mu_4\\\mu_5\end{pmatrix}\)</span> y vector de covarianzas
<span class="math display">\[
\mathbf{\Sigma&#39;}=\mathbf{C}\mathbf{\Sigma}\mathbf{C}^\top = \begin{pmatrix}\sigma_{22} &amp; \sigma_{24} &amp; \sigma_{25}\\ \sigma_{42} &amp; \sigma_{44} &amp; \sigma_{45} \\  \sigma_{52} &amp; \sigma_{54} &amp; \sigma_{55}\end{pmatrix}, 
\]</span>
tal como indica la última proposición sobre distribuciones marginales.</p>
</div>
<h2 id="independencia-de-variables-aleatorias-1"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias</h2>
<h3 id="independencia-de-variables-aleatorias-discretas-2"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias discretas</h3>
<p>La generalización de <strong>independencia</strong> a variables aleatorias <span class="math inline">\(n\)</span>-dimensionales es clara:</p>
<p><l class="definition">Definición de independencia para variables aleatorias <span class="math inline">\(n\)</span>-dimensionales discretas. </l>
Sean <span class="math inline">\((X_1\ldots,X_n)\)</span> una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional discreta</strong> con <span class="math inline">\((X_1\ldots,X_n)(\Omega)=\{(x_{i_1},\ldots,x_{i_n}),\ i_1=1,2,\ldots,i_n=1,2,\ldots\}\)</span> y <strong>función de probabilidad conjunta</strong> <span class="math inline">\(P_{X_1\ldots X_n}\)</span> y <strong>funciones de probabilidad marginales</strong> <span class="math inline">\(P_{X_1},\ldots P_{X_n}\)</span>. Entonces <span class="math inline">\(X_1,\ldots X_n\)</span> son independientes si:
<span class="math display">\[
P_{X_1\ldots X_n}(x_{i_1},\ldots,x_{i_n})=P_{X_1}(x_{i_1})\cdots P_{X_n}(x_{i_n}),\ i_1=1,2,\ldots,i_n=1,2,\ldots
\]</span></p>
<p>o dicho de otra forma:</p>
<p><span class="math display">\[
P(X_1=x_{i_1},\ X_n=x_{i_n})=P(X_1=x_{i_1})\cdots P(X_n=x_{i_n}),\ i_1=1,2,\ldots,i_n=1,2,\ldots
\]</span></p>
<h3 id="ejemplo-120"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo del lanzamiento de un dado tres veces</strong></p>
<p>Consideremos el experimento aleatorio que consiste en lanzar un dado tres veces.</p>
<p>Recordemos que hemos estudiado la variable aleatoria <span class="math inline">\((X_1,X_2,X_3)\)</span> donde <span class="math inline">\(X_1\)</span> nos daba el número de 1’s que han salido, <span class="math inline">\(X_2\)</span>, el número de 2’s y <span class="math inline">\(X_3\)</span>, el número de 3’s.</p>
<p>Las variables aleatorias anteriores no son independientes ya que, por ejemplo:
<span class="math display">\[
P_{X_1,X_2,X_3}(1,1,3)=0\neq P_{X_1}(1)\cdot P_{X_2}(1)\cdot P_{X_3}(3)=0.3472222\cdot 0.3472222\cdot 0.0046296=6\times 10^{-4}.
\]</span></p>
</div>
<p><l class="observ">Observación. </l>
Al igual que pasaba con las variables bidimensionales, si la tabla de la <strong>función de probabilidad conjunta</strong> de <span class="math inline">\((X_1,\ldots,X_n)\)</span> contiene algún <span class="math inline">\(0\)</span>, las variables <span class="math inline">\(X_1,\ldots, X_n\)</span> no pueden ser independientes. ¿Podéis decir por qué?</p>
<p><l class="observ"> Observación. </l>
Si <span class="math inline">\(X_1,\ldots, X_n\)</span> son variables aleatorias independientes, y consideramos una distribución marginal, por ejemplo <span class="math inline">\((X_{s_1},\ldots,X_{s_k})\)</span>, entonces las variables <span class="math inline">\(X_{s_1},\ldots,X_{s_k}\)</span> también son independientes.</p>
<h3 id="independencia-de-variables-aleatorias-continuas-1"><span class="header-section-number">2.2.6</span> Independencia de variables aleatorias continuas</h3>
<p>La definición dada para <strong>variables aleatorias discretas</strong> se traslada de forma natural a las <strong>variables aleatorias continuas</strong>:</p>
<p><l class="definition">Definición de independencia para variables aleatorias <span class="math inline">\(n\)</span>-dimensionales continuas. </l>
Sean <span class="math inline">\((X_1\ldots,X_n)\)</span> una <strong>variable aleatoria <span class="math inline">\(n\)</span>-dimensional continua</strong> con <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{X_1\ldots X_n}\)</span> y <strong>funciones de densidad marginales</strong> <span class="math inline">\(f_{X_1},\ldots,f_{X_n}\)</span>. Entonces <span class="math inline">\(X_1,\ldots, X_n\)</span> son independientes si:
<span class="math display">\[
f_{X_1\ldots X_n}(x_1,\ldots,x_n)=f_{X_1}(x_1)\cdots f_{X_n}(x_n),\ \mbox{para todo $x_1,\ldots,x_n\in\mathbb{R}$.}
\]</span></p>
<h3 id="ejemplo-121"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos el ejemplo siguiente visto donde teníamos una <strong>variable aleatoria <span class="math inline">\(3\)</span>-dimensional continua</strong> <span class="math inline">\((X_1,X_2,X_3)\)</span> con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
8 x_1\cdot x_2\cdot x_3, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
0, &amp; \mbox{en caso contrario.}\\
\end{cases}
\]</span>
y con densidad marginales:
<span class="math display">\[
\begin{array}{rl}
f_{X_1}(x_1) &amp; =\begin{cases}
2x_1, &amp; \mbox{ si }0\leq x\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}\quad f_{X_2}(x_2)=\begin{cases}
2x_2, &amp; \mbox{ si }0\leq x_2\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}\\ f_{X_3}(x_3) &amp; =\begin{cases}
2x_3, &amp; \mbox{ si }0\leq x_3\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-122"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Veamos que son independientes.</p>
<p>Consideremos dos casos:</p>
<ul>
<li><p><span class="math inline">\((x_1,x_2,x_3)\in [0,1]\times [0,1]\times [0,1]\)</span>. En este caso:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3) =8 x_1 x_2 x_3 =2 x_1 2 x_2 2 x_3=f_{X_1}(x_1)\cdot f_{X_2}(x_2)\cdot f_{X_3}(x_3).
\]</span></p></li>
<li><span class="math inline">\((x_1,x_2,x_3)\not\in [0,1]\times [0,1]\times [0,1]\)</span>. En este caso:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3)  =0 = f_{X_1}(x_1)\cdot f_{X_2}(x_2)\cdot f_{X_3}(x_3),
\]</span>
ya que si <span class="math inline">\((x_1,x_2,x_3)\not\in [0,1]\times [0,1]\times [0,1]\)</span>, o <span class="math inline">\(x_1\not\in [0,1]\)</span> o <span class="math inline">\(x_2\not\in [0,1]\)</span>, o <span class="math inline">\(x_3\not\in [0,1]\)</span>. Por tanto <span class="math inline">\(f_{X_1}(x_1)=0\)</span> o <span class="math inline">\(f_{X_2}(x_2)=0\)</span> o <span class="math inline">\(f_{X_3}(x_3)=0\)</span>. En cualquier caso, <span class="math inline">\(f_{X_1}(x_1)\cdot f_{X_2}(x_2)\cdot f_{X_3}(x_3)=0\)</span>.</li>
</ul>
</div>
<h3 id="ejemplo-de-la-variable-gaussiana-n-dimensional"><span class="header-section-number">2.2.6</span> Ejemplo de la variable gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>En este caso, recordemos que la <strong>función de densidad conjunta</strong> de <span class="math inline">\((X_1,\ldots,X_n)\)</span> es:
<span class="math display">\[
\begin{array}{rl}
&amp; f_{X_1\ldots X_n}(x_1,\ldots,x_n)=\frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{\mathbf{|\Sigma|}}}\mathrm{e}^{-\frac{1}{2}(\mathbf{x-\mu})^\top\mathbf{\Sigma}^{-1}(\mathbf{x-\mu})},\\ &amp; \qquad  -\infty &lt;x_1,\ldots,x_n&lt;\infty,
\end{array}
\]</span>
donde <span class="math inline">\(\mathbf{\mu}\)</span> es el vector de medias y <span class="math inline">\(\mathbf{\Sigma}\)</span>, la matriz de covarianzas.</p>
<p>Recordemos también que las <strong>funciones de densidad marginales</strong> de <span class="math inline">\(X_1,\ldots, X_n\)</span> correspondían a <span class="math inline">\(N(\mu_i,\sigma_{ii})\)</span>, con <span class="math inline">\(i=1,\ldots, n\)</span>:
<span class="math display">\[
f_{X_i}(x_i)  =\frac{1}{\sqrt{2\pi\sigma_{ii}^2}}\mathrm{e}^{-\frac{(x_i-\mu_i)^2}{2\sigma_{ii}^2}},\ -\infty &lt;x_i&lt;\infty,\ i=1,\ldots,n.
\]</span></p>
<h3 id="ejemplo-de-la-variable-gaussiana-n-dimensional-1"><span class="header-section-number">2.2.6</span> Ejemplo de la variable gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>¿Cómo tiene que ser la matriz de covarianzas <span class="math inline">\(\mathbf{\Sigma}\)</span> para que las variables <span class="math inline">\(X_1,\ldots,X_n\)</span> sean independientes?</p>
<p>O, expresado matemáticamente,
<span class="math display">\[
\begin{array}{rl}
f_{X_1}(x_1)\cdots f_{X_n}(x_n) &amp; =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}\sigma_{11}\cdots \sigma_{nn}}\mathrm{e}^{-\sum\limits_{i=1}^n\frac{(x_i-\mu_i)^2}{2\sigma_{ii}^2}}=f_{X_1\ldots X_n}(x_1,\ldots,x_n) \\ &amp; =\frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{\mathbf{|\Sigma|}}}\mathrm{e}^{-\frac{1}{2}(\mathbf{x-\mu})^\top\mathbf{\Sigma}^{-1}(\mathbf{x-\mu})}.
\end{array}
\]</span></p>
<h3 id="ejemplo-de-la-variable-gaussiana-n-dimensional-2"><span class="header-section-number">2.2.6</span> Ejemplo de la variable gaussiana <span class="math inline">\(n\)</span>-dimensional</h3>
<p>La respuesta es claramente cuando la matriz de covarianzas <span class="math inline">\(\mathbf{\Sigma}\)</span> es diagonal, o sea, si <span class="math inline">\(\sigma_{ij}=0,\)</span> para <span class="math inline">\(i\neq j\)</span>, para todo <span class="math inline">\(i,j=1,\ldots,n\)</span>.</p>
<p>En resumen, cuando la covarianza entre dos variables cualesquiera <span class="math inline">\(X_i\)</span> y <span class="math inline">\(X_j\)</span> distintas es cero, las variables normales <span class="math inline">\(X_1,\ldots,X_n\)</span> son independientes.</p>
<h3 id="relación-de-la-independencia-y-la-función-de-distribución-1"><span class="header-section-number">2.2.6</span> Relación de la independencia y la función de distribución</h3>
<p>El siguiente resultado nos da la relación entre la <strong>independencia de variables aleatorias</strong> y su <strong>función de distribución conjunta</strong>:</p>
<p><l class="prop">Teorema. </l>
Sea <span class="math inline">\((X_1\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional. Entonces
<span class="math inline">\(X_1,\ldots,X_n\)</span> son independientes si, y sólo si, la <strong>función de distribución conjunta</strong> es el producto de las <strong>funciones de distribución marginales</strong> en todo valor <span class="math inline">\((x_1,\ldots,x_n)\in\mathbb{R}^n\)</span>:
<span class="math display">\[
F_{X_1\ldots X_n}(x_1,\ldots,x_n)=F_{X_1}(x_1)\cdots F_{X_n}(x_n),\ (x_1,\ldots,x_n)\in\mathbb{R}^n.
\]</span></p>
<h3 id="ejemplo-123"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos la variable aleatoria <span class="math inline">\(3\)</span>-dimensional continua con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
8 x_1\cdot x_2\cdot x_3, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
0, &amp; \mbox{en caso contrario.}\\
\end{cases}
\]</span>
Recordemos también <strong>función de distribución conjunta</strong> es:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
0, &amp; \mbox{si }x_1&lt;0,\mbox{ o }x_2&lt;0,\mbox{ o }x_3 &lt;0\\
x_1^2\cdot x_2^2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
 x_2^2\cdot x_3^2, &amp; \mbox{si }x_1&gt; 1,\ 0\leq x_2\leq  1,\ 0\leq x_3\leq  1, \\
 x_1^2\cdot x_3^2, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 x_3^2, &amp; \mbox{si }x_1&gt; 1,\ x_2&gt; 1,\ \ 0\leq x_3\leq  1, \\
 x_1^2\cdot x_2^2, &amp; \mbox{si }0\leq x_1\leq  1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
 x_1^2, &amp; \mbox{si }0\leq x_1\leq  1,\ x_2 &gt;  1,\ x_3&gt; 1,\\
 x_2^2, &amp; \mbox{si }x_1&gt;1,\ 0\leq x_2\leq  1,\ x_3&gt; 1,\\
1, &amp; \mbox{si }x_1\geq 1,\ x_2\geq 1,\ x_3\geq 1.
\end{cases}
\]</span></p>
</div>
<h3 id="ejemplo-124"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Recordemos las <strong>funciones de densidad</strong> de las <strong>distribuciones marginales</strong>:
<span class="math display">\[
\begin{array}{rl}
f_{X_1}(x_1) &amp; =\begin{cases}
2x_1, &amp; \mbox{ si }0\leq x\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}\quad f_{X_2}(x_2)=\begin{cases}
2x_2, &amp; \mbox{ si }0\leq x_2\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}\\ f_{X_3}(x_3) &amp; =\begin{cases}
2x_3, &amp; \mbox{ si }0\leq x_3\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\end{array}
\]</span>
Dejamos como ejercicio verificar que las expresiones siguientes correspondes a las <strong>funciones de distribución marginales</strong>:</p>
<p><span class="math display">\[
\begin{array}{rl}
F_{X_1}(x_1) &amp; =\begin{cases}
0, &amp; \mbox{ si }x_1&lt;0, \\
x_1^2, &amp; \mbox{ si }0\leq x_1\leq 1,\\
1, &amp; \mbox{ si }x_1 &gt; 1.
\end{cases}\quad F_{X_2}(x_2) &amp; =\begin{cases}
0, &amp; \mbox{ si }x_2&lt;0, \\
x_2^2, &amp; \mbox{ si }0\leq x_2\leq 1,\\
1, &amp; \mbox{ si }x_2 &gt; 1.
\end{cases}\\ F_{X_3}(x_3) &amp; =\begin{cases}
0, &amp; \mbox{ si }x_3&lt;0, \\
x_3^2, &amp; \mbox{ si }0\leq x_3\leq 1,\\
1, &amp; \mbox{ si }x_3 &gt; 1.
\end{cases}
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-125"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Recordemos que <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span> son independientes. Por tanto verifiquemos que <span class="math inline">\(F_{X_1X_2X_3}(x_1,x_2,x_3)=F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3)\)</span> para todos los valores <span class="math inline">\(x_1,x_2,x_3\in\mathbb{R}\)</span>.</p>
<p>Distingamos los mismos casos que en la <strong>función de distribución conjunta</strong>:</p>
<ul>
<li><p><span class="math inline">\(x_1&lt;0\)</span>, o <span class="math inline">\(x_2&lt;0\)</span>, o <span class="math inline">\(x_3 &lt;0\)</span>. En este caso, o <span class="math inline">\(F_{X_1}(x_1)=0\)</span>, o <span class="math inline">\(F_{X_2}(x_2)=0\)</span> o <span class="math inline">\(F_{X_3}(x_3)=0\)</span>. En cualquier caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=0= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
<li><p><span class="math inline">\(0\leq x_1\leq 1\)</span>, y <span class="math inline">\(0\leq x_2\leq 1\)</span>, y <span class="math inline">\(0\leq x_3\leq 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2\cdot x_2^2\cdot x_3^2= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
<li><p><span class="math inline">\(x_1&gt; 1\)</span>, y <span class="math inline">\(0\leq x_2\leq 1\)</span>, y <span class="math inline">\(0\leq x_3\leq 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_2^2\cdot x_3^2=1\cdot x_2^2\cdot x_3^2= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
<li><p><span class="math inline">\(0\leq x_1\leq 1,\)</span> y <span class="math inline">\(x_2&gt; 1\)</span>, y <span class="math inline">\(0\leq x_3\leq 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2\cdot x_3^2=x_1^2\cdot 1\cdot x_3^2= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
</ul>
</div>
<h3 id="ejemplo-126"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<ul>
<li><p><span class="math inline">\(x_1&gt; 1\)</span>, y <span class="math inline">\(x_2&gt; 1\)</span>, y <span class="math inline">\(0\leq x_3\leq 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_3^2=1\cdot 1\cdot x_3^2= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
<li><p><span class="math inline">\(0\leq x_1\leq 1\)</span>, y <span class="math inline">\(0\leq x_2\leq 1\)</span>, y <span class="math inline">\(x_3&gt; 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2\cdot x_2^2=x_1^2\cdot x_2^2\cdot 1= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
<li><p><span class="math inline">\(0\leq x_1\leq 1\)</span>, y <span class="math inline">\(x_2 &gt; 1\)</span>, y <span class="math inline">\(x_3&gt; 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2=x_1^2\cdot 1\cdot 1= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
<li><p><span class="math inline">\(x_1&gt; 1\)</span>, y <span class="math inline">\(0\leq x_2 \leq 1\)</span>, y <span class="math inline">\(x_3&gt; 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=x_2^2=1\cdot x_2^2\cdot 1= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></p></li>
<li><span class="math inline">\(x_1&gt; 1\)</span>, y <span class="math inline">\(x_2&gt;1\)</span>, y <span class="math inline">\(x_3&gt; 1\)</span>. En este caso:
<span class="math display">\[
F_{X_1X_2X_3}(x_1,x_2,x_3)=1=1\cdot 1\cdot 1= F_{X_1}(x_1)\cdot F_{X_2}(x_2)\cdot F_{X_3}(x_3).
\]</span></li>
</ul>
</div>
<h2 id="momentos-conjuntos-y-valores-esperados-conjuntos-1"><span class="header-section-number">2.2.6</span> Momentos conjuntos y valores esperados conjuntos</h2>
<h3 id="valor-esperado-de-una-función-de-n-variables-aleatorias"><span class="header-section-number">2.2.6</span> Valor esperado de una función de <span class="math inline">\(n\)</span> variables aleatorias</h3>
<p>Sea <span class="math inline">\((X_1,\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional.</p>
<p>Sea <span class="math inline">\(P_{X_1\ldots X_n}\)</span> su <strong>función de probabilidad conjunta</strong> en el caso en que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea <strong>discreta</strong> y <span class="math inline">\(f_{X_1\ldots X_n}\)</span> su <strong>función de densidad conjunta</strong> en el caso en que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea <strong>continua</strong>.</p>
<h3 id="valor-esperado-de-una-función-de-n-variables-aleatorias-1"><span class="header-section-number">2.2.6</span> Valor esperado de una función de <span class="math inline">\(n\)</span> variables aleatorias</h3>
<p>Sea <span class="math inline">\(Z=g(X_1,\ldots,X_n)\)</span> una <strong>variable aleatoria unidimensional</strong> función de las variables <span class="math inline">\(X_1,\ldots,X_n\)</span>. Por ejemplo:</p>
<ul>
<li>Media aritmética de las <span class="math inline">\(n\)</span> variables <span class="math inline">\(g(x_1,\ldots,x_n)=\frac{x_1+\cdots + x_n}{n}\)</span>: <span class="math inline">\(Z=\frac{X_1+\cdots +X_n}{n}\)</span>.</li>
<li>Media geométrica de las <span class="math inline">\(n\)</span> variables <span class="math inline">\(g(x_1,\ldots,x_n)=\sqrt[n]{x_1\cdots x_n}\)</span>: <span class="math inline">\(Z=\sqrt[n]{X_1\cdots X_n}\)</span>.</li>
<li>Suma de los cuadrados de las variables <span class="math inline">\(g(x_1,\ldots,x_n)=x_1^2+\cdots +x_n^2\)</span>: <span class="math inline">\(Z=X_1^2+\cdots +X_n^2\)</span>.</li>
</ul>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-3"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias</h3>
<p>Hay que tener en cuenta que <span class="math inline">\(Z\)</span>, como <strong>variable aleatoria unidimensional</strong> tiene una <strong>función de probabilidad</strong> <span class="math inline">\(P_Z\)</span> en el caso en que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea discreta y una <strong>función de densidad</strong> <span class="math inline">\(f_Z\)</span> en el caso en que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea continua.</p>
<p>El siguiente resultado nos dice cómo calcular el <strong>valor esperado</strong> de <span class="math inline">\(Z\)</span> sin tener que calcular <span class="math inline">\(P_Z\)</span> o <span class="math inline">\(f_Z\)</span>, sólo usando la información de la <strong>variable aleatoria conjunta</strong> <span class="math inline">\((X_1,\ldots,X_n)\)</span>:</p>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-4"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias</h3>
<p><l class="prop">Proposición. </l>
El valor esperado de <span class="math inline">\(Z\)</span> se puede hallar usando la expresión siguiente:</p>
<ul>
<li>en el caso en que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea discreta con <span class="math inline">\((X_1,\ldots,X_n)(\Omega)=\{(x_{i_1},\ldots,x_{i_n}),\ i_1=1,2,\ldots, i_n=1,2,\ldots\}\)</span>,
<span class="math display">\[
E(Z)  = E(g(X_1\ldots,X_n))  =\sum_{x_{i_1}}\cdots\sum_{x_{i_n}}g(x_{i_1},\ldots,x_{i_n})P(x_{i_1},\ldots,x_{i_n}),
\]</span></li>
</ul>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-5"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias</h3>
<ul>
<li>en el caso en que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea continua:
<span class="math display">\[
\begin{array}{rl}
&amp; E(Z)=E(g(X_1\ldots,X_n)) \\ &amp; \quad =\int_{-\infty}^\infty\cdots\int_{-\infty}^\infty g(x_1,\ldots,x_n)f_{X_1\ldots X_n}(x_1,\ldots,x_n)\, dx_1\ldots dx_n.
\end{array}
\]</span></li>
</ul>
<h3 id="ejemplo-127"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo del lanzamiento de un dado tres veces</strong></p>
<p>Consideremos el ejemplo de la variable <span class="math inline">\((X_1,X_2,X_3)\)</span> que nos daba el número de 1’s, 2’s y 3’s en el lanzamiento de un dado tres veces.</p>
<p>Vamos a calcular <span class="math inline">\(E\left(X_1\cdot X_2\cdot X_3\right)\)</span>.
El valor esperado anterior se calcularía de la forma siguiente:
<span class="math display">\[
E\left(X_1\cdot X_2\cdot X_3\right)=\sum_{i_1=0}^3\sum_{i_2=0}^3
\sum_{i_3=0}^3 i_1\cdot i_2\cdot i_3\cdot P_{X_1X_2X_3}(i_1,i_2,i_3),
\]</span>
en total <span class="math inline">\(4^3=64\)</span> términos. Como el cálculo es tedioso, lo vamos a realizar con ayuda de <code>R</code>.</p>
</div>
<h3 id="ejemplo-con-r-21"><span class="header-section-number">2.2.6</span> Ejemplo con <code>R</code></h3>
<div class="example">
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1">valor.esperado=<span class="dv">0</span>;</a>
<a class="sourceLine" id="cb104-2" data-line-number="2"><span class="cf">for</span> (i1 <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb104-3" data-line-number="3">  <span class="cf">for</span> (i2 <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb104-4" data-line-number="4">    <span class="cf">for</span> (i3 <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb104-5" data-line-number="5">      valor.esperado=valor.esperado<span class="op">+</span>i1<span class="op">*</span>i2<span class="op">*</span>i3<span class="op">*</span><span class="kw">fun.prob.con</span>(i1,i2,i3)</a>
<a class="sourceLine" id="cb104-6" data-line-number="6">    }</a>
<a class="sourceLine" id="cb104-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb104-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb104-9" data-line-number="9">valor.esperado</a></code></pre></div>
<pre><code>## [1] 0.02777778</code></pre>
</div>
<h3 id="ejemplo-128"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Recordemos la variable aleatoria <span class="math inline">\(3\)</span>-dimensional <span class="math inline">\((X_1,X_2,X_3)\)</span> con <strong>función de densidad conjunta</strong>:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
8 x_1\cdot x_2\cdot x_3, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
0, &amp; \mbox{en caso contrario.}\\
\end{cases}
\]</span>
Calculemos <span class="math inline">\(E(X_1^2+X_2^2+X_3^2)\)</span>:
<span class="math display">\[
\begin{array}{rl}
E(X_1^2+X_2^2+X_3^2) &amp; =\int_{x_1=0}^{x_1=1} \int_{x_2=0}^{x_2=1}\int_{x_3=0}^{x_3=1} (x_1^2+x_2^2+x_3^2) 8 x_1 x_2 x_3 \,dx_1\, dx_2\, dx_3\\ &amp; =8\left(\int_{x_1=0}^{x_1=1} \int_{x_2=0}^{x_2=1}\int_{x_3=0}^{x_3=1}   x_1^3 x_2 x_3 \,dx_1\, dx_2\, dx_3 + \int_{x_1=0}^{x_1=1} \int_{x_2=0}^{x_2=1}\int_{x_3=0}^{x_3=1}   x_1 x_2^3 x_3 \,dx_1\, dx_2\, dx_3 \right.\\ &amp; \left. + \int_{x_1=0}^{x_1=1} \int_{x_2=0}^{x_2=1}\int_{x_3=0}^{x_3=1}   x_1 x_2 x_3^3 \,dx_1\, dx_2\, dx_3\right) \\ &amp; =
8\left(\left[\frac{x_1^4}{4}\right]_0^1 \left[\frac{x_2^2}{2}\right]_0^1 \left[\frac{x_3^2}{2}\right]_0^1 + \left[\frac{x_1^2}{2}\right]_0^1 \left[\frac{x_2^4}{4}\right]_0^1 \left[\frac{x_3^2}{2}\right]_0^1 + \left[\frac{x_1^2}{2}\right]_0^1 \left[\frac{x_2^2}{2}\right]_0^1 \left[\frac{x_3^4}{4}\right]_0^1\right) \\ &amp; =8\cdot 3\cdot \frac{1}{16}=\frac{3}{2}=1.5.
\end{array}
\]</span></p>
</div>
<h3 id="propiedad-del-valor-esperado-de-la-suma-de-variables"><span class="header-section-number">2.2.6</span> Propiedad del valor esperado de la suma de variables</h3>
<p>En estos momentos estamos en condiciones de demostrar el resultado siguiente:</p>
<p><l class="prop">Proposición. Valor esperado de la suma de variables aleatorias.</l>
Sea <span class="math inline">\((X_1,\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional. Entonces el valor esperado de la variable aleatoria suma de las variables es igual a la suma de los valores esperados de cada variable:
<span class="math display">\[
E(X_1+\cdots + X_n)=E(X_1)+\cdots + E(X_n).
\]</span></p>
<h3 id="propiedad-del-valor-esperado-de-la-suma-de-variables-1"><span class="header-section-number">2.2.6</span> Propiedad del valor esperado de la suma de variables</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Haremos la demostración para el caso continuo. Dejamos como ejercicio la demostración en el caso discreto.</p>
<p>El valor esperado de la suma de variables será en función de la <strong>función de densidad conjunta</strong> <span class="math inline">\(f_{X_1\ldots X_n}\)</span>:
<span class="math display">\[
\begin{array}{rl}
E(X_1+\cdots + X_n) &amp; = \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty (x_1+\cdots + x_n)f_{X_1\ldots X_n}(x_1,\ldots,x_n)\,dx_1\ldots dx_n \\ &amp; = \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty x_1f_{X_1\ldots X_n}(x_1,\ldots,x_n)\,dx_1\ldots dx_n+ \cdots \\ &amp; + \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty x_n f_{X_1\ldots X_n}(x_1,\ldots,x_n)\,dx_1\ldots dx_n \\ &amp; = E(X_1)+\cdots + E(X_n).
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-129"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo del lanzamiento de un dado tres veces</strong></p>
<p>Consideremos el ejemplo de la variable <span class="math inline">\((X_1,X_2,X_3)\)</span> que nos daba el número de 1’s, 2’s y 3’s en el lanzamiento de un dado tres veces.</p>
<p>Comprobemos en este caso que <span class="math inline">\(E(X_1+X_2+X_3)=E(X_1)+E(X_2)+E(X_3)\)</span>.</p>
</div>
<h3 id="ejemplo-130"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Calculemos <span class="math inline">\(E(X_1+X_2+X_3)\)</span> con <code>R</code> usando la misma técnica que en el ejemplo donde calculábamos <span class="math inline">\(E(X_1\cdot X_2\cdot X_3)\)</span>:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1">valor.esperado=<span class="dv">0</span>;</a>
<a class="sourceLine" id="cb106-2" data-line-number="2"><span class="cf">for</span> (i1 <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb106-3" data-line-number="3">  <span class="cf">for</span> (i2 <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb106-4" data-line-number="4">    <span class="cf">for</span> (i3 <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb106-5" data-line-number="5">      valor.esperado=valor.esperado<span class="op">+</span>(i1<span class="op">+</span>i2<span class="op">+</span>i3)<span class="op">*</span><span class="kw">fun.prob.con</span>(i1,i2,i3)</a>
<a class="sourceLine" id="cb106-6" data-line-number="6">    }</a>
<a class="sourceLine" id="cb106-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb106-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb106-9" data-line-number="9">valor.esperado</a></code></pre></div>
<pre><code>## [1] 1.5</code></pre>
</div>
<h3 id="ejemplo-131"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>La distribución marginal de cada una de las variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span> recordemos que es la siguiente:</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_{X_1}\)</span></td>
<td><span class="math inline">\(0.5787\)</span></td>
<td><span class="math inline">\(0.3472\)</span></td>
<td><span class="math inline">\(0.0694\)</span></td>
<td><span class="math inline">\(0.0046\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="ejemplo-132"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>El valor de <span class="math inline">\(E(X_1)=E(X_2)=E(X_3)\)</span> será:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1">esperanza.X1=<span class="dv">0</span></a>
<a class="sourceLine" id="cb108-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb108-3" data-line-number="3">  esperanza.X1=esperanza.X1<span class="op">+</span>i<span class="op">*</span><span class="kw">fun.marginal.X1</span>(i)</a>
<a class="sourceLine" id="cb108-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb108-5" data-line-number="5">esperanza.X1</a></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<p>Claramente:
<span class="math display">\[
E(X_1+X_2+X_3)=1.5=E(X_1)+E(X_2)+E(X_3)=0.5+0.5+0.5.
\]</span></p>
</div>
<h3 id="valor-esperado-de-una-función-de-n-variables-aleatorias-independientes"><span class="header-section-number">2.2.6</span> Valor esperado de una función de <span class="math inline">\(n\)</span> variables aleatorias independientes</h3>
<p>El siguiente resultado nos simplifica el cálculo del <strong>valor esperado de una función de <span class="math inline">\(n\)</span> variables aleatorias</strong> en el caso en que sean <strong>independientes</strong>:</p>
<p><l class="prop">Proposición: cálculo del valor esperado de una función de <span class="math inline">\(n\)</span> variables aleatorias en el caso de independencia. </l>
Sea <span class="math inline">\((X_1,\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional donde suponemos que <span class="math inline">\(X_1,\ldots,X_n\)</span> son independientes.
Sea <span class="math inline">\(Z=g(X_1,\ldots,X_n)\)</span> una variable aleatoria unidimensional función de <span class="math inline">\(X_1,\ldots,X_n\)</span> donde suponemos que podemos “separar” las variables <span class="math inline">\(x_1,\ldots, x_n\)</span> en la función <span class="math inline">\(g\)</span>. O sea, existen <span class="math inline">\(n\)</span> funciones <span class="math inline">\(g_1,\ldots, g_n\)</span> tal que <span class="math inline">\(g(x_1,\ldots,x_n)=g_1(x_1)\cdots g_n(x_n)\)</span> para todo valor <span class="math inline">\(x_1,\ldots,x_n\in\mathbb{R}\)</span>. En este caso, el valor esperado de <span class="math inline">\(Z\)</span> se puede calcular como:
<span class="math display">\[
E(Z)=E(g(X_1\ldots,X_n))=E_{X_1}(g_1(X_1))\cdots E_{X_n}(g_n(X_n)).
\]</span></p>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-independientes-3"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias independientes</h3>
<p>O sea, el cálculo de <span class="math inline">\(E(g(X_1\ldots,X_n))\)</span> que sería una suma múltiple en el caso de que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea <strong>discreta</strong> o una integral múltiple en el caso en que <span class="math inline">\((X_1,\ldots,X_n)\)</span> sea continua se transforma en el producto de <span class="math inline">\(n\)</span> sumas simples (caso <strong>discreto</strong>) o el producto de <span class="math inline">\(n\)</span> integrales simples (caso <strong>continuo</strong>):
<span class="math display">\[
\begin{array}{rl}
E(Z) &amp; =E(g(X_1\ldots,X_n))\\ &amp; =\left(\sum_{x_{i_1}} g_1(x_{i_1})\cdot P_{X_1}(x_{i_1})\right)\cdots \left(\sum_{x_{i_n}} g_n(x_{i_n})\cdot P_{X_n}(x_{i_n})\right), \ \mbox{caso discreto},\\
E(Z) &amp; =E(g(X_1\ldots,X_n)) \\ &amp; =\left(\int_{-\infty}^\infty g_1(x_1)\cdot f_{X_1}(x_1)\, dx_1\right)\cdots \left(\int_{-\infty}^\infty g_n(x_n)\cdot f_{X_n}(x_n)\right), \ \mbox{caso continuo}.
\end{array}
\]</span></p>
<h3 id="valor-esperado-de-una-función-de-dos-variables-aleatorias-independientes-4"><span class="header-section-number">2.2.6</span> Valor esperado de una función de dos variables aleatorias independientes</h3>
<p>Un caso particular de aplicación de la proposición anterior sería cuando queramos calcular <span class="math inline">\(E(X_1\cdots X_n)\)</span>. En este caso <span class="math inline">\(g(x_1,\ldots,x_n)=x_1\cdots x_n\)</span>, <span class="math inline">\(g_1(x_1)=x_1\)</span>, y <span class="math inline">\(g_n(x_n)=x_n\)</span>.</p>
<p>Podemos escribir, por tanto:
<span class="math display">\[
E(X_1\cdots X_n)=E_{X_1}(X_1)\cdots E_{X_n}(X_n),
\]</span>
si <span class="math inline">\(X_1,\ldots,X_n\)</span> son independientes.</p>
<h3 id="ejemplo-133"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Recordemos el ejemplo con función de densidad conjunta:
<span class="math display">\[
f_{X_1X_2X_3}(x_1,x_2,x_3)=\begin{cases}
8 x_1\cdot x_2\cdot x_3, &amp; \mbox{si }0\leq x_1\leq 1,\ 0\leq x_2\leq 1,\ 0\leq x_3\leq 1, \\
0, &amp; \mbox{en caso contrario.}\\
\end{cases}
\]</span>
Recordemos que son independientes. Comprobemos que <span class="math inline">\(E(X_1\cdot X_2\cdot X_3)=E(X_1)\cdot E(X_2)\cdot E(X_3)\)</span>.</p>
<p>Calculemos <span class="math inline">\(E(X_1\cdot X_2\cdot X_3)\)</span>:
<span class="math display">\[
\begin{array}{rl}
E(X_1\cdot X_2\cdot X_3)  &amp; = \int_0^1\int_0^1\int_0^1 x_1\cdot x_2\cdot x_3\cdot 8 x_1\cdot x_2\cdot x_3\,dx_1\, dx_2\, dx_3 \\ &amp; = 8\int_0^1\int_0^1\int_0^1 x_1^2\cdot x_2^2\cdot x_3^2\,dx_1\, dx_2\, dx_3 =8\left[\frac{x_1^3}{3}\right]_0^1\left[\frac{x_2^3}{3}\right]_0^1
\left[\frac{x_3^3}{3}\right]_0^1 = 8\cdot \frac{1}{3^3}=\frac{8}{27}=0.2963.
\end{array}
\]</span></p>
<p>Recordemos que las <strong>densidades marginales</strong> eran:
<span class="math display">\[
\begin{array}{rl}
f_{X_1}(x_1) &amp; =\begin{cases}
2x_1, &amp; \mbox{ si }0\leq x\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}\quad f_{X_2}(x_2)=\begin{cases}
2x_2, &amp; \mbox{ si }0\leq x_2\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}\\ f_{X_3}(x_3) &amp; =\begin{cases}
2x_3, &amp; \mbox{ si }0\leq x_3\leq 1,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-134"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Por tanto:
<span class="math display">\[
E(X_1)=E(X_2)=E(X_3)=\int_0^1 x\cdot 2 x\, dx =2 \int_0^1 x^2\, dx=2\left[\frac{x^3}{3}\right]_0^1=\frac{2}{3}.
\]</span>
Claramente, se verifica:
<span class="math display">\[
E(X_1\cdot X_2\cdot X_3)=\frac{8}{27}=E(X_1)\cdot E(X_2)\cdot E(X_3)=\left(\frac{2}{3}\right)^3.
\]</span></p>
</div>
<h3 id="propiedades-de-la-covarianza-2"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<p>Veamos cómo se calcula la <strong>covarianza</strong> de dos combinaciones lineales de variables aleatorias:</p>
<p><l class="prop">Proposición (covarianza de dos combinaciones lineales de variables aleatorias). </l>
Sean <span class="math inline">\((X_1,\ldots,X_n)\)</span> e <span class="math inline">\((Y_1,\ldots, Y_n)\)</span> dos variables aleatorias <span class="math inline">\(n\)</span>-dimensionales. Sean <span class="math inline">\(a_1, \ldots, a_n\)</span> y <span class="math inline">\(b_1,\ldots, b_n\)</span> <span class="math inline">\(n\)</span> parejas de valores reales. Sean <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> las variables aleatorias siguientes:
<span class="math inline">\(U=\sum\limits_{i=1}^n a_i X_i,\  V=\sum\limits_{i=1}^n b_i Y_i.\)</span>
Entonces la <strong>covarianza</strong> de las variables <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> se calcula usando la expresión siguiente:
<span class="math display">\[
\mathrm{Cov}(U,V)=\sum_{i=1}^n\sum_{j=1}^n a_i b_j \mathrm{Cov}(X_i,Y_j).
\]</span></p>
<h3 id="propiedades-de-la-covarianza-3"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Usando la expresión de la <strong>covarianza</strong> podemos calcular la covarianza entre las variables <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> como:
<span class="math display">\[
\begin{array}{rl}
\mathrm{Cov}(U,V) &amp; =E(UV)-E(U)E(V)=E\left(\sum\limits_{i=1}^n a_i X_i\cdot \sum\limits_{j=1}^n b_j Y_j\right)- E\left(\sum\limits_{i=1}^n a_i X_i\right)E\left(\sum\limits_{j=1}^n b_j Y_j\right) \\ &amp; =\sum\limits_{i=1}^n \sum\limits_{j=1}^n a_i b_j E\left(X_i Y_j\right)-\sum\limits_{i=1}^n a_i E(X_i)\sum\limits_{j=1}^n b_j E(Y_j) = \sum\limits_{i=1}^n \sum\limits_{j=1}^n a_i b_j E\left(X_i Y_j\right)-\sum\limits_{i=1}^n\sum\limits_{j=1}^n a_i b_j E(X_i) E(Y_j) \\ &amp; = \sum\limits_{i=1}^n \sum\limits_{j=1}^n a_i b_j \left(E\left(X_i Y_j\right) - E(X_i) E(Y_j)\right) = \sum\limits_{i=1}^n \sum\limits_{j=1}^n a_i b_j \mathrm{Cov}(X_i,Y_j),
\end{array}
\]</span>
tal como queríamos ver.</p>
</div>
<h3 id="propiedades-de-la-covarianza-4"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<p>Si en la combinación lineal sólo hay las componentes de una sola variable aleatoria <span class="math inline">\(n\)</span>-dimensional, la proposición anterior se convierte en la proposición siguiente:</p>
<p><l class="prop">Proposición (covarianza de una combinación lineal de variables aleatorias). </l>
Sean <span class="math inline">\((X_1,\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional. Sean <span class="math inline">\(a_1, \ldots, a_n\)</span> y <span class="math inline">\(b_1,\ldots, b_n\)</span> <span class="math inline">\(n\)</span> parejas de valores reales. Sean <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> las variables aleatorias siguientes:
<span class="math inline">\(U=\sum\limits_{i=1}^n a_i X_i,\  V=\sum\limits_{i=1}^n b_i X_i.\)</span>
Entonces la <strong>covarianza</strong> de las variables <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> se calcula usando la expresión siguiente:
<span class="math display">\[
\mathrm{Cov}(U,V)=\sum_{i=1}^n a_i b_i \mathrm{Var}(X_i)+\sum_{i=1}^n\sum_{j=1,j\neq i}^n a_i b_j \mathrm{Cov}(X_i,X_j).
\]</span></p>
<h3 id="propiedades-de-la-covarianza-5"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Usando la misma técnica de demostración que en la proposición anterior, podemos calcular la covarianza entre las variables <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> como:
<span class="math display">\[
\begin{array}{rl}
\mathrm{Cov}(U,V) &amp; = \sum\limits_{i=1}^n \sum\limits_{j=1}^n a_i b_j \left(E\left(X_i X_j\right) - E(X_i) E(X_j)\right) = \sum\limits_{i=1}^n a_i b_i \left(E\left(X_i^2\right) - E(X_i)^2\right)+\sum\limits_{i=1}^n \sum\limits_{j=1,j\neq i}^n a_i b_j \mathrm{Cov}(X_i,X_j) \\ &amp; = \sum\limits_{i=1}^n a_i b_i \mathrm{Var}(X_i)+\sum\limits_{i=1}^n \sum\limits_{j=1,j\neq i}^n a_i b_j \mathrm{Cov}(X_i,X_j),
\end{array}
\]</span>
tal como queríamos ver.</p>
</div>
<h3 id="propiedades-de-la-covarianza-6"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<p><l class="observ"> Observación. </l>
Una forma equivalente de escribir la covarianza anterior sería:
<span class="math display">\[
\mathrm{Cov}(U,V)=\sum_{i=1}^n a_i b_i \mathrm{Var}(X_i)+2\sum_{i=1}^n\sum_{j=1,j&gt;i}^n a_i b_j \mathrm{Cov}(X_i,X_j).
\]</span></p>
<h3 id="propiedades-de-la-covarianza-7"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<p>Como, en general <span class="math inline">\(\mathrm{Var}(U)=\mathrm{Cov}(U,U)\)</span>, una consecuencia directa de la proposición anterior es la expresión de la varianza de una combinación lineal de variables aleatorias:</p>
<p><l class="prop">Proposición (varianza de una combinación lineal de variables aleatorias). </l>
Sean <span class="math inline">\((X_1,\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional. Sean <span class="math inline">\(a_1, \ldots, a_n\)</span> <span class="math inline">\(n\)</span> valores reales. Sea <span class="math inline">\(U\)</span> la variable aleatoria siguiente:
<span class="math inline">\(U=\sum\limits_{i=1}^n a_i X_i.\)</span>
Entonces la <strong>varianza</strong> de la variable <span class="math inline">\(U\)</span> se calcula usando la expresión siguiente:
<span class="math display">\[
\mathrm{Var}(U)=\sum_{i=1}^n a_i^2 \mathrm{Var}(X_i)+\sum_{i=1}^n\sum_{j=1,j\neq i}^n a_i a_j \mathrm{Cov}(X_i,X_j).
\]</span></p>
<h3 id="propiedades-de-la-covarianza-8"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Para la demostración, basta tener en cuenta que <span class="math inline">\(\mathrm{Var}(U)=\mathrm{Cov}(U,U)\)</span> y aplicar la expresión de la covariancia entre <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> en la proposición que nos da la covarianza de una combinación lineal de variables aleatorias.</p>
</div>
<p><l class="observ"> Observación. </l>
En este caso, también existe una forma equivalente de escribir la varianza anterior:
<span class="math display">\[
\mathrm{Var}(U)=\sum_{i=1}^n a_i^2 \mathrm{Var}(X_i)+2\sum_{i=1}^n\sum_{j=1,j&gt;i}^n a_i a_j \mathrm{Cov}(X_i,X_j).
\]</span></p>
<h3 id="propiedades-de-la-covarianza-9"><span class="header-section-number">2.2.6</span> Propiedades de la covarianza</h3>
<p>Una consecuencia de la proposición anterior es que si las variables son <strong>independientes</strong>, la <strong>varianza</strong> de la suma es la suma de varianzas:</p>
<p><l class="prop">Proposición (varianza de la suma de variables aleatorias independientes). </l>
Sean <span class="math inline">\((X_1,\ldots,X_n)\)</span> una variable aleatoria <span class="math inline">\(n\)</span>-dimensional donde <span class="math inline">\(X_1,\ldots, X_n\)</span> son independientes.<br />
Entonces la <strong>varianza</strong> de la variable <span class="math inline">\(X_1+\cdots X_n\)</span> es la suma de las varianzas de cada variable aleatoria:
<span class="math display">\[
\mathrm{Var}(X_1+\cdots + X_n)=\sum_{i=1}^n \mathrm{Var}(X_i).
\]</span></p>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Para la demostración, basta aplicar la proposición anterior de la varianza de una combinación lineal de variables aleatorias con <span class="math inline">\(a_i=1\)</span>, para todo <span class="math inline">\(i=1,\ldots,n\)</span> y tener en cuenta que como son independientes, <span class="math inline">\(\mathrm{Cov}(X_i,X_j)=0\)</span>, para todo <span class="math inline">\(i\neq j\)</span>.</p>
</div>

<h1 id="ley-de-los-grandes-números-y-teorema-central-del-límite"><span class="header-section-number">2.2.6</span> Ley de los grandes números y Teorema Central del Límite</h1>
<h2 id="muestras-aleatorias-simples"><span class="header-section-number">2.2.6</span> Muestras aleatorias simples</h2>
<p>El pilar básico sobre el que se sustenta la <strong>estadística inferencial</strong> es el concepto de <strong>muestra aleatoria simple</strong>.</p>
<p>Una <strong>muestra aleatoria simple</strong>, desde el punto de vista de la probabilidad es una distribución <span class="math inline">\(n\)</span> variables aleatorias, <span class="math inline">\(X_1,\ldots, X_n\)</span> todas independientes entre sí e idénticamente distribuidas ya que queremos simular la repetición de un experimento <span class="math inline">\(n\)</span> veces de forma independiente.</p>
<p>Por tanto, estudiar una <strong>muestra aleatoria simple</strong> equivale a estudiar su distribución.</p>
<h3 id="introducción-13"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>En muchos casos, nos bastará estudiar la distribución de una variable que “represente” a dicha <strong>muestra aleatoria simple</strong>: la media muestral definida como <span class="math inline">\(\overline{X}=\frac{X_1+\cdots + X_n}{n}\)</span>.</p>
<p>Las <strong>leyes de los grandes números</strong> nos dicen que, de alguna manera (que concretaremos más adelante), la media muestral y la media poblacional se “parecen” a la larga o cuando el número de repeticiones <span class="math inline">\(n\)</span> tiende a infinito.</p>
<p>El <strong>Teorema Central del Límite</strong> nos dice que la distribución de la media muestral tiende, sea cual sea la distribución de las variables <span class="math inline">\(X_i\)</span>, a una normal. De ahí que la <strong>distribución normal</strong> sea la más importante en probabilidades y estadística.</p>
<h3 id="la-distribución-de-la-media-muestral"><span class="header-section-number">2.2.6</span> La distribución de la media muestral</h3>
<p>Vamos cómo se distribuye la media de un conjunto de variables normales e idénticamente distribuidas:</p>
<p><l class="prop">Proposición. Distribución de la media muestral de <span class="math inline">\(n\)</span> variables normales independientes e idénticamente distribuidas. </l>
Sean <span class="math inline">\(X_1,\ldots, X_n\)</span> <span class="math inline">\(n\)</span> variables normales de media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2\)</span>, todas normales e independientes. Consideramos la variable <span class="math inline">\(\overline{X}=\frac{X_1+\cdots + X_n}{n}\)</span> la media muestral. Entonces la distribución de la variable aleatoria <span class="math inline">\(\overline{X}\)</span> es normal de la misma media <span class="math inline">\(\mu\)</span> de las <span class="math inline">\(X_i\)</span> y varianza <span class="math inline">\(\frac{\sigma^2}{n}\)</span>.</p>
<h3 id="la-distribución-de-la-media-muestral-1"><span class="header-section-number">2.2.6</span> La distribución de la media muestral</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Consideramos la variable aleatoria <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\(\mathbf{X}=(X_1,\ldots,X_n)\)</span>. Dicha variable tendrá la distribución normal <span class="math inline">\(n\)</span>-dimensional con vector de medias <span class="math inline">\(\mathbf{\mu}=(\mu,\ldots,\mu)^\top\)</span> y matriz de covarianzas <span class="math inline">\(\mathbf{\Sigma}\)</span> diagonal ya que recordemos que las <span class="math inline">\(X_i\)</span> son independientes y, por tanto, incorreladas o de covarianza nula:
<span class="math display">\[
\mathbf{\Sigma}=\begin{pmatrix}
\sigma^2 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; \sigma^2 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; \sigma^2
\end{pmatrix}.
\]</span></p>
</div>
<h3 id="la-distribución-de-la-media-muestral-2"><span class="header-section-number">2.2.6</span> La distribución de la media muestral</h3>
<div class="dem">
<p>Para hallar la variable <span class="math inline">\(\overline{X}\)</span>, realizamos la transformación afín siguiente:
<span class="math display">\[
\overline{X}=\left(\frac{1}{n},\ldots,\frac{1}{n}\right)\begin{pmatrix} X_1 \\ X_2\\\vdots \\ X_n \end{pmatrix}.
\]</span>
Aplicando la proposición sobre la transformación afín sobre una variable normal <span class="math inline">\(n\)</span>-dimensional que vimos en el capítulo de distribuciones <span class="math inline">\(n\)</span>-dimensionales con matriz de cambio <span class="math inline">\(\mathbf{C}=\left(\frac{1}{n},\ldots,\frac{1}{n}\right)\)</span> y <span class="math inline">\(\mathbf{c}=0\)</span>, tenemos que la distribución de <span class="math inline">\(\overline{X}\)</span> será normal de media <span class="math inline">\(\mathbf{c}+\mathbf{C}\mathbf{\mu} = \mu\)</span> y varianza (o matriz de covarianzas <span class="math inline">\(1\times 1\)</span>):
<span class="math display">\[
\mathbf{C}\mathbf{\Sigma}\mathbf{C}^\top =\left(\frac{1}{n},\ldots,\frac{1}{n}\right)\begin{pmatrix}
\sigma^2 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; \sigma^2 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; \sigma^2
\end{pmatrix} \begin{pmatrix}\frac{1}{n}\\\frac{1}{n}\\\vdots\\\frac{1}{n}\end{pmatrix} =\frac{\sigma^2}{n}.
\]</span></p>
</div>
<h2 id="convergencia-de-sucesiones-de-variables-aleatorias"><span class="header-section-number">2.2.6</span> Convergencia de sucesiones de variables aleatorias</h2>
<h3 id="introducción-14"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>En esta sección vamos a intentar concretar cómo la media muestral y la media poblacional de una <strong>muestra aleatoria simple</strong> se van pareciendo, así como la distribución de la <strong>media muestral</strong> se va “acercando” a la normalidad.</p>
<p>Para ello, necesitamos introducir un conjunto de conceptos relacionados con la convergencia de variables aleatorias.</p>
<p>En primer lugar, introduciremos el concepto de <strong>sucesión de variables aleatorias</strong>:</p>
<p><l class="definition"> Definición de sucesión de variables aleatorias. </l>
Consideremos un experimento aleatorio sobre un <strong>espacio muestral</strong> <span class="math inline">\(\Omega\)</span>. Sea <span class="math inline">\(P\)</span> una probabilidad definida sobre el conjunto de sucesos de <span class="math inline">\(\Omega\)</span>. Entonces, si <span class="math inline">\(X_1,X_2,\ldots,X_n,\ldots\)</span> son variables aleatorias definidas sobre <span class="math inline">\(\Omega,P\)</span>, diremos que forman una <strong>sucesión de variables aleatorias</strong> y lo denotaremos por <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span>.</p>
<h3 id="ejemplo-135"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo: lanzamiento de un dado</strong></p>
<p>Consideremos el experimento aleatorio de ir lanzando un dado no trucado. Definimos la variable aleatoria <span class="math inline">\(X_n\)</span> como el resultado del dado el lanzamiento <span class="math inline">\(n\)</span>-ésimo.</p>
<p>Entonces, la sucesión de variables aleatorias <span class="math inline">\(X_1,\ldots,X_n,\ldots\)</span> sería la asociada al lanzamiento del dado.</p>
<p>¡Ojo! no confundir la sucesión de variables aleatorias <span class="math inline">\(X_1,\ldots,X_n,\ldots\)</span> con la sucesión de resultados de dichas variables aleatorias <span class="math inline">\(x_1,\ldots, x_n,\ldots\)</span>. Lo primero correspondería a variables aleatorias con su función de probabilidad, esperanza, varianza, etc., y lo segundo sería simplemente una sucesión numérica de valores enteros entre 1 y 6.</p>
</div>
<h3 id="convergencia-casi-segura"><span class="header-section-number">2.2.6</span> Convergencia casi segura</h3>
<p><l class="definition"> Definición de convergencia casi segura. </l>
Sea <span class="math inline">\(X_1,\ldots,X_n,\ldots\)</span> una sucesión de variables aleatorias y sea <span class="math inline">\(X\)</span> una variable aleatoria definida sobre el mismo espacio muestral <span class="math inline">\(\Omega\)</span> y con la misma probabilidad de sucesos. Diremos que la sucesión <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> converge <strong>casi seguramente</strong> hacia <span class="math inline">\(X\)</span> si
<span class="math display">\[
P(\{w\in \Omega\ |\ \lim_{n\to\infty} X_n(w)=X(w)\})=1.
\]</span>
Lo denotaremos por <span class="math inline">\(X_n\stackrel{c.s.}{\longrightarrow}X\)</span>.</p>
<h3 id="convergencia-casi-segura-1"><span class="header-section-number">2.2.6</span> Convergencia casi segura</h3>
<p>O sea, si el conjunto de elementos <span class="math inline">\(w\)</span> del espacio muestral <span class="math inline">\(\Omega\)</span> que cumplen que el límite de la sucesión de números reales <span class="math inline">\((X_n(w))_n\)</span> tiende a <span class="math inline">\(X(w)\)</span> tiene probabilidad <span class="math inline">\(1\)</span>.</p>
<p>De ahí viene el nombre de <strong>casi segura</strong>: el conjunto de valores <span class="math inline">\(w\)</span> del espacio muestral tal que la sucesión numérica <span class="math inline">\((X_n(w))_n\)</span> <strong>no converge</strong> a <span class="math inline">\(X(w)\)</span> tiene probabilidad 0.</p>
<h3 id="convergencia-casi-segura-2"><span class="header-section-number">2.2.6</span> Convergencia casi segura</h3>
<p>Comprobar la <strong>convergencia casi segura</strong> a partir de la definición puede ser muy complicado. Por suerte, existe la proposición siguiente que nos hace la vida más fácil:</p>
<p><l class="prop"> Proposición. </l>
Sea <span class="math inline">\(X_1,\ldots,X_n,\ldots\)</span> una sucesión de variables aleatorias y sea <span class="math inline">\(X\)</span> una variable aleatoria definida sobre el mismo espacio muestral <span class="math inline">\(\Omega\)</span> y con la misma probabilidad de sucesos.
Entonces <span class="math inline">\(X_n\stackrel{c.s.}{\longrightarrow}X\)</span> si, y sólo si, para todo valor <span class="math inline">\(\epsilon &gt;0\)</span>, la serie siguiente
<span class="math display">\[
\sum_{n=1}^\infty P(|X_n-X|&gt;\epsilon),
\]</span>
es convergente.</p>
<h3 id="ejemplo-136"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
Veamos si la sucesión <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> tiene convergencia <strong>casi segura</strong> hacia la variable <span class="math inline">\(X\)</span> cuya <strong>función de probabilidad</strong> es:
<div class="center">
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(X\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P_X\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>En este caso el espacio muestral <span class="math inline">\(\Omega\)</span> es <span class="math inline">\(\Omega=\{1,2,3,4,5,6\}\)</span> y la <strong>función de probabilidad</strong> de cada <span class="math inline">\(X_i\)</span> corresponde con la tabla anterior.</p>
<p>Seguidamente, de cara a aplica la proposición anterior, vamos a hallar la <strong>función de probabilidad</strong> de la variable <span class="math inline">\(D_n=X_n-X\)</span>.
Los valores de la variable anterior son: <span class="math inline">\(D_n(\Omega)=\{-5,-4,-3,-2,-1,0,1,2,3,4,5\}\)</span>.</p>
<p>La <strong>función de probabilidad</strong> conjunta de la variable <span class="math inline">\((X_n,X)\)</span> será al ser <span class="math inline">\(X_n\)</span> y <span class="math inline">\(X\)</span> independientes:
<span class="math display">\[
P_{X_nX}(x_n,x)=P_{X_n}(x_n)\cdot P_X(x)=\frac{1}{6}\cdot \frac{1}{6}=\frac{1}{36},
\]</span>
para todo <span class="math inline">\(x_n=1,2,3,4,5,6\)</span> y para todo <span class="math inline">\(x=1,2,3,4,5,6\)</span>.</p>
</div>
<h3 id="ejemplo-137"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>La <strong>función de probabilidad</strong> de la variable <span class="math inline">\(D_n\)</span> será:
<span class="math display">\[
\begin{array}{rl}
P_{D_n}(-5) &amp; =P_{X_nX}(1,6)=\frac{1}{36}, \\
P_{D_n}(-4) &amp; =P_{X_nX}(2,6)+P_{X_nX}(1,5)=\frac{2}{36}, \\
P_{D_n}(-3) &amp; =P_{X_nX}(3,6)+P_{X_nX}(2,5)+P_{X_nX}(1,4)=\frac{3}{36}, \\
P_{D_n}(-2) &amp; =P_{X_nX}(4,6)+P_{X_nX}(3,5)+P_{X_nX}(2,4)+P_{X_nX}(1,3)=\frac{4}{36}, \\
P_{D_n}(-1) &amp; =P_{X_nX}(5,6)+P_{X_nX}(4,5)+P_{X_nX}(3,4)+P_{X_nX}(2,3)+P_{X_nX}(1,2)=\frac{5}{36}, \\
P_{D_n}(0) &amp; =P_{X_nX}(6,6)+P_{X_nX}(5,5)+P_{X_nX}(4,4)+P_{X_nX}(3,3)+P_{X_nX}(2,2)+P_{X_nX}(1,1)=\frac{6}{36}, \\
P_{D_n}(1) &amp; =P_{X_nX}(6,5)+P_{X_nX}(5,4)+P_{X_nX}(4,3)+P_{X_nX}(3,2)+P_{X_nX}(2,1)=\frac{5}{36}, \\
P_{D_n}(2) &amp; =P_{X_nX}(6,4)+P_{X_nX}(5,3)+P_{X_nX}(4,2)+P_{X_nX}(3,1)=\frac{4}{36}, \\
P_{D_n}(3) &amp; =P_{X_nX}(6,3)+P_{X_nX}(5,2)+P_{X_nX}(4,1)=\frac{3}{36}, \\
P_{D_n}(4) &amp; =P_{X_nX}(6,2)+P_{X_nX}(5,1)=\frac{2}{36}, \\
P_{D_n}(5) &amp; =P_{X_nX}(6,1)=\frac{1}{36}.
\end{array}
\]</span></p>
</div>
<h3 id="ejemplo-138"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Sea <span class="math inline">\(\epsilon\)</span> un valor real entre 0 y 1: <span class="math inline">\(0&lt;\epsilon &lt;1\)</span>. Entonces el suceso <span class="math inline">\(\{|D_n|&gt;\epsilon\}\)</span> será el complementario del suceso <span class="math inline">\(\{D_n=0\}\)</span> ya que el único valor entre <span class="math inline">\(-5\)</span> y <span class="math inline">\(5\)</span> que no cumple <span class="math inline">\(|D_n|&gt;\epsilon\)</span> es el valor <span class="math inline">\(D_n=0\)</span>. Por tanto:
<span class="math display">\[
P(|D_n|&gt;\epsilon)=1-P(D_n=0)=1-P_{D_n}(0)=1-\frac{1}{6}=\frac{5}{6}.
\]</span>
La serie <span class="math inline">\(\sum\limits_{n=1}^\infty \frac{5}{6}\)</span> no es convergente de forma obvia. Por tanto, deducimos que la sucesión <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> no converge <strong>casi seguramente</strong> hacia la variable <span class="math inline">\(X\)</span>.</p>
</div>
<h3 id="convergencia-en-probabilidad"><span class="header-section-number">2.2.6</span> Convergencia en probabilidad</h3>
<p><l class="definition"> Definición de convergencia en probabilidad. </l>
Sea <span class="math inline">\(X_1,\ldots,X_n,\ldots\)</span> una sucesión de variables aleatorias y sea <span class="math inline">\(X\)</span> una variable aleatoria definida sobre el mismo espacio muestral <span class="math inline">\(\Omega\)</span> y con la misma probabilidad de sucesos. Diremos que la sucesión <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> converge <strong>en probabilidad</strong> hacia <span class="math inline">\(X\)</span> si para cualquier valor <span class="math inline">\(\epsilon &gt;0\)</span>,
<span class="math display">\[
\lim_{n\to\infty} P(|X_n-X|&gt;\epsilon \})=0.
\]</span>
Lo denotaremos por <span class="math inline">\(X_n\stackrel{c.p.}{\longrightarrow}X\)</span>.</p>
<h3 id="convergencia-en-probabilidad-1"><span class="header-section-number">2.2.6</span> Convergencia en probabilidad</h3>
<p>O sea, límite de la probabilidad de los sucesos formados por los <span class="math inline">\(w\in\Omega\)</span> tal que <span class="math inline">\(|X_n(w)-X(w)|&gt;\epsilon\)</span> vale 0.</p>
<p><l class="observ">Observación.</l>
Una definición equivalente de <strong>convergencia en probabilidad</strong> es que para todo valor <span class="math inline">\(\epsilon &gt;0\)</span>,
<span class="math display">\[
\lim_{n\to\infty} P(|X_n(w)-X(w)|\leq \epsilon \})=1.
\]</span>
<l class="observ">Observación. </l>
La convergencia <strong>casi segura</strong> implica la convergencia <strong>en probabilidad</strong> ya que si la sucesión <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> converge <strong>casi seguramente</strong> hacia <span class="math inline">\(X\)</span>, la serie <span class="math inline">\(\sum_{n=1}^\infty P(|X_n-X|&gt;\epsilon)\)</span> será convergente y, por tanto, el límite de su término <span class="math inline">\(P(|X_n-X|&gt;\epsilon)\)</span> tenderá a cero, hecho que equivale a la convergencia <strong>en probabilidad</strong>.</p>
<h3 id="convergencia-en-probabilidad-2"><span class="header-section-number">2.2.6</span> Convergencia en probabilidad</h3>
<p>El siguiente resultado nos puede ayudar algunas veces a comprobar la <strong>convergencia en probabilidad</strong>:</p>
<p><l class="prop">Proposición. </l>
Sea <span class="math inline">\(X_1,\ldots,X_n,\ldots\)</span> una sucesión de variables aleatorias. Sea <span class="math inline">\(\mu_n\)</span> el valor medio de la variable <span class="math inline">\(X_n\)</span>, <span class="math inline">\(E(X_n)=\mu_n\)</span> y <span class="math inline">\(\sigma_n^2\)</span> su varianza: <span class="math inline">\(\mathrm{Var}(X_n)=\sigma_n^2\)</span>. Supongamos que <span class="math inline">\(\lim_{n\to\infty}\sigma_n^2=0\)</span>. Entonces,
<span class="math display">\[
X_n-\mu_n\stackrel{c.p.}{\longrightarrow} 0.
\]</span></p>
<h3 id="convergencia-en-probabilidad-3"><span class="header-section-number">2.2.6</span> Convergencia en probabilidad</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Usando la desigualdad de Chebyschev, podemos escribir:
<span class="math display">\[
P(|X_n-\mu_n|&gt;\epsilon \}) \leq \frac{\sigma_n^2}{\epsilon^2}.
\]</span>
Tomando límite a cada parte de la desigualdad anterior tenemos:
<span class="math display">\[
0\leq \lim_{n\to\infty} P(|X_n-X|&gt;\epsilon \}) \leq \lim_{n\to\infty}\frac{\sigma_n^2}{\epsilon^2}=0,
\]</span>
de donde deducimos que <span class="math inline">\(\lim_{n\to\infty} P(|X_n-X|&gt;\epsilon \})=0\)</span>, tal como queríamos ver.</p>
</div>
<h3 id="ejemplo-139"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo del lanzamiento de un dado</strong></p>
<p>En el ejemplo anterior del lanzamiento de un dado, no hay convergencia en probabilidad ya que comprobamos que para <span class="math inline">\(0&lt;\epsilon&lt;1\)</span>,
<span class="math display">\[
P(|X_n-X|&gt;\epsilon)=\frac{5}{6}.
\]</span>
Por tanto, para <span class="math inline">\(0&lt;\epsilon&lt;1\)</span>, <span class="math inline">\(\lim_{n\to\infty} P(|X_n-X|&gt;\epsilon \})=\frac{5}{6}\neq 0.\)</span></p>
</div>
<h3 id="ejemplo-140"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Consideremos las variables aleatorias <span class="math inline">\(X_n\)</span> con función de densidad:
<span class="math display">\[
f_{X_n}(x)=\begin{cases}
\lambda n\mathrm{e}^{-\lambda n x}, &amp; \mbox{si }x&gt;0,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span>
O sea, <span class="math inline">\(X_n\)</span> son exponenciales de parámetro <span class="math inline">\(\lambda n\)</span>.</p>
<p>Veamos que <span class="math inline">\(\{X_n\}_{n=1}^\infty\stackrel{c.p}{\longrightarrow} 0\)</span>.</p>
<p>Dado <span class="math inline">\(\epsilon &gt;0\)</span>, calculemos <span class="math inline">\(P(|X_n|&gt;\epsilon \})\)</span>:
<span class="math display">\[
P(|X_n|&gt;\epsilon \}) = \int_\epsilon^\infty \lambda n\mathrm{e}^{-\lambda n x}\, dx =\lambda n \left[\frac{1}{-\lambda n}\mathrm{e}^{-\lambda n x}\right]_\epsilon^\infty =\mathrm{e}^{-\lambda n \epsilon}\stackrel{n\to\infty}{\longrightarrow} 0,
\]</span>
tal como queríamos ver.</p>
</div>
<h3 id="convergencia-en-ley-o-en-distribución"><span class="header-section-number">2.2.6</span> Convergencia en ley o en distribución</h3>
<p><l class="definition"> Definición de convergencia en ley o distribución. </l>
Sea <span class="math inline">\(X_1,\ldots,X_n,\ldots\)</span> una sucesión de variables aleatorias y sea <span class="math inline">\(X\)</span> una variable aleatoria definida sobre el mismo espacio muestral <span class="math inline">\(\Omega\)</span> y con la misma probabilidad de sucesos. Sea <span class="math inline">\(F_{X_n}\)</span> y <span class="math inline">\(F_X\)</span> las funciones de distribución de la variable <span class="math inline">\(X_n\)</span> y <span class="math inline">\(X\)</span>, respectivamente. Diremos que la sucesión <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> converge <strong>en ley, o en distribución</strong> hacia <span class="math inline">\(X\)</span> si,
<span class="math display">\[
\lim_{n\to\infty} F_{X_n}(x)=F(x),
\]</span>
para todo valor <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<p>Lo denotaremos por <span class="math inline">\(X_n\stackrel{{\cal L}}{\longrightarrow}X\)</span>.</p>
<h3 id="convergencia-en-ley-o-en-distribución-1"><span class="header-section-number">2.2.6</span> Convergencia en ley o en distribución</h3>
<p>El resultado siguiente simplifica algunas veces comprobar que la sucesión <span class="math inline">\(X_n\)</span> converge en ley hacia <span class="math inline">\(X\)</span>:</p>
<p><l class="prop"> Proposición. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias y sea <span class="math inline">\(X\)</span> una variable aleatoria definida sobre el mismo espacio muestral <span class="math inline">\(\Omega\)</span> y con la misma probabilidad de sucesos. Sean <span class="math inline">\(\phi_{X_n}\)</span> y <span class="math inline">\(\phi_X\)</span> las funciones características de <span class="math inline">\(X_n\)</span> y <span class="math inline">\(X\)</span>, respectivamente. Entonces, la sucesión converge <strong>en ley</strong> hacia <span class="math inline">\(X\)</span>, <span class="math inline">\(X_n\stackrel{{\cal L}}{\longrightarrow}X\)</span>, si, y sólo si,
<span class="math display">\[
\lim_{n} \phi_{X_n}(t) = \phi_X(t),
\]</span>
para cualquier número <span class="math inline">\(t\in\mathbb{R}\)</span>.</p>
<h3 id="ejemplo-141"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de la distribución binomial <span class="math inline">\(B(n,p)\)</span></strong></p>
<p>Veamos que si <span class="math inline">\(X_n=B(n,p_n)\)</span> tiene distribución binomial de parámetros <span class="math inline">\(n\)</span> y <span class="math inline">\(p_n\)</span>, con <span class="math inline">\(p_n=\frac{\lambda}{n}\)</span>, con <span class="math inline">\(\lambda\)</span> fijo,
<span class="math display">\[
B(n,p)\stackrel{{\cal L}}{\longrightarrow}Poiss(\lambda).
\]</span></p>
<p>En el tema de distribuciones notables demostramos que para todo <span class="math inline">\(k\in\{0,\ldots,n\}\)</span>,
<span class="math display">\[
P(X_n = k)=\binom{n}{k}\cdot p_n^k\cdot (1-p_n)^{n-k}\stackrel{n\to\infty}{\longrightarrow} P(X=k)=\frac{\lambda^k}{k!}\cdot\mathrm{e}^{-\lambda}.
\]</span>
Entonces tenemos que dado <span class="math inline">\(x\in\mathbb{R}\)</span>, existe <span class="math inline">\(k\in\{0,\ldots,n\}\)</span>, tal que <span class="math inline">\(k\leq x&lt; k+1\)</span>. Por tanto,
<span class="math display">\[
\begin{array}{rl}
\lim\limits_{n\to\infty} F_{X_n}(x) &amp; = \lim\limits_{n\to\infty} F_{X_n}(k)=\lim\limits_{n\to\infty} P(X_n=0)+\cdots + P(X_n=k) \\  &amp; =\lim\limits_{n\to\infty} P(X_n=0)+\cdots + \lim\limits_{n\to\infty} P(X_n=k) = P(X=0)+\cdots + P(X=k)\\ &amp;  =F_X(k)=F_X(x),
\end{array}
\]</span>
tal como queríamos demostrar.</p>
</div>
<h3 id="relaciones-entre-las-distintas-convergencias"><span class="header-section-number">2.2.6</span> Relaciones entre las distintas convergencias</h3>
<p>El resultado siguiente nos dice cuando un tipo de convergencia implica la otra:</p>
<p><l class="prop"> Proposición. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias y sea <span class="math inline">\(X\)</span> una variable aleatoria definida sobre el mismo espacio muestral <span class="math inline">\(\Omega\)</span> y con la misma probabilidad de sucesos. Entonces:</p>
<ul>
<li><p>Si <span class="math inline">\(X_n\stackrel{c.s.}{\longrightarrow} X\)</span>, entonces <span class="math inline">\(X_n\stackrel{c.p.}{\longrightarrow} X\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X_n\stackrel{c.p.}{\longrightarrow} X\)</span>, entonces <span class="math inline">\(X_n\stackrel{{\cal L}}{\longrightarrow} X\)</span>.</p></li>
</ul>
<p>En resumen, la convergencia más fuerte es la <strong>casi segura</strong>, luego vendría la convergencia <strong>en probabilidad</strong> y, por último, la convergencia <strong>en ley</strong>:</p>
<p><span class="math display">\[
\mbox{Conv. casi segura }\Rightarrow \mbox{ Conv. en probabilidad }\Rightarrow\mbox{ Conv. en ley.}
\]</span></p>
<h2 id="leyes-de-los-grandes-números"><span class="header-section-number">2.2.6</span> Leyes de los grandes números</h2>
<h3 id="introducción-15"><span class="header-section-number">2.2.6</span> Introducción</h3>
<p>Como ya comentamos al principio del tema, las <strong>leyes de los grandes números</strong> estudian el comportamiento de la <strong>media muestral</strong> <span class="math inline">\(\overline{X}_n\)</span> cuando la sucesión de variables aleatorias <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> se va hacia infinito.</p>
<p>Más concretamente, diremos que una sucesión de variables aleatorias <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> cumple una <strong>ley de los grandes números</strong> si existe un sucesión numérica <span class="math inline">\((a_n)_n\)</span> tal que la sucesión de variables aleatorias <span class="math inline">\(\{\overline{X}_n-a_n\}\)</span> converge “de alguna manera” de las que hemos visto hacia 0.</p>
<p>Si este “alguna manera” es la convergencia más fuerte, o la <strong>casi segura</strong>, tendremos la <strong>ley fuerte de los grandes números</strong>.</p>
<p>En cambio, si la convergencia es <strong>en probabilidad</strong>, tendremos la <strong>ley débil de los grandes números</strong>.</p>
<h3 id="leyes-débiles-de-los-grandes-números"><span class="header-section-number">2.2.6</span> Leyes débiles de los grandes números</h3>
<p><l class="prop"> Teorema. Ley débil de los grandes números. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes dos a dos tal que sus varianzas existen y están acotadas por una constante independiente de <span class="math inline">\(n\)</span>. Entonces,
<span class="math display">\[
\overline{X}_n-\frac{1}{n}\sum_{i=1}^n \mu_i\stackrel{c.p.}{\longrightarrow} 0,
\]</span>
donde <span class="math inline">\(\mu_i = E(X_i)\)</span>.</p>
<p>Dicho en otras palabras: en las condiciones de la proposición anterior, la diferencia entre la sucesión de <strong>medias muestrales</strong> como variables aleatorias y la sucesión numérica de la medias poblacionales de dichas variables aleatorias tiende en <strong>probabilidad</strong> hacia 0.</p>
<h3 id="leyes-débiles-de-los-grandes-números-1"><span class="header-section-number">2.2.6</span> Leyes débiles de los grandes números</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Como las variables son independientes dos a dos la varianza de la suma es la suma de las varianzas:
<span class="math display">\[
\mathrm{Var}(\overline{X}_n)=\frac{1}{n^2}\mathrm{Var}(\sum_{i=1}^n X_i)=\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2,
\]</span>
donde <span class="math inline">\(\sigma_i^2 = \mathrm{Var}(X_i)\)</span>.</p>
<p>Sabemos por hipótesis que existe una constante <span class="math inline">\(M\)</span> tal que <span class="math inline">\(\sigma_i^2\leq M\)</span> para todo <span class="math inline">\(i\)</span>. Por tanto,
<span class="math display">\[
\mathrm{Var}(\overline{X}_n)=\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2\leq \frac{1}{n^2}Mn =\frac{M}{n}.
\]</span></p>
</div>
<h3 id="leyes-débiles-de-los-grandes-números-2"><span class="header-section-number">2.2.6</span> Leyes débiles de los grandes números</h3>
<div class="dem">
<p>El valor del valor medio de la media muestral será:
<span class="math display">\[
E(\overline{X}_n)=\frac{1}{n}\sum_{i=1}^n E(X_i)=\frac{1}{n}\sum_{i=1}^n \mu_i. 
\]</span>
Usando la desigualdad de Chebyschev, deducimos, dado un <span class="math inline">\(\epsilon &gt;0\)</span>:
<span class="math display">\[
P\left(\left|\overline{X}_n-\frac{1}{n}\sum_{i=1}^n \mu_i\right|&gt;\epsilon\right) \leq \frac{\mathrm{Var}(\overline{X}_n)}{\epsilon^2}\leq \frac{M}{n\epsilon^2}.
\]</span>
Por tanto, tomando límites en las dos partes de la desigualdad anterior, deducimos
<span class="math display">\[
\lim_{n\to \infty}P\left(\left|\overline{X}_n-\frac{1}{n}\sum_{i=1}^n \mu_i\right|&gt;\epsilon\right) =0,
\]</span>
tal como queríamos ver.</p>
</div>
<h3 id="leyes-débiles-de-los-grandes-números-3"><span class="header-section-number">2.2.6</span> Leyes débiles de los grandes números</h3>
<p>Derivadas del teorema anterior tenemos las consecuencias siguientes:</p>
<p><l class="prop">Corolario. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes dos a dos tal que todas tienes la misma esperanza <span class="math inline">\(\mu\)</span> y la misma varianza <span class="math inline">\(\sigma^2\)</span>. Entonces,
<span class="math display">\[
\overline{X}_n\stackrel{c.p.}{\longrightarrow} \mu,
\]</span></p>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>En este caso tenemos que <span class="math inline">\(\mu_i=\mu\)</span> y, por tanto, <span class="math inline">\(\frac{1}{n}\sum\limits_{i=1}^n \mu_i =\frac{1}{n}\cdot n\mu=\mu\)</span>. Si aplicamos el teorema de la <strong>ley débil de los grandes números</strong> nos sale el resultado enunciado.</p>
</div>
<h3 id="leyes-débiles-de-los-grandes-números-4"><span class="header-section-number">2.2.6</span> Leyes débiles de los grandes números</h3>
<p>Derivadas del teorema anterior tenemos las consecuencias siguientes:</p>
<p><l class="prop">Corolario. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes dos a dos e idénticamente distribuidas tal que todas tienes la misma esperanza <span class="math inline">\(\mu\)</span>. Entonces,
<span class="math display">\[
\overline{X}_n\stackrel{c.p.}{\longrightarrow} \mu,
\]</span></p>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Trivial a partir del Corolario anterior.</p>
</div>
<h3 id="ejemplo-142"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo del lanzamiento de una moneda</strong></p>
<p>Vamos a simular la <strong>ley débil de los grandes números</strong> en el caso en que el experimento aleatorio sea el lanzamiento de una moneda.</p>
<p>En este caso, tendremos que las variables aleatorias <span class="math inline">\(X_n\)</span> tendrán distribución de Bernoulli de parámetro <span class="math inline">\(p=\frac{1}{2}\)</span>.</p>
<p>La variable <span class="math inline">\(\overline{X}_n\)</span> representa la proporción de caras (<span class="math inline">\(X_n=1\)</span>) en el lanzamiento de la moneda <span class="math inline">\(n\)</span> veces. Nos preguntamos si dicha proporción de caras tiende al parámetro <span class="math inline">\(p\)</span> en probabilidad.</p>
<p>Vamos a hallar una muestra para cada variable <span class="math inline">\(\overline{X}_n=\frac{\sum\limits_{i=1}^n X_i}{n}\)</span>.</p>
<p>Para ello, vamos a repetir el experimento de lanzar la moneda <span class="math inline">\(N=100\)</span> veces y lo repetimos <span class="math inline">\(k=500\)</span> ocasiones.</p>
<p>Los resultados estarán en una matriz <span class="math inline">\(k\times N =500\times 100\)</span> donde cada fila de la matriz simulará una repetición del experimento de lanzar la moneda <span class="math inline">\(N=100\)</span> veces.</p>
</div>
<h3 id="ejemplo-143"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Dada la fila <span class="math inline">\(i\)</span>-ésima, iremos calculando <span class="math inline">\(\overline{X}_1^{(i)},\overline{X}_2^{(i)},\ldots,\overline{X}_{N=100}^{(i)}\)</span>.</p>
<p>Luego, fijado un <span class="math inline">\(\epsilon\)</span>, para cada <span class="math inline">\(n\)</span>, aproximaremos la probabilidad <span class="math inline">\(P\left(\left|\overline{X}_n-\frac{1}{2}\right|&gt;\epsilon\right)\)</span> usando la fórmula de Laplace:
<span class="math display">\[
p_n=P\left(\left|\overline{X}_n-\frac{1}{2}\right|&gt;\epsilon\right) \approx\frac{\#\left\{\mbox{$i$ tal que  $\left|\overline{X}_n^{(i)}-\frac{1}{2}\right|&gt;\epsilon$}\right\}}{k}.
\]</span></p>
<p>Para comprobar dicha afirmación, la idea es hallar para cada valor <span class="math inline">\(n\)</span>, una muestra para cada variable <span class="math inline">\(\overline{X}_n=\frac{\sum\limits_{i=1}^n X_i}{n}\)</span>.</p>
</div>
<h3 id="ejemplo-144"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Para hallar una muestra de cada variable <span class="math inline">\(\overline{X}_n\)</span>, seguimos los pasos siguientes:</p>
<ul>
<li>En primer lugar, simulamos la repetición del experimento de lanzar la moneda <span class="math inline">\(N=100\)</span> veces y lo repetimos <span class="math inline">\(k=500\)</span> ocasiones.
Los resultados estarán en una matriz <span class="math inline">\(k\times N =500\times 100\)</span> donde cada fila de la matriz simulará una repetición del experimento de lanzar la moneda <span class="math inline">\(N=100\)</span> veces:</li>
</ul>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1">N=<span class="dv">100</span></a>
<a class="sourceLine" id="cb110-2" data-line-number="2">k=<span class="dv">500</span></a>
<a class="sourceLine" id="cb110-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">2019</span>) <span class="co">## fijamos la semila de aleatoriedad para que a todos nos dé lo mismo</span></a>
<a class="sourceLine" id="cb110-4" data-line-number="4">valores.experimento=<span class="kw">matrix</span>(<span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),N<span class="op">*</span>k,<span class="dt">replace=</span><span class="ot">TRUE</span>),k,N)</a></code></pre></div>
<p>Los primeros resultados son:</p>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## [1,]    0    1    1    1    1    1    1    0    1     1     0     0     0
## [2,]    0    0    0    0    0    1    0    1    1     0     1     1     0
## [3,]    1    1    1    0    0    1    1    0    0     1     0     1     0
## [4,]    0    0    1    1    1    0    0    0    1     1     0     1     1
## [5,]    0    1    1    1    1    1    0    1    0     1     1     1     1</code></pre>
<p>…</p>
</div>
<h3 id="ejemplo-145"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<ul>
<li>En segundo lugar, dada la fila <span class="math inline">\(i\)</span>-ésima de la matriz anterior, iremos calculando <span class="math inline">\(\overline{X}_1^{(i)},\overline{X}_2^{(i)},\ldots,\overline{X}_{N=100}^{(i)}\)</span> guardando los resultados en una matriz de medias muestrales.
Antes de nada, creamos la función que nos realizará la operación anterior dado un vector cualquiera <code>x</code>:</li>
</ul>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1">cálculo.xnbarra =<span class="st"> </span><span class="cf">function</span>(x){</a>
<a class="sourceLine" id="cb112-2" data-line-number="2">  <span class="kw">return</span>(<span class="kw">cumsum</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)))</a>
<a class="sourceLine" id="cb112-3" data-line-number="3">}</a></code></pre></div>
<p>A partir de la matriz de los resultados, aplicamos la función anterior a cada fila y hallaremos una matriz con todas las <span class="math inline">\(\overline{X}_n^{(i)}\)</span>:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1">matriz.medias.muestrales =<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(valores.experimento,<span class="dv">1</span>,cálculo.xnbarra))</a></code></pre></div>
<p>La columna <span class="math inline">\(j\)</span>-ésima de la matriz <code>matriz.medias.muestrales</code> contiene una muestra de <span class="math inline">\(k=500\)</span> valores de la variable <span class="math inline">\(\overline{X}_j\)</span>.</p>
</div>
<h3 id="ejemplo-146"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<ul>
<li>En último lugar, fijado un <span class="math inline">\(\epsilon\)</span>, para cada <span class="math inline">\(n\)</span>, aproximaremos la probabilidad <span class="math inline">\(P\left(\left|\overline{X}_n-\frac{1}{2}\right|&gt;\epsilon\right)\)</span> usando la fórmula de Laplace:
<span class="math display">\[
p_n=P\left(\left|\overline{X}_n-\frac{1}{2}\right|&gt;\epsilon\right) \approx\frac{\#\left\{\mbox{$i$ tal que  $\left|\overline{X}_n^{(i)}-\frac{1}{2}\right|&gt;\epsilon$}\right\}}{k}.
\]</span>
La columna <span class="math inline">\(j\)</span>-ésima de la matriz <code>matriz.medias.muestrales</code> es una muestra de la variable <span class="math inline">\(\overline{X}_j\)</span>. Por tanto, para hallar la aproximación de <span class="math inline">\(p_n\)</span>, miramos cuántos valores de la columna <span class="math inline">\(j\)</span>-ésima de la matriz anterior verifican <span class="math inline">\(\left|\overline{X}_j^{l}-\frac{1}{2}\right|&gt;\epsilon\)</span>, para <span class="math inline">\(l=1,\ldots, k\)</span>:</li>
</ul>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1">epsilon=<span class="fl">0.1</span></a>
<a class="sourceLine" id="cb114-2" data-line-number="2">probabilidades.pn=<span class="st"> </span><span class="kw">colSums</span>(<span class="kw">abs</span>(matriz.medias.muestrales<span class="fl">-0.5</span>) <span class="op">&gt;</span><span class="st"> </span>epsilon)<span class="op">/</span>k</a></code></pre></div>
</div>
<h3 id="ejemplo-147"><span class="header-section-number">2.2.6</span> Ejemplo</h3>
<div class="example">
<p>Para ver los resultados, dibujamos el gráfico <span class="math inline">\(n\)</span> vs. <span class="math inline">\(p_n\)</span>:</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span>N,probabilidades.pn,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">xlab=</span><span class="kw">expression</span>(N),<span class="dt">ylab=</span><span class="kw">expression</span>(p[n]),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p>Observamos que las probabilidades tienden a cero tal como nos dice el <strong>Teorema de la ley débil de los grandes números</strong>.</p>
</div>
</div>
</div>
<div id="ejemplo-148" class="section level3">
<h3><span class="header-section-number">2.2.7</span> Ejemplo</h3>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
</div>
</div>
<div id="convergencia-de-los-momentos-muestrales" class="section level3">
<h3><span class="header-section-number">2.2.8</span> Convergencia de los momentos muestrales</h3>
<p>Dada una sucesión de variables aleatorias, definimos los momentos muestrales de la forma siguiente:</p>
<p><l class="definition"> Definición de los momentos muestrales de una sucesión de variables aleatorias.</l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias. Dado <span class="math inline">\(k\)</span> valor entero positivo, definimos el <strong>momento muestral</strong> de orden <span class="math inline">\(k\)</span> como la sucesión de variables aleatorias siguientes:
<span class="math display">\[
M_k^{(n)} = \frac{1}{n}\sum_{i=1}^n X_i^k.
\]</span></p>
<p><l class="observ"> Observación: </l>
el <strong>momento muestral</strong> de orden <span class="math inline">\(k=1\)</span> es la <strong>media muestral</strong> <span class="math inline">\(\overline{X}_n\)</span>.</p>
</div>
<div id="convergencia-de-los-momentos-muestrales-1" class="section level3">
<h3><span class="header-section-number">2.2.9</span> Convergencia de los momentos muestrales</h3>
<p><l class="definition"> Definición de los momentos muestrales centrados de una sucesión de variables aleatorias.</l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias. Dado <span class="math inline">\(k\)</span> valor entero positivo, definimos el <strong>momento muestral centrado en la media</strong> de orden <span class="math inline">\(k\)</span> como la sucesión de variables aleatorias siguientes:
<span class="math display">\[
MC_k^{(n)} = \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X}_n)^k.
\]</span></p>
<p><l class="observ"> Observación: </l>
el <strong>momento muestral centrado en la media</strong> de orden <span class="math inline">\(k=2\)</span> es la <strong>varianza muestral</strong> <span class="math inline">\(S_{X_n}^2\)</span>.</p>
</div>
<div id="convergencia-de-los-momentos-muestrales-2" class="section level3">
<h3><span class="header-section-number">2.2.10</span> Convergencia de los momentos muestrales</h3>
<p><l class="definition"> Definición de la covarianza y el coeficiente de correlación muestral de una sucesión de variables aleatorias.</l>
Sea <span class="math inline">\(\{(X_n,Y_n)\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias bidimensionales. Definimos la <strong>covarianza muestral</strong> como la sucesión de variables aleatorias siguientes:
<span class="math display">\[
S_{X_n,Y_n} = \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X}_n)(Y_i-\overline{Y}_n),
\]</span>
y el <strong>coeficiente de correlación muestral</strong> como la sucesión de variables aleatorias siguientes:
<span class="math display">\[
R_{X_n,Y_n}=\frac{S_{X_n,Y_n}}{\sqrt{S_{X_n}^2 S_{Y_n}^2}}.
\]</span></p>
</div>
<div id="convergencia-de-los-momentos-muestrales-3" class="section level3">
<h3><span class="header-section-number">2.2.11</span> Convergencia de los momentos muestrales</h3>
<p>Dada <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias y <span class="math inline">\(k\)</span> un valor entero positivo, en el tema de Complementos de variables aleatorias, definimos los momentos y los momentos centrales de orden <span class="math inline">\(k\)</span> para cada de dichas variables como:
<span class="math display">\[
m_k^{(n)} = E\left(X_n^k\right),\quad\mu_k^{(n)}=E\left(\left(X_n-\mu_n\right)^k\right),
\]</span>
donde <span class="math inline">\(\mu_n\)</span> es el valor medio de la variable <span class="math inline">\(X_n\)</span>: <span class="math inline">\(\mu_n = E(X_n)\)</span>.</p>
</div>
<div id="convergencia-de-los-momentos-muestrales-4" class="section level3">
<h3><span class="header-section-number">2.2.12</span> Convergencia de los momentos muestrales</h3>
<p>Así mismo, dada <span class="math inline">\(\{(X_n,Y_n)\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias bidimensionales, en el tema de variables aleatorias bidimensionales definimos para cada variable <span class="math inline">\((X_n,Y_n)\)</span> la covarianza <span class="math inline">\(\sigma_{X_nY_n}\)</span> y el coeficiente de correlación <span class="math inline">\(\rho_{X_nY_n}\)</span>:
<span class="math display">\[
\sigma_{X_nY_n}=E((X_n-\mu_{X_n})(Y_n-\mu_{Y_n})),\quad \rho_{X_nY_n}=\frac{\sigma_{X_nY_n}}{\sqrt{\sigma_{X_n}^2\sigma_{Y_n}^2}}.
\]</span></p>
</div>
<div id="convergencia-de-los-momentos-muestrales-5" class="section level3">
<h3><span class="header-section-number">2.2.13</span> Convergencia de los momentos muestrales</h3>
<p>Dada una sucesión de variables aleatorias <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span>, el resultado siguiente nos relaciona los <strong>momentos muestrales</strong> y los <strong>momentos muestrales centrados en la media</strong> con los <strong>momentos</strong> y los <strong>momentos centrales</strong> de cada variable:</p>
<p><l class="prop"> Teorema. Convergencia de los momentos muestrales y los momentos muestrales centrados en la media. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias <strong>independientes dos a dos e idénticamente distribuidas</strong> y dado un entero positivo <span class="math inline">\(k\)</span>, supongamos que para cada <span class="math inline">\(n\)</span>, existe el <strong>momento de orden <span class="math inline">\(k\)</span></strong>, <span class="math inline">\(m_k\)</span> y el <strong>momento central de orden <span class="math inline">\(k\)</span></strong>, <span class="math inline">\(\mu_k\)</span>, que no dependerán de <span class="math inline">\(n\)</span> al ser idénticamente distribuidas. Entonces las sucesiones de variables aleatorias <span class="math inline">\(\{M_n^{(k)}\}_{n=1}^\infty\)</span> y <span class="math inline">\(\{MC_n^{(k)}\}_{n=1}^\infty\)</span> tienden a <span class="math inline">\(m_k\)</span> y <span class="math inline">\(\mu_k\)</span>, respectivamente, en <strong>probabilidad</strong>
<span class="math display">\[
M_n^{(k)}\stackrel{c.p.}{\longrightarrow} m_k,\quad MC_n^{(k)}\stackrel{c.p.}{\longrightarrow} \mu_k.
\]</span></p>
</div>
<div id="convergencia-de-los-momentos-muestrales-6" class="section level3">
<h3><span class="header-section-number">2.2.14</span> Convergencia de los momentos muestrales</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Consideremos la sucesión de variables aleatorias <span class="math inline">\(\{X_n^k\}_{n=1}^\infty\)</span>. Como las variables aleatorias de la sucesión <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> son independientes dos a dos e idénticamente distribuidas, las variables de la sucesión <span class="math inline">\(\{X_n^k\}_{n=1}^\infty\)</span> también lo serán.</p>
<p>La idea es aplicar la <strong>ley débil de los grandes números</strong> a la sucesión anterior.</p>
<p>El valor medio de cada variable de la sucesión <span class="math inline">\(\{X_n^k\}_{n=1}^\infty\)</span> será: <span class="math inline">\(\tilde{\mu}_n^{(k)}= E(X_n^{k})=m_k\)</span> el momento de orden <span class="math inline">\(k\)</span>.</p>
<p>Entonces, si hacemos <span class="math inline">\(\frac{1}{n}\sum\limits_{i=1}^n \tilde{\mu}_n^{(k)}\)</span> obtenemos:
<span class="math inline">\(\frac{1}{n} n\cdot m_k=m_k.\)</span></p>
</div>
</div>
<div id="convergencia-de-los-momentos-muestrales-7" class="section level3">
<h3><span class="header-section-number">2.2.15</span> Convergencia de los momentos muestrales</h3>
<div class="dem">
<p>Aplicando la <strong>ley débil de los grandes números</strong> a la sucesión <span class="math inline">\(\{X_n^k\}_{n=1}^\infty\)</span>, tendremos que
<span class="math display">\[
\overline{X^k}_n \stackrel{c.p.}{\longrightarrow}m_k,
\]</span>
pero <span class="math inline">\(\overline{X^k}_n\)</span> vale:
<span class="math display">\[
\overline{X^k}_n=\frac{1}{n}\sum_{i=1}^n X_i^k,
\]</span>
variable aleatoria que coincide con el momento muestral de orden <span class="math inline">\(k\)</span>, <span class="math inline">\(M_n^{(k)}\)</span>, tal como queríamos demostrar.</p>
<p>Dejamos como ejercicio la demostración de los momentos centrales. Razonando de la misma manera, no tiene dificultad alguna.</p>
</div>
</div>
<div id="convergencia-de-los-momentos-muestrales-8" class="section level3">
<h3><span class="header-section-number">2.2.16</span> Convergencia de los momentos muestrales</h3>
<p>Enunciemos ahora el resultado para las covarianzas y las correlaciones muestrales:</p>
<p><l class="prop"> Teorema: convergencia de la covarianza y el coeficiente de correlación muestrales. </l>
Sea <span class="math inline">\(\{(X_n,Y_n)\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias bidimensionales independientes dos a dos e idénticamente distribuidas.
Sea <span class="math inline">\(\sigma_{X,Y}, \rho_{XY}\)</span> la covarianza y el coeficiente de correlación de cada par de variables que, al ser idénticamente distribuidas, no dependen de <span class="math inline">\(n\)</span>. Entonces las sucesiones de las covarianzas muestrales <span class="math inline">\(\{S_{X_n,Y_n}\}_{n=1}^\infty\)</span> y los coeficientes de correlación muestrales <span class="math inline">\(\{R_{X_nY_n}\}_{n=1}^\infty\)</span> tienden en probabilidad hacia <span class="math inline">\(\sigma_{XY}\)</span> y <span class="math inline">\(\rho_{XY}\)</span>, respectivamente:
<span class="math display">\[
S_{X_n,Y_n}\stackrel{c.p.}{\longrightarrow}\sigma_{XY},\quad R_{X_nY_n}\stackrel{c.p.}{\longrightarrow}\rho_{XY}.
\]</span></p>
</div>
<div id="convergencia-de-los-momentos-muestrales-9" class="section level3">
<h3><span class="header-section-number">2.2.17</span> Convergencia de los momentos muestrales</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Para la demostración basta aplicar la <strong>ley débil de los grandes números</strong> a las sucesiones <span class="math inline">\(\{S_{X_n,Y_n}\}_{n=1}^\infty\)</span> y <span class="math inline">\(\{R_{X_nY_n}\}_{n=1}^\infty\)</span>. Dejamos los detalles como ejercicio.</p>
</div>
</div>
<div id="leyes-fuertes-de-los-grandes-números" class="section level3">
<h3><span class="header-section-number">2.2.18</span> Leyes fuertes de los grandes números</h3>
<p>Vamos a dar una versión de la ley débil de los grandes números pero en lugar de tener convergencia <strong>en probabilidad</strong>, tendremos convergencia <strong>casi segura</strong>.</p>
<p><l class="prop"> Teorema de Kolmogorov. Ley fuerte de los grandes números. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes y con varianza <span class="math inline">\(\sigma_n^2\)</span>. Supongamos que la serie
<span class="math inline">\(\sum\limits_{n=1}^\infty \frac{\sigma_n^2}{n^2},\)</span>
es convergente. Entonces la sucesión de las medias muestrales <span class="math inline">\(\{\overline{X}_n\}_{n=1}^\infty\)</span> cumplen la
llamada <strong>ley fuerte de los grandes números</strong>:
<span class="math display">\[
\overline{X}_n-\frac{1}{n}\sum_{i=1}^n \mu_i \stackrel{c.s.}{\longrightarrow} 0.
\]</span></p>
</div>
<div id="leyes-fuertes-de-los-grandes-números-1" class="section level3">
<h3><span class="header-section-number">2.2.19</span> Leyes fuertes de los grandes números</h3>
<p>Asociados al resultado anterior tenemos los corolarios siguientes:</p>
<p><l class="prop"> Corolario. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes y con varianza <span class="math inline">\(\sigma_n^2\)</span>. Supongamos que existe una constante <span class="math inline">\(M\)</span> tal que todas las varianzas están acotadas por <span class="math inline">\(M\)</span>: <span class="math inline">\(\sigma_n^2\leq M\)</span>, para todo <span class="math inline">\(n\)</span>.
Entonces la sucesión de las medias muestrales <span class="math inline">\(\{\overline{X}_n\}_{n=1}^\infty\)</span> cumplen la
llamada <strong>ley fuerte de los grandes números</strong>:
<span class="math display">\[
\overline{X}_n-\frac{1}{n}\sum_{i=1}^n \mu_i \stackrel{c.s.}{\longrightarrow} 0.
\]</span></p>
</div>
<div id="leyes-fuertes-de-los-grandes-números-2" class="section level3">
<h3><span class="header-section-number">2.2.20</span> Leyes fuertes de los grandes números</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Si <span class="math inline">\(\sigma_n^2\leq M\)</span> para todo <span class="math inline">\(n\)</span>, la serie numérica <span class="math inline">\(\sum\limits_{n=1}^\infty \frac{\sigma_n^2}{n^2}\)</span> será convergente ya que, por el criterio de acotación,
<span class="math display">\[
\sum\limits_{n=1}^\infty \frac{\sigma_n^2}{n^2}\leq M\sum\limits_{n=1}^\infty \frac{1}{n^2},
\]</span>
que es convergente.</p>
<p>Entonces aplicando el <strong>Teorema de Kolmogorov</strong> o la <strong>ley fuerte de los grandes números</strong>, tenemos el resultado.</p>
</div>
</div>
<div id="leyes-fuertes-de-los-grandes-números-3" class="section level3">
<h3><span class="header-section-number">2.2.21</span> Leyes fuertes de los grandes números</h3>
<p><l class="prop"> Corolario. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes de Bernoulli con el mismo parámetro <span class="math inline">\(p\)</span> que es lo mismo que decir que son idénticamente distribuidas.
Entonces la sucesión de las medias muestrales convergen <strong>casi seguramente</strong> hacia <span class="math inline">\(p\)</span>:
<span class="math display">\[
\overline{X}_n \stackrel{c.s.}{\longrightarrow} p.
\]</span></p>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>En este caso:
<span class="math display">\[
\frac{1}{n}\sum_{i=1}^n \mu_i = \frac{1}{n}\cdot np=p.
\]</span>
También se verifica que <span class="math inline">\(\sigma_n^2 =p(1-p)\)</span>. Por tanto, existe una constante <span class="math inline">\(M\)</span> (<span class="math inline">\(M=p(1-p)\)</span>) tal que <span class="math inline">\(\sigma_n^2\leq M\)</span>. Aplicando el Corolario anterior, obtenemos el resultado.</p>
</div>
</div>
<div id="ejemplo-149" class="section level3">
<h3><span class="header-section-number">2.2.22</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Vamos a repetir el ejemplo de las variables aleatorias de Bernoulli <span class="math inline">\(X_n\)</span>, todas de parámetro <span class="math inline">\(p=\frac{1}{2}\)</span> y comprobar que las proporciones de caras cuando lanzamos la moneda <span class="math inline">\(n\)</span> veces, o sea, las medias muestrales <span class="math inline">\(\overline{X}_n\)</span> tienden <strong>casi seguramente</strong> hacia <span class="math inline">\(p=\frac{1}{2}\)</span>.</p>
<p>La comprobación anterior es equivalente a ver que la serie:
<span class="math display">\[
\sum_{n=1}^\infty P(|\overline{X}_n-p|&gt;\epsilon),
\]</span>
es convergente fijado <span class="math inline">\(\epsilon &gt;0\)</span>.</p>
</div>
</div>
<div id="ejemplo-150" class="section level3">
<h3><span class="header-section-number">2.2.23</span> Ejemplo</h3>
<div class="example">
<p>Recordemos que en la variable <code>probabilidades.pn</code> calculábamos las probabilidades <span class="math inline">\(P(|\overline{X}_n-p|&gt;\epsilon)\)</span> para un <span class="math inline">\(\epsilon =0.1\)</span>.</p>
<p>Comprobar que la serie anterior es convergente es equivalente a comprobar que las sumas parciales convergen:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="kw">cumsum</span>(probabilidades.pn)</a></code></pre></div>
<p>El problema es que la <code>n</code> y la <code>N</code> escogidas son demasiado pequeñas. Para realizar el experimento actual tenéis que considerar <code>n=1000</code> y <code>N=5000</code>. Id con cuidado que el programa os tardará un rato.</p>
<p>El gráfico de las sumas parciales se muestra a continuación:</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1">N=<span class="dv">1000</span></a>
<a class="sourceLine" id="cb117-2" data-line-number="2"><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span>N,<span class="kw">cumsum</span>(probabilidades.pn),<span class="dt">xlab=</span><span class="kw">expression</span>(n),</a>
<a class="sourceLine" id="cb117-3" data-line-number="3">     <span class="dt">ylab=</span><span class="st">&quot;Sumas parciales&quot;</span>,<span class="dt">col=</span><span class="st">&#39;red&#39;</span>, <span class="dt">type=</span><span class="st">&#39;l&#39;</span>)</a></code></pre></div>
<p>Como se puede observar, la serie parece que converge.</p>
</div>
</div>
<div id="ejemplo-151" class="section level3">
<h3><span class="header-section-number">2.2.24</span> Ejemplo</h3>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-94-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="teorema-central-del-límite" class="section level2">
<h2><span class="header-section-number">2.3</span> Teorema Central del Límite</h2>
<div id="introducción-16" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Introducción</h3>
<p>Sabemos que si una sucesión <span class="math inline">\(\{X_n\}\)</span> está formada por variables normales, la sucesión de medias muestrales <span class="math inline">\(\left\{\overline{X}_n=\frac{\sum\limits_{i=1}^n X_i}{n}\right\}_{n=1}^\infty\)</span> también son normales ya que vimos en el tema de variables multidimensionales que si aplicamos una transformación afín (y, en particular, lineal) a una variable normal multidimensional, el resultado es una normal.</p>
<p>Para calcular la variable <span class="math inline">\(\overline{X}_n\)</span>, es obvio que la transformación lineal es la siguiente:
<span class="math display">\[
\overline{X}_n = \left(\frac{1}{n},\ldots,\frac{1}{n}\right)\begin{pmatrix}X_1 \\\vdots\\ X_n\end{pmatrix}.
\]</span></p>
</div>
<div id="introducción-17" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Introducción</h3>
<p>Si además la sucesión de variables <span class="math inline">\(X_n\)</span> son normales todas con media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2\)</span>, la sucesión <span class="math inline">\(\left\{\overline{X}_n\right\}_{n=1}^\infty\)</span> serán normales de media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\frac{\sigma^2}{n}\)</span>.</p>
<p>Estandarizando las variables anteriores, podemos concluir que las variables medias estandarizadas <span class="math inline">\(Z_n =\left\{\frac{\overline{X}_n-\mu}{\frac{\sigma}{\sqrt{n}}}\right\}_{n=1}^\infty\)</span> todas son <span class="math inline">\(N(0,1)\)</span>.</p>
<p>El <strong>Teorema Central del Límite</strong> generaliza el resultado anterior en el sentido de que si las variables <span class="math inline">\(X_n\)</span> no tienen por qué tener la distribución normal pero son independientes e idénticamente distribuidas, las variables <span class="math inline">\(Z_n\)</span> correspondientes tienden <strong>en ley</strong> a una distribución normal estándar <span class="math inline">\(N(0,1)\)</span>.</p>
<p>En general, se dice que los valores medios de cualquier secuencia de números aproximadamente corresponde a una muestra de una normal.</p>
</div>
<div id="teorema-central-del-límite-1" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Teorema Central del Límite</h3>
<p><l class="prop"> Teorema Central del Límite </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes e idénticamente distribuidas con <span class="math inline">\(E(X_n)=\mu\)</span> y <span class="math inline">\(\mathrm{Var}(X_n)=\sigma^2\)</span> para todo <span class="math inline">\(n\)</span>. Entonces:
<span class="math display">\[
\frac{\sum\limits_{i=1}^n X_i-n\mu}{\sigma\sqrt{n}}\stackrel{{\cal L}}{\longrightarrow} N(0,1).
\]</span></p>
</div>
<div id="teorema-central-del-límite-2" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Teorema Central del Límite</h3>
<p><l class="observ"> Observación. </l>
Una condición equivalente a la tesis del <strong>Teorema Central del Límite</strong> es:
<span class="math display">\[
\frac{\overline{X}_n-\mu}{\frac{\sigma}{\sqrt{n}}}\stackrel{{\cal L}}{\longrightarrow} N(0,1).
\]</span>
Basta dividir por <span class="math inline">\(n\)</span> el numerador y el denominador de la tesis original del <strong>Teorema Central del Límite</strong>.</p>
</div>
<div id="demostración-3" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Demostración</h3>
<p>Para la demostración, usaremos dos propiedades de la función característica:</p>
<p><l class="prop"> Proposición. </l>
Sea <span class="math inline">\(Y_1,\ldots, Y_n\)</span> <span class="math inline">\(n\)</span> variables aleatorias independientes. Sea <span class="math inline">\(S_n\)</span> la variable aleatoria suma de las variables anteriores, <span class="math inline">\(S_n=\sum\limits_{i=1}^n Y_i\)</span>. Entonces, para calcular <span class="math inline">\(\phi_{S_n}\)</span>, podemos usar la expresión siguiente::
<span class="math display">\[
\phi_{S_n}(w)=\phi_{Y_1}(w)\cdots \phi_{Y_n}(w),
\]</span>
donde <span class="math inline">\(w\)</span> es cualquier valor real.</p>
</div>
<div id="demostración-4" class="section level3">
<h3><span class="header-section-number">2.3.6</span> Demostración</h3>
<div class="dem">
<p><strong>Demostración de la proposición</strong></p>
<p>Por definición:
<span class="math display">\[
\begin{array}{rl}
\phi_{S_n}(w) &amp; =E\left(\mathrm{e}^{\mathrm{i} w S_n}\right)=E\left(\mathrm{e}^{\mathrm{i} w \sum\limits_{i=1}^n Y_i}\right) = E\left(\mathrm{e}^{i w Y_1}\cdots \mathrm{e}^{i w Y_n}\right)\\ &amp; \stackrel{\mbox{$Y_1,\ldots,Y_n$ son independientes}}{=} E\left(\mathrm{e}^{i w Y_1}\right)\cdots E\left(\mathrm{e}^{i w Y_n}\right) =\phi_{Y_1}(w)\cdots \phi_{Y_n}(w).
\end{array}
\]</span></p>
</div>
</div>
<div id="demostración-5" class="section level3">
<h3><span class="header-section-number">2.3.7</span> Demostración</h3>
<p><l class="prop"> Proposición. </l>
Sea <span class="math inline">\(Y\)</span> una variable aleatoria. Sea <span class="math inline">\(U=kY\)</span> la variable aleatoria <span class="math inline">\(Y\)</span> multiplicada por un valor real <span class="math inline">\(k\)</span>. Entonces, para calcular <span class="math inline">\(\phi_{U}\)</span>, podemos usar la expresión siguiente:
<span class="math display">\[
\phi_{U}(w)=\phi_Y(kw),
\]</span>
donde <span class="math inline">\(w\)</span> es cualquier valor real.</p>
<div class="dem">
<p><strong>Demostración de la proposición</strong></p>
<p>Por definición:
<span class="math display">\[
\phi_{U}(w)=E\left(\mathrm{e}^{\mathrm{i} w U}\right) = E\left(\mathrm{e}^{\mathrm{i} w k Y}\right)=\phi_Y(kw).
\]</span></p>
</div>
</div>
<div id="demostración-6" class="section level3">
<h3><span class="header-section-number">2.3.8</span> Demostración</h3>
<div class="dem">
<p><strong>Demostración del Teorema Central del Límite</strong></p>
<p>Usando la proposición que vimos al introducir la <strong>convergencia en ley</strong> que dice que una sucesión <span class="math inline">\(\{X_n\}\)</span> converge <strong>en ley</strong> hacia <span class="math inline">\(X\)</span> si, y sólo si, <span class="math inline">\(\lim\limits_{\phi_{X_n}(t)}=\phi_{X}(t)\)</span>, donde <span class="math inline">\(\phi\)</span> representa la función característica y la condición anterior tiene que verificarse para todo valor <span class="math inline">\(t\in\mathbb{R}\)</span>, basta demostrar que, si llamamos <span class="math inline">\(Z_n\)</span> a
<span class="math inline">\(Z_n = \frac{\sum\limits_{i=1}^n X_i-n\mu}{\sigma\sqrt{n}},\)</span>
<span class="math display">\[
\lim_{n\to \infty}\phi_{Z_n}(w)=\phi_Z(w),
\]</span>
para cualquier valor <span class="math inline">\(w\in\mathbb{R}\)</span>, siendo <span class="math inline">\(Z=N(0,1)\)</span>.</p>
</div>
</div>
<div id="demostración-7" class="section level3">
<h3><span class="header-section-number">2.3.9</span> Demostración</h3>
<div class="dem">
<p>Seguidamente, simplifiquemos la expresión <span class="math inline">\(\phi_{Z_n}(w)=\phi_{\frac{\sum\limits_{i=1}^n X_i-n\mu}{\sigma\sqrt{n}}}(w)\)</span> usando las dos proposiciones anteriores. En primer lugar, teniendo en cuenta que las variables <span class="math inline">\(\left\{\frac{X_i-\mu}{\sigma\sqrt{n}}\right\}\)</span> son independientes e idénticamente distribuidas, usando la primera proposición podemos escribir:
<span class="math display">\[
\phi_{Z_n}(w) = \left(\phi_{\frac{X-\mu}{\sigma\sqrt{n}}}(w)\right)^n,
\]</span>
donde <span class="math inline">\(X\)</span> representa cualquiera de las variables <span class="math inline">\(X_i\)</span>.</p>
<p>Usando la segunda proposición, podemos simplificar la expresión anterior aún más:
<span class="math display">\[
\phi_{Z_n}(w) = \left(\phi_{\frac{X-\mu}{\sigma\sqrt{n}}}(w)\right)^n = \left(\phi_{X-\mu}\left(\frac{w}{\sigma\sqrt{n}}\right)\right)^n.
\]</span></p>
</div>
</div>
<div id="demostración-8" class="section level3">
<h3><span class="header-section-number">2.3.10</span> Demostración</h3>
<div class="dem">
<p>Si desarrollamos por Taylor alrededor del valor <span class="math inline">\(\hat{w}=0\)</span> la función característica <span class="math inline">\(\phi_{X-\mu}\left(\hat{w}\right)\)</span> hasta segundo orden, obtenemos:
<span class="math display">\[
\phi_{X-\mu}\left(\hat{w}\right) = \phi_{X-\mu}\left(0\right)+ \phi_{X-\mu}&#39;\left(0\right) \hat{w}+ \phi_{X-\mu}&#39;&#39;\left(0\right)\frac{\hat{w}^2}{2}+O(\hat{w}^3),
\]</span>
donde <span class="math inline">\(O(\hat{w}^3)\)</span> simboliza los términos de orden <span class="math inline">\(\hat{w}^3\)</span> y superiores.</p>
<p>Los valores <span class="math inline">\(\phi_{X-\mu}\left(0\right)\)</span>, <span class="math inline">\(\phi_{X-\mu}&#39;\left(0\right)\)</span> y <span class="math inline">\(\phi_{X-\mu}&#39;&#39;\left(0\right)\)</span> valen: (ver tema de Complementos de variables aleatorias)
<span class="math display">\[
\phi_{X-\mu}\left(0\right)=1, \ \phi_{X-\mu}&#39;\left(0\right)=\frac{1}{\mathrm{i}}E(X-\mu)=0,\ \phi_{X-\mu}&#39;&#39;\left(0\right)=\frac{1}{\mathrm{i}^2}E\left((X-\mu)^2\right)=-\sigma^2.
\]</span>
El desarrollo anterior será:
<span class="math display">\[
\phi_{X-\mu}\left(\hat{w}\right) =1 - \frac{1}{2}\hat{w}^2\sigma^2+O(\hat{w}^3),
\]</span></p>
</div>
</div>
<div id="demostración-9" class="section level3">
<h3><span class="header-section-number">2.3.11</span> Demostración</h3>
<div class="dem">
<p>Aplicando la expresión anterior para <span class="math inline">\(\hat{w}=\frac{w}{\sigma\sqrt{n}}\)</span>, obtenemos:
<span class="math display">\[
\phi_{X-\mu}\left(\frac{w}{\sigma\sqrt{n}}\right) =1 - \frac{1}{2}\left(\frac{w}{\sigma\sqrt{n}}\right)^2\sigma^2+O\left(\frac{w}{\sigma\sqrt{n}}\right)^3= 1-\frac{w^2}{2n}+O\left(\frac{w^3}{n^{\frac{3}{2}}}\right),
\]</span></p>
<p>La función característica de la variable <span class="math inline">\(Z_n\)</span> será usando la expresión anterior:
<span class="math display">\[
\phi_{Z_n}(w)=\left(\phi_{X-\mu}\left(\frac{w}{\sigma\sqrt{n}}\right)\right)^n = \left(1-\frac{w^2}{2n}+O\left(\frac{w^3}{n^{\frac{3}{2}}}\right)\right)^n
\]</span>
El objetivo es calcular el límite de la expresión anterior:
<span class="math display">\[
\lim_{n\to \infty}\phi_{Z_n}(w) = \lim_{n\to\infty} \left(1-\frac{w^2}{2n}+O\left(\frac{w^3}{n^{\frac{3}{2}}}\right)\right)^n = 
\lim_{n\to \infty}\mathrm{e}^{n\cdot \ln \left(1-\frac{w^2}{2n}+O\left(\frac{w^3}{n^{\frac{3}{2}}}\right)\right)}.
\]</span></p>
</div>
</div>
<div id="demostración-10" class="section level3">
<h3><span class="header-section-number">2.3.12</span> Demostración</h3>
<div class="dem">
<p>Usando que para <span class="math inline">\(z\approx 0\)</span>, <span class="math inline">\(\ln(1-z)=z+O(z^2)\)</span>, el límite anterior será:
<span class="math display">\[
\lim_{n\to \infty}\phi_{Z_n}(w) = 
\lim_{n\to \infty}\mathrm{e}^{n\cdot \left(-\frac{w^2}{2n}+O\left(\frac{w^4}{n^{2}}\right)\right)} = \lim_{n\to \infty}\mathrm{e}^{ \left(-\frac{w^2}{2}+O\left(\frac{w^4}{n}\right)\right)} = \mathrm{e}^{-\frac{w^2}{2}},
\]</span>
y dicha expresión coincide con la función característica de la variable <span class="math inline">\(N(0,1)\)</span>, <span class="math inline">\(\phi_{Z}(w)\)</span>.</p>
<p>Recordad que en el tema de Complementos de variables aleatorias vimos que si la variable <span class="math inline">\(U\)</span> era <span class="math inline">\(N(\mu,\sigma)\)</span>, <span class="math inline">\(\phi_{U}(w)=\mathrm{e}^{\mathrm{i}w\mu-\frac{w^2\sigma^2}{2}}\)</span>. Aplicando la fórmula anterior para <span class="math inline">\(\mu=0\)</span> y <span class="math inline">\(\sigma=1\)</span>, obtenemos <span class="math inline">\(\phi_{Z}(w)=\mathrm{e}^{-\frac{w^2}{2}}.\)</span></p>
</div>
</div>
<div id="teorema-central-del-límite-en-la-práctica" class="section level3">
<h3><span class="header-section-number">2.3.13</span> Teorema Central del Límite en la práctica</h3>
<p>El <strong>Teorema Central del Límite</strong> se aplica a la práctica en la forma siguiente:</p>
<p>Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes e idénticamente distribuidas con <span class="math inline">\(E(X_i)=\mu\)</span> y <span class="math inline">\(\mathrm{Var}(X_i)=\sigma^2\)</span>. Entonces, podemos aproximar para <span class="math inline">\(n\)</span> grande (<span class="math inline">\(n\geq 30\)</span>), la media muestral <span class="math inline">\(\overline{X}_n\)</span> por:
<span class="math display">\[
\overline{X}_n =\frac{1}{n}\sum_{i=1}^n X_i \approx N\left(\mu,\frac{\sigma}{\sqrt{n}}\right),
\]</span>
o también:
<span class="math display">\[
\sum_{i=1}^n X_i \approx N\left(n\mu,\sigma\sqrt{n}\right),
\]</span></p>
</div>
<div id="teorema-central-del-límite-en-la-práctica-1" class="section level3">
<h3><span class="header-section-number">2.3.14</span> Teorema Central del Límite en la práctica</h3>
<p>Las aproximaciones anteriores se pueden obtener teniendo en cuenta que el <strong>Teorema Central del Límite</strong> nos dice que la variable
<span class="math inline">\(Z_n= \frac{\sum\limits_{i=1}^n X_i-n\mu}{\sigma\sqrt{n}}\)</span> es aproximadamente una <span class="math inline">\(N(0,1)\)</span>. Por tanto,
<span class="math display">\[
\frac{\sum\limits_{i=1}^n X_i-n\mu}{\sigma\sqrt{n}} \approx N(0,1),\ \Rightarrow \sum_{i=1}^n X_i\approx \sigma\sqrt{n}\cdot N(0,1)+n\mu = N\left(n\mu,\sigma\sqrt{n}\right).
\]</span></p>
<p>Dividiendo por <span class="math inline">\(n\)</span> la aproximación anterior, obtenemos:
<span class="math display">\[
\overline{X}_n =\frac{1}{n}\sum_{i=1}^n X_i \approx \frac{1}{n}N\left(n\mu,\sigma\sqrt{n}\right) =N\left(\mu,\frac{\sigma}{\sqrt{n}}\right).
\]</span></p>
</div>
<div id="teorema-de-moivre-laplace" class="section level3">
<h3><span class="header-section-number">2.3.15</span> Teorema de Moivre-Laplace</h3>
<p>Si aplicamos el <strong>Teorema Central del Límite</strong> en el caso en que las variables <span class="math inline">\(X_n\)</span> son de Bernoulli de parámetro <span class="math inline">\(p\)</span>, obtenemos el llamado <strong>Teorema de Moivre-Laplace</strong>:</p>
<p><l class="prop"> Teorema de Moivre-Laplace </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes Bernoulli de parámetro <span class="math inline">\(p\)</span>. La variable <span class="math inline">\(\sum\limits_{i=1}^n X_i\)</span> será binomial de parámetros <span class="math inline">\(n\)</span> y <span class="math inline">\(p\)</span>, <span class="math inline">\(B(n,p)\)</span>. Entonces:
<span class="math display">\[
\frac{B(n,p)-np}{\sqrt{n\cdot p\cdot (1-p)}}\stackrel{{\cal L}}{\longrightarrow} N(0,1).
\]</span></p>
</div>
<div id="teorema-de-moivre-laplace-1" class="section level3">
<h3><span class="header-section-number">2.3.16</span> Teorema de Moivre-Laplace</h3>
<p>En la práctica, decimos que podemos aproximar una variable binomial de parámetros <span class="math inline">\(n\)</span> y <span class="math inline">\(p\)</span> por una distribución normal de parámetros <span class="math inline">\(\mu=np\)</span> y <span class="math inline">\(\sigma =\sqrt{n\cdot p\cdot (1-p)}\)</span>:
<span class="math display">\[
B(n,p)\approx N(np,\sqrt{n\cdot p\cdot (1-p)}).
\]</span></p>
</div>
<div id="aproximación-de-una-suma-de-variables-poisson" class="section level3">
<h3><span class="header-section-number">2.3.17</span> Aproximación de una suma de variables Poisson</h3>
<p>Si aplicamos el <strong>Teorema Central del Límite</strong> en el caso en que las variables <span class="math inline">\(X_n\)</span> son de Poisson de parámetro <span class="math inline">\(\lambda\)</span>, obtenemos el resultado siguiente:</p>
<p><l class="prop"> Proposición. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes Poisson de parámetro <span class="math inline">\(\lambda\)</span>. Entonces:
<span class="math display">\[
\frac{\sum\limits_{i=1}^n X_i -n\lambda}{\sqrt{n\cdot \lambda}}\stackrel{{\cal L}}{\longrightarrow} N(0,1).
\]</span></p>
</div>
<div id="aproximación-de-una-suma-de-variables-poisson-1" class="section level3">
<h3><span class="header-section-number">2.3.18</span> Aproximación de una suma de variables Poisson</h3>
<p>Antes de ver la aplicación práctica del resultado anterior, veamos que suma de variables Poisson independientes de parámetro <span class="math inline">\(\lambda\)</span> es una variable Poisson de parámetro <span class="math inline">\(n\lambda\)</span>:</p>
<p><l class="prop"> Proposición. </l>
Sea <span class="math inline">\(\{X_n\}_{n=1}^\infty\)</span> una sucesión de variables aleatorias independientes Poisson de parámetro <span class="math inline">\(\lambda\)</span>. Entonces la variable <span class="math inline">\(\sum\limits_{i=1}^n X_i\)</span> sigue la distribución de Poisson de parámetro <span class="math inline">\(n\lambda\)</span>.</p>
</div>
<div id="aproximación-de-una-suma-de-variables-poisson-2" class="section level3">
<h3><span class="header-section-number">2.3.19</span> Aproximación de una suma de variables Poisson</h3>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>En primer lugar, hallemos la función característica de la distribución de Poisson de parámetro <span class="math inline">\(\lambda\)</span>. Sea <span class="math inline">\(X=Poiss(\lambda)\)</span>. Su función característica en un valor <span class="math inline">\(w\)</span> será:
<span class="math display">\[
\phi_X(w)=E\left(\mathrm{e}^{\mathrm{i} w X}\right)=\sum_{k=0}^\infty \mathrm{e}^{i w k}\frac{\lambda^k}{k!}\mathrm{e}^{-\lambda}=\mathrm{e}^{-\lambda} \sum_{k=0}^\infty \frac{\left(\lambda\mathrm{e}^{iw}\right)^k}{k!}=\mathrm{e}^{-\lambda}\cdot \mathrm{e}^{\lambda\mathrm{e}^{iw}}=\mathrm{e}^{\lambda \left(\mathrm{e}^{iw}-1\right)}.
\]</span>
Sea ahora la variable <span class="math inline">\(S_n=\sum\limits_{i=1}^n X_i\)</span>. Usando la proposición anterior que nos calcula la función característica de sumas de variables independientes, podemos escribir:
<span class="math display">\[
\phi_{S_n}(w)=\phi_{X_1}(w)\cdots \phi_{X_n}(w)=\left(\mathrm{e}^{\lambda \left(\mathrm{e}^{iw}-1\right)}\right)^n =\mathrm{e}^{n\lambda \left(\mathrm{e}^{iw}-1\right)},
\]</span>
función característica que corresponde a una variable de Poisson de parámetro <span class="math inline">\(n\lambda\)</span>, <span class="math inline">\(Poiss(n\lambda)\)</span>.</p>
</div>
</div>
<div id="aproximación-de-una-suma-de-variables-poisson-3" class="section level3">
<h3><span class="header-section-number">2.3.20</span> Aproximación de una suma de variables Poisson</h3>
<p>Usando la proposición anterior, tenemos que la suma de variables Poisson independientes de parámetro <span class="math inline">\(\lambda\)</span> sigue una distribución Poisson de parámetro <span class="math inline">\(n\lambda\)</span>. Por tanto, podemos escribir usando el corolario del <strong>Teorema Central del Límite</strong> aplicado a variables Poisson:
<span class="math display">\[
Poiss(n\lambda)\approx N(n\lambda,\sqrt{n\lambda}).
\]</span></p>
</div>
<div id="ejemplo-152" class="section level3">
<h3><span class="header-section-number">2.3.21</span> Ejemplo</h3>
<div class="example">
<p><strong>Ejemplo de aplicación del Teorema de Moivre-Laplace</strong></p>
<p>Sea <span class="math inline">\(X\)</span> una distribución binomial de parámetros <span class="math inline">\(n=50\)</span> y <span class="math inline">\(p=\frac{1}{3}\)</span>.</p>
<p>Imaginemos que nos piden <span class="math inline">\(P(X &lt; 15)\)</span> y <span class="math inline">\(P(10\leq X\leq 20)\)</span>.</p>
<p>Vamos a calcular las probabilidades anteriores usando el <strong>Teorema de Moivre-Laplace</strong>.</p>
<p>La variable <span class="math inline">\(X\)</span> es aproximadamente una distribución normal <span class="math inline">\(X_N\)</span> de parámetros <span class="math inline">\(\mu = np=\frac{50}{3}=16.6667\)</span> y <span class="math inline">\(\sigma=\sqrt{50\cdot\frac{1}{3}\cdot \frac{2}{3}}=3.3333\)</span>.</p>
<p>Por tanto:
<span class="math display">\[
\begin{array}{rl}
P(X&lt; 15) &amp; = P(X\leq 14) \approx P(X_N \leq 14)=P\left(Z\leq \frac{14-16.6667}{3.3333}\right) =P(Z\leq -0.8) = 0.2119,\\
P(10\leq X\leq 20) &amp; \approx P(10\leq X_N \leq 20) = P\left(\frac{10-16.6667}{3.3333}\leq  Z\leq \frac{20-16.6667}{3.3333}\right) = P(-2\leq Z\leq 1) \\ &amp; = P(Z\leq 1)-P(Z\leq -2)=0.8413447-0.0227501 = 0.8186,
\end{array}
\]</span>
donde <span class="math inline">\(Z=N(0,1)\)</span>.</p>
</div>
</div>
<div id="ejemplo-153" class="section level3">
<h3><span class="header-section-number">2.3.22</span> Ejemplo</h3>
<div class="example">
<p>Comparemos los valores aproximados anteriores con los valores “exactos” proporcionados por <code>R</code>:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1"><span class="kw">pbinom</span>(<span class="dv">14</span>,<span class="dv">50</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [1] 0.2612386</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw">pbinom</span>(<span class="dv">20</span>,<span class="dv">50</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)<span class="op">-</span><span class="kw">pbinom</span>(<span class="dv">9</span>,<span class="dv">50</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [1] 0.8613685</code></pre>
<p>Tenemos errores de 0.04938 y 0.04277, respectivamente.</p>
<p>Aunque <span class="math inline">\(n\)</span> no es pequeño, <span class="math inline">\(n=50\)</span>, los errores anteriores no son demasiado pequeños.</p>
<p>Una razón por la que dichos errores no son pequeños es que aproximamos una distribución discreta (Binomial) cuyos valores van de 1 en 1 por una distribución normal, que es continua.</p>
<p>La corrección de continuidad de Fisher nos mejora la aproximación disminuyendo dichos errores.</p>
</div>
</div>
<div id="corrección-de-continuidad-de-fisher" class="section level3">
<h3><span class="header-section-number">2.3.23</span> Corrección de continuidad de Fisher</h3>
<p>Cuando aplicamos el <strong>Teorema Central del Límite</strong> y aproximamos una distribución discreta que tiene valores enteros por una normal, hemos de aplicar lo que se llama <strong>corrección de continuidad de Fisher</strong>.</p>
<p>Sea <span class="math inline">\(X\)</span> la variable discreta que queremos aproximar y <span class="math inline">\(X_N\)</span> la variable normal que nos aparece cuando aplicamos el <strong>Teorema Central del Límite</strong>. Supongamos que queremos calcular <span class="math inline">\(P(X\leq k)\)</span>, para un <span class="math inline">\(k\)</span> entero. Entonces debemos hacer:
<span class="math display">\[
P(X\leq k)\approx P(X_N\leq k+0.5).
\]</span>
O sea, para tener en cuenta el valor <span class="math inline">\(k\)</span> en la aproximación <span class="math inline">\(X_N\)</span> hay que sumarle la mitad entre dos valores consecutivos (0.5 si los valores son enteros) de la variable <span class="math inline">\(X\)</span>.</p>
</div>
<div id="corrección-de-continuidad-de-fisher-1" class="section level3">
<h3><span class="header-section-number">2.3.24</span> Corrección de continuidad de Fisher</h3>
<p>Id con cuidado, si queremos calcular <span class="math inline">\(P(X&lt;k)\)</span>, hay que hacer <span class="math inline">\(P(X&lt;k) =P(X\leq k-1)\approx P(X_N \leq k-1+0.5)=P(X_N\leq k-0.5)\)</span>.</p>
<div class="example">
<p><strong>Ejemplo anterior</strong></p>
<p>Si aplicamos la continuidad de Fisher en el ejemplo anterior, obtenemos:
<span class="math display">\[
\begin{array}{rl}
P(X&lt; 15) &amp; = P(X\leq 14) \approx P(X_N \leq 14.5)=P\left(Z\leq \frac{14.5-16.6667}{3.3333}\right) =P(Z\leq -0.65) = 0.2578,\\
P(10\leq X\leq 20) &amp; P(X\leq 20)-P(X\leq 9)\approx P(X_N \leq 20.5)-P(X_N\leq 9.5) \\ &amp; = P\left(Z\leq \frac{20.5-16.6667}{3.3333}\right) - P\left(Z\leq \frac{9.5-16.6667}{3.3333}\right)=  P(Z\leq 1.15)-P(Z\leq -2.15)\\ &amp; =0.8749281-0.0157776 = 0.8592,
\end{array}
\]</span>
obteniendo unos errores de sólo 0.00339 y 0.00222, respectivamente.</p>
</div>
</div>
<div id="simulación-del-teorema-central-del-límite" class="section level3">
<h3><span class="header-section-number">2.3.25</span> Simulación del Teorema Central del Límite</h3>
<div class="example">
<p><strong>Ejemplo de simulación de la aproximación de una variable binomial a una distribución normal</strong></p>
<p>Para realizar la simulación anterior, consideremos una distribución binomial de parámetros <span class="math inline">\(n=100\)</span> y <span class="math inline">\(p=\frac{1}{2}\)</span>.</p>
<p>Según el <strong>Teorema Central del Límite</strong>, tenemos que
<span class="math display">\[
\overline{X}_n=\frac{1}{n}B\left(n=100,p=\frac{1}{2}\right)\approx N\left(\mu = p=\frac{1}{2}=0.5,\sigma=\sqrt{\frac{\frac{1}{2}\cdot \frac{1}{2}}{100}}=0.05\right).
\]</span></p>
<p>Para ver dicha aproximación, en primer lugar vamos a generar una muestra de <span class="math inline">\(N=1000\)</span> valores de una binomial de parámetros <span class="math inline">\(n=100\)</span> y <span class="math inline">\(p=\frac{1}{2}\)</span> y dividiendo por <span class="math inline">\(n=100\)</span>, tenemos una muestra de <span class="math inline">\(\overline{X}_n\)</span>:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1">n=<span class="dv">100</span></a>
<a class="sourceLine" id="cb122-2" data-line-number="2">p=<span class="dv">1</span><span class="op">/</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb122-3" data-line-number="3">sigma=p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)</a>
<a class="sourceLine" id="cb122-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb122-5" data-line-number="5">muestra.binomial =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1000</span>,n,p)</a>
<a class="sourceLine" id="cb122-6" data-line-number="6">muestra.xnbarra =<span class="st"> </span>muestra.binomial<span class="op">/</span>n</a></code></pre></div>
</div>
</div>
<div id="simulación-del-teorema-central-del-límite-1" class="section level3">
<h3><span class="header-section-number">2.3.26</span> Simulación del Teorema Central del Límite</h3>
<div class="example">
<p>Para ver si la aproximación funciona, dibujaremos en una misma gráfica el histograma de frecuencias relativas de la muestra anterior y la curva de la función de densidad de la distribución normal de parámetros <span class="math inline">\(\mu =\frac{1}{2}\)</span> y <span class="math inline">\(\sigma = 0.05\)</span>:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1"><span class="kw">hist</span>(muestra.xnbarra,<span class="dt">freq=</span><span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb123-2" data-line-number="2">     <span class="dt">breaks=</span><span class="kw">seq</span>(<span class="dt">from=</span><span class="kw">min</span>(muestra.xnbarra)<span class="op">-</span><span class="fl">0.1</span>,<span class="dt">to=</span><span class="kw">max</span>(muestra.xnbarra)<span class="op">+</span><span class="fl">0.1</span>,<span class="dt">by=</span><span class="fl">0.01</span>),</a>
<a class="sourceLine" id="cb123-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Histograma de la distribución de las medias muestrales&quot;</span>,</a>
<a class="sourceLine" id="cb123-4" data-line-number="4">     <span class="dt">xlab=</span><span class="st">&quot;valores variable&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;frecuencias relativas&quot;</span>)</a>
<a class="sourceLine" id="cb123-5" data-line-number="5">mu=p</a>
<a class="sourceLine" id="cb123-6" data-line-number="6">sigma.xnbarra=<span class="kw">sqrt</span>(p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)<span class="op">/</span>n)</a>
<a class="sourceLine" id="cb123-7" data-line-number="7">x=<span class="kw">seq</span>(<span class="dt">from=</span><span class="kw">min</span>(muestra.xnbarra),<span class="dt">to=</span><span class="kw">max</span>(muestra.xnbarra),<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb123-8" data-line-number="8"><span class="kw">lines</span>(x,<span class="kw">dnorm</span>(x,mu,sigma.xnbarra),<span class="dt">col=</span><span class="st">&#39;red&#39;</span>)</a></code></pre></div>
<p>Observamos que la aproximación es bastante buena.</p>
</div>
</div>
<div id="simulación-del-teorema-central-del-límite-2" class="section level3">
<h3><span class="header-section-number">2.3.27</span> Simulación del Teorema Central del Límite</h3>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-98-1.png" width="672" /></p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probabilidad.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/joanby/probabilidad/edit/master/2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["curso-probabilidad-udemy.pdf", "curso-probabilidad-udemy.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
